{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Redes Neurais e Deep Learning \u2014 Insper","text":"<p>Este portf\u00f3lio re\u00fane as atividades e projetos desenvolvidos na disciplina de Redes Neurais e Deep Learning do Insper. O objetivo \u00e9 documentar de forma clara e organizada cada etapa do aprendizado, desde os primeiros conceitos de separabilidade de dados at\u00e9 a implementa\u00e7\u00e3o de modelos mais avan\u00e7ados, como redes neurais multicamadas e modelos generativos.</p> <p>Ao longo deste material, est\u00e3o descritos:</p> <ul> <li>A motiva\u00e7\u00e3o de cada exerc\u00edcio ou projeto,</li> <li>O passo a passo de implementa\u00e7\u00e3o,</li> <li>As an\u00e1lises dos resultados obtidos,</li> <li>E as conclus\u00f5es em rela\u00e7\u00e3o ao uso de redes neurais para diferentes problemas.</li> </ul>"},{"location":"#estrutura-do-portfolio","title":"Estrutura do Portf\u00f3lio","text":""},{"location":"#exercicios","title":"\ud83d\udcdd Exerc\u00edcios","text":"<p>Os exerc\u00edcios pr\u00e1ticos t\u00eam como foco a explora\u00e7\u00e3o de conceitos fundamentais de redes neurais.  </p> <ol> <li>Data \u2014 Gera\u00e7\u00e3o e an\u00e1lise de dados sint\u00e9ticos para explorar separabilidade de classes e limites de decis\u00e3o.  </li> <li>Perceptron \u2014 Implementa\u00e7\u00e3o e avalia\u00e7\u00e3o de um perceptron simples para problemas linearmente separ\u00e1veis.  </li> <li>MLP (Multi-Layer Perceptron) \u2014 Constru\u00e7\u00e3o e treinamento de uma rede neural multicamadas para lidar com problemas n\u00e3o lineares.  </li> <li>Metrics \u2014 An\u00e1lise de m\u00e9tricas de avalia\u00e7\u00e3o, discutindo acur\u00e1cia, precis\u00e3o, recall e F1-score no contexto de classifica\u00e7\u00e3o.</li> </ol>"},{"location":"#projetos","title":"\ud83d\ude80 Projetos","text":"<p>Os projetos aplicam os conceitos estudados em problemas mais complexos e realistas.  </p> <ol> <li>Classification \u2014 Modelos de classifica\u00e7\u00e3o em diferentes conjuntos de dados, explorando arquiteturas de redes neurais.  </li> <li>Regression \u2014 Aplica\u00e7\u00e3o de redes neurais em tarefas de regress\u00e3o, analisando desempenho e capacidade de generaliza\u00e7\u00e3o.  </li> <li>Generative Models \u2014 Estudo e implementa\u00e7\u00e3o de modelos generativos (como autoencoders ou GANs), avaliando seu potencial em criar ou reconstruir dados.</li> </ol>"},{"location":"#status-de-desenvolvimento","title":"\ud83d\udccc Status de Desenvolvimento","text":"<ul> <li> Exerc\u00edcio 1 (Data) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 2 (Perceptron) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 3 (MLP) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 4 (Metrics) conclu\u00eddo e documentado.  </li> <li>[] Projeto 1 (Classification) em andamento.  </li> <li>[] Projeto 2 (Regression) em andamento.  </li> <li>[] Projeto 3 (Generative Models) planejado para pr\u00f3xima etapa.  </li> </ul>"},{"location":"#conclusao","title":"\ud83c\udfaf Conclus\u00e3o","text":"<p>Este portf\u00f3lio funciona como um di\u00e1rio de bordo da disciplina, mostrando n\u00e3o apenas os c\u00f3digos implementados, mas tamb\u00e9m as reflex\u00f5es sobre os resultados e os aprendizados obtidos. A ideia \u00e9 que ele sirva como refer\u00eancia tanto para revisitar conceitos importantes quanto para inspirar futuros trabalhos na \u00e1rea de Intelig\u00eancia Artificial aplicada a Redes Neurais.</p>"},{"location":"data/exercicio1/exercicio1/","title":"Exerc\u00edcio 1 \u2014 Data","text":""},{"location":"data/exercicio1/exercicio1/#objetivo","title":"Objetivo","text":"<p>O objetivo deste exerc\u00edcio foi gerar um conjunto de dados sint\u00e9ticos em 2 dimens\u00f5es para analisar sua separabilidade entre classes. Esse processo \u00e9 importante porque ajuda a entender como uma rede neural simples ou mais profunda teria que se adaptar para classificar os dados corretamente.</p>"},{"location":"data/exercicio1/exercicio1/#etapa-1-geracao-dos-dados-sinteticos","title":"Etapa 1 \u2014 Gera\u00e7\u00e3o dos Dados Sint\u00e9ticos","text":"<p>O objetivo desta etapa foi criar um conjunto de dados bidimensionais (vari\u00e1veis <code>x1</code> e <code>x2</code>) divididos em 4 classes distintas, cada uma com 100 pontos, totalizando 400 amostras.</p> <p>Para garantir reprodutibilidade, defini a semente do gerador de n\u00fameros aleat\u00f3rios (<code>np.random.seed(42)</code>), o que faz com que os mesmos pontos sejam gerados a cada execu\u00e7\u00e3o do c\u00f3digo.</p> <p>Cada classe foi gerada usando a fun\u00e7\u00e3o <code>np.random.normal</code>, que amostra valores de uma distribui\u00e7\u00e3o normal (gaussiana) a partir de um valor m\u00e9dio (<code>loc</code>) e um desvio padr\u00e3o (<code>scale</code>). Assim:</p> <ul> <li>Classe 0: centrada em (2, 3), mais espalhada no eixo y (<code>scale=2.5</code>).  </li> <li>Classe 1: centrada em (5, 6), com varia\u00e7\u00e3o moderada em ambos os eixos.  </li> <li>Classe 2: centrada em (8, 1), compacta em torno da m\u00e9dia.  </li> <li>Classe 3: centrada em (15, 4), bem concentrada em <code>x1</code> mas com maior varia\u00e7\u00e3o em <code>x2</code>.</li> </ul> <p>Por fim, usei <code>np.column_stack</code> para juntar as duas vari\u00e1veis (<code>x1</code>, <code>x2</code>) em um array bidimensional, representando os pontos de cada classe.</p> <pre><code>np.random.seed(42)  \nn = 100\n\n# Classe 0\nc0_x1 = np.random.normal(loc=2, scale=0.8, size=n)\nc0_x2 = np.random.normal(loc=3, scale=2.5, size=n)\nc0 = np.column_stack((c0_x1, c0_x2))\n\n# Classe 1\nc1_x1 = np.random.normal(loc=5, scale=1.2, size=n)\nc1_x2 = np.random.normal(loc=6, scale=1.9, size=n)\nc1 = np.column_stack((c1_x1, c1_x2))\n\n# Classe 2\nc2_x1 = np.random.normal(loc=8, scale=0.9, size=n)\nc2_x2 = np.random.normal(loc=1, scale=0.9, size=n)\nc2 = np.column_stack((c2_x1, c2_x2))\n\n# Classe 3\nc3_x1 = np.random.normal(loc=15, scale=0.5, size=n)\nc3_x2 = np.random.normal(loc=4, scale=2.0, size=n)\nc3 = np.column_stack((c3_x1, c3_x2))\n</code></pre>"},{"location":"data/exercicio1/exercicio1/#etapa-2-visualizacao-dos-dados","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o dos Dados","text":"<p>Com as quatro classes j\u00e1 geradas, o pr\u00f3ximo passo foi visualizar a distribui\u00e7\u00e3o dos pontos em um gr\u00e1fico de dispers\u00e3o (scatter plot).</p> <p>Esse gr\u00e1fico \u00e9 essencial porque permite observar a separabilidade entre classes e j\u00e1 adianta poss\u00edveis regi\u00f5es de sobreposi\u00e7\u00e3o que precisaremos analisar na pr\u00f3xima etapa.</p> <pre><code>plt.figure(figsize=(7,5))\nplt.scatter(c0[:,0], c0[:,1], label=\"Class 0\", s=18, alpha=0.8)\nplt.scatter(c1[:,0], c1[:,1], label=\"Class 1\", s=18, alpha=0.8)\nplt.scatter(c2[:,0], c2[:,1], label=\"Class 2\", s=18, alpha=0.8)\nplt.scatter(c3[:,0], c3[:,1], label=\"Class 3\", s=18, alpha=0.8)\n\nplt.title(\"Data\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p> <p>Observando o gr\u00e1fico de dispers\u00e3o obtido:</p> <ul> <li> <p>Classe 0 (azul): est\u00e1 localizada \u00e0 esquerda, centrada em torno de <code>x1 \u2248 2</code>.   Apresenta grande varia\u00e7\u00e3o em <code>x2</code>, formando uma nuvem vertical que vai de valores negativos at\u00e9 acima de 7.  </p> </li> <li> <p>Classe 1 (laranja): posicionada mais ao centro, em <code>x1 \u2248 5</code>.   Distribui-se em n\u00edveis mais altos de <code>x2</code> (acima de 5), com dispers\u00e3o consider\u00e1vel, o que gera sobreposi\u00e7\u00e3o com a Classe 0 em algumas regi\u00f5es.  </p> </li> <li> <p>Classe 2 (verde): centrada em <code>x1 \u2248 8</code> e valores baixos de <code>x2</code> (por volta de 1).   \u00c9 a classe mais compacta, com baixa dispers\u00e3o, o que facilita sua identifica\u00e7\u00e3o. No entanto, apresenta certa proximidade com a Classe 1 na regi\u00e3o de fronteira.  </p> </li> <li> <p>Classe 3 (vermelha): bem afastada das demais, em <code>x1 \u2248 15</code>.   Mesmo com varia\u00e7\u00e3o em <code>x2</code>, mant\u00e9m-se completamente separada das outras classes, o que torna sua classifica\u00e7\u00e3o a mais simples do conjunto.</p> </li> </ul>"},{"location":"data/exercicio1/exercicio1/#conclusoes-sobre-separabilidade","title":"Conclus\u00f5es sobre separabilidade","text":"<ul> <li>Existe uma ordem clara das classes ao longo do eixo <code>x1</code>: C0 \u2192 C1 \u2192 C2 \u2192 C3.  </li> <li>Por\u00e9m, as classes 0 e 1 apresentam sobreposi\u00e7\u00e3o em parte do espa\u00e7o (principalmente entre <code>x1=2</code> e <code>x1=5</code>), o que dificulta a separa\u00e7\u00e3o por um limite totalmente linear.  </li> <li>A Classe 2 \u00e9 bem definida e isolada verticalmente, embora esteja pr\u00f3xima da Classe 1 na horizontal.  </li> <li>A Classe 3 est\u00e1 completamente isolada, sendo a mais f\u00e1cil de separar.  </li> </ul>"},{"location":"data/exercicio1/exercicio1/#etapa-4-definicao-de-fronteiras-lineares-simples","title":"Etapa 4 \u2014 Defini\u00e7\u00e3o de Fronteiras Lineares Simples","text":"<p>Para explorar a separabilidade dos dados, tracei fronteiras lineares verticais ao longo do eixo <code>x1</code>, representando limites de decis\u00e3o iniciais entre as classes.</p> <pre><code>xlines = [\n    (2 + 5) / 2,   \n    (5 + 8) / 2,   \n    (8 + 15) / 2   \n]\n\nplt.figure(figsize=(7,5))\nplt.scatter(c0[:,0], c0[:,1], label=\"Class 0\", s=18, alpha=0.8)\nplt.scatter(c1[:,0], c1[:,1], label=\"Class 1\", s=18, alpha=0.8)\nplt.scatter(c2[:,0], c2[:,1], label=\"Class 2\", s=18, alpha=0.8)\nplt.scatter(c3[:,0], c3[:,1], label=\"Class 3\", s=18, alpha=0.8)\n\nfor x in xlines:\n    plt.axvline(x, linestyle=\"--\", linewidth=2)\n\nplt.title(\"Scatter + fronteiras lineares (simples)\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Divis\u00e3o das classes</p>"},{"location":"data/exercicio1/exercicio1/#analise","title":"An\u00e1lise","text":"<ul> <li>Esse m\u00e9todo de fronteira \u00e9 simples e intuitivo, funcionando bem quando as classes est\u00e3o distribu\u00eddas principalmente em torno de valores diferentes de <code>x1</code>.  </li> <li>No entanto, ele n\u00e3o considera a dispers\u00e3o em <code>x2</code>, o que gera problemas de sobreposi\u00e7\u00e3o:  </li> <li>Classes 0 e 1 continuam com regi\u00f5es de confus\u00e3o, pois se sobrep\u00f5em verticalmente.  </li> <li>Classes 1 e 2 tamb\u00e9m podem apresentar mistura em regi\u00f5es pr\u00f3ximas \u00e0 linha divis\u00f3ria.  </li> <li>A Classe 3, por estar bem afastada, \u00e9 separada de forma perfeita com esse limite linear.</li> </ul>"},{"location":"data/exercicio1/exercicio1/#implicacao-para-redes-neurais","title":"Implica\u00e7\u00e3o para redes neurais","text":"<ul> <li>Um modelo linear simples poderia usar limites parecidos com estes para classificar os dados.  </li> <li>Por\u00e9m, como h\u00e1 sobreposi\u00e7\u00e3o entre classes, uma rede neural com m\u00faltiplas camadas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares teria mais flexibilidade para ajustar fronteiras curvas ou inclinadas, capturando melhor as regi\u00f5es de confus\u00e3o.</li> </ul>"},{"location":"data/exercicio2/exercicio2/","title":"Exercicio 2 - Data","text":""},{"location":"data/exercicio2/exercicio2/#objetivo","title":"Objetivo","text":"<p>O objetivo deste exerc\u00edcio foi criar dois conjuntos de dados em 5 dimens\u00f5es (Classes A e B), cada um com 500 amostras, a partir de distribui\u00e7\u00f5es normais multivariadas. Em seguida, reduzir a dimensionalidade com PCA para 2D e analisar a separabilidade linear.  </p> <p>Minha hip\u00f3tese inicial era de que, como as classes possuem covari\u00e2ncias diferentes, a fronteira \u00f3tima n\u00e3o seria linear, representando um desafio para modelos simples como Perceptron ou Regress\u00e3o Log\u00edstica, e justificando o uso de modelos mais complexos (ex: redes neurais).</p>"},{"location":"data/exercicio2/exercicio2/#etapa-1-definicao-dos-parametros-e-semente-do-gerador","title":"Etapa 1 \u2014 Defini\u00e7\u00e3o dos par\u00e2metros e semente do gerador","text":"<p>Aqui eu preparei o gerador de n\u00fameros aleat\u00f3rios e defini os par\u00e2metros das distribui\u00e7\u00f5es normais multivariadas para as duas classes (A e B).</p> <ul> <li> <p><code>rng = np.random.default_rng(42)</code>   Criei um gerador pseudoaleat\u00f3rio com semente 42 para garantir reprodutibilidade. Isso significa que, ao rodar o c\u00f3digo v\u00e1rias vezes, os mesmos dados ser\u00e3o gerados.</p> </li> <li> <p><code>mu_A</code> e <code>mu_B</code>   S\u00e3o os vetores de m\u00e9dias das classes, cada um com 5 dimens\u00f5es.  </p> </li> <li>Classe A \u00e9 centrada na origem.  </li> <li> <p>Classe B \u00e9 deslocada em rela\u00e7\u00e3o \u00e0 A (todas as m\u00e9dias iguais a 1.5).</p> </li> <li> <p><code>Sigma_A</code> e <code>Sigma_B</code>   S\u00e3o as matrizes de covari\u00e2ncia (5\u00d75) que controlam o espalhamento e as correla\u00e7\u00f5es entre as vari\u00e1veis de cada classe.  </p> </li> <li>Em A, h\u00e1 correla\u00e7\u00f5es positivas entre algumas vari\u00e1veis.  </li> <li>Em B, h\u00e1 correla\u00e7\u00f5es negativas e vari\u00e2ncias maiores.  </li> </ul> <p>Esses par\u00e2metros ser\u00e3o usados na fun\u00e7\u00e3o <code>rng.multivariate_normal</code> para gerar os pontos de cada classe na etapa seguinte.</p> <pre><code>rng = np.random.default_rng(42)\n\nmu_A = np.array([0., 0., 0., 0., 0.])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n])\n\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n])\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-2-amostragem-dos-dados-e-criacao-dos-rotulos","title":"Etapa 2 \u2014 Amostragem dos dados e cria\u00e7\u00e3o dos r\u00f3tulos","text":"<p>Nesta parte eu realmente gerei os pontos das duas classes e preparei os conjuntos de dados e r\u00f3tulos.</p> <ul> <li> <p><code>nA = nB = 500</code>   Defini que cada classe ter\u00e1 500 amostras, totalizando 1000 pontos.</p> </li> <li> <p><code>rng.multivariate_normal(mean=..., cov=..., size=...)</code>   Fun\u00e7\u00e3o que gera amostras de uma distribui\u00e7\u00e3o normal multivariada.  </p> </li> <li><code>mean</code> \u2192 vetor de m\u00e9dias da classe.  </li> <li><code>cov</code> \u2192 matriz de covari\u00e2ncia que define vari\u00e2ncias e correla\u00e7\u00f5es.  </li> <li> <p><code>size</code> \u2192 n\u00famero de amostras a serem geradas.   Assim, <code>A</code> cont\u00e9m os pontos da Classe A (500\u00d75) e <code>B</code> os da Classe B (500\u00d75).</p> </li> <li> <p><code>np.vstack([A, B])</code>   Empilhei os dois conjuntos verticalmente, formando a matriz <code>X</code> com shape (1000, 5).</p> </li> <li> <p><code>np.hstack([np.zeros(nA, int), np.ones(nB, int)])</code>   Criei o vetor de r\u00f3tulos <code>y</code>:  </p> </li> <li><code>0</code> representa a Classe A.  </li> <li><code>1</code> representa a Classe B.   Isso me permite identificar de qual classe cada ponto pertence.</li> </ul> <p>Com isso, foi finalizada a gera\u00e7\u00e3o do dataset 5D completo, pronto para redu\u00e7\u00e3o de dimensionalidade e an\u00e1lise.</p> <pre><code>nA = nB = 500\nA = rng.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nB = rng.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\n\nX = np.vstack([A, B])           \ny = np.hstack([np.zeros(nA, int), np.ones(nB, int)])\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-3-reducao-de-dimensionalidade-com-pca-5d-2d","title":"Etapa 3 \u2014 Redu\u00e7\u00e3o de dimensionalidade com PCA (5D \u2192 2D)","text":"<p>Nesta etapa eu utilizei o PCA (Principal Component Analysis) para reduzir os dados de 5 dimens\u00f5es para apenas 2, permitindo visualiza\u00e7\u00e3o em gr\u00e1fico de dispers\u00e3o.</p> <ul> <li><code>pca = PCA(n_components=2, random_state=42)</code>   Criei um objeto PCA que mant\u00e9m apenas 2 componentes principais.  </li> <li> <p>O PCA encontra dire\u00e7\u00f5es de maior vari\u00e2ncia nos dados, ignorando r\u00f3tulos de classe.</p> </li> <li> <p><code>X2 = pca.fit_transform(X)</code>   Aqui eu ajustei o PCA aos dados 5D (<code>fit</code>) e projetei os pontos nesses dois eixos principais (<code>transform</code>).   O resultado \u00e9 uma matriz <code>X2</code> com shape (1000, 2), representando cada amostra em duas dimens\u00f5es (PC1 e PC2).</p> </li> <li> <p><code>df[\"pc1\"] = X2[:, 0]</code> e <code>df[\"pc2\"] = X2[:, 1]</code>   Adicionei as duas novas colunas ao DataFrame para facilitar visualiza\u00e7\u00e3o e plotagem.</p> </li> <li> <p><code>pca.explained_variance_ratio_</code>   Essa fun\u00e7\u00e3o retorna quanto da vari\u00e2ncia total dos dados originais foi preservada por cada componente.  </p> </li> <li>O <code>print</code> mostra a vari\u00e2ncia explicada por PC1 e PC2 separadamente.  </li> <li>Tamb\u00e9m calculei a soma, para verificar quanta informa\u00e7\u00e3o 5D conseguimos reter em 2D.</li> </ul> <pre><code>pca = PCA(n_components=2, random_state=42)\nX2 = pca.fit_transform(X)\ndf[\"pc1\"] = X2[:, 0]\ndf[\"pc2\"] = X2[:, 1]\n\nprint(\"Vari\u00e2ncia explicada (PC1, PC2):\", np.round(pca.explained_variance_ratio_, 3))\nprint(\"Soma da vari\u00e2ncia explicada:\", np.round(pca.explained_variance_ratio_.sum(), 3))\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-4-visualizacao-grafica-das-classes-no-espaco-2d","title":"Etapa 4 \u2014 Visualiza\u00e7\u00e3o gr\u00e1fica das classes no espa\u00e7o 2D","text":"<p>Nesta etapa eu fiz a visualiza\u00e7\u00e3o final dos dados ap\u00f3s a redu\u00e7\u00e3o de dimensionalidade com PCA. Usei o Matplotlib para criar um gr\u00e1fico de dispers\u00e3o que mostra as duas classes projetadas no plano formado pelos dois componentes principais (PC1 e PC2).</p> <p>Esse gr\u00e1fico permite comparar visualmente como as duas classes se distribuem em 2D e perceber se existe ou n\u00e3o separa\u00e7\u00e3o linear entre elas.</p> <pre><code>plt.figure(figsize=(7, 5))\nplt.scatter(df.loc[df[\"class\"] == 0, \"pc1\"], df.loc[df[\"class\"] == 0, \"pc2\"],\n            s=14, alpha=0.8, label=\"Classe A\")\nplt.scatter(df.loc[df[\"class\"] == 1, \"pc1\"], df.loc[df[\"class\"] == 1, \"pc2\"],\n            s=14, alpha=0.8, label=\"Classe B\")\nplt.title(\"PCA (5D \u2192 2D): Classe A vs Classe B\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p> <p>Separabilidade linear (an\u00e1lise final):</p> <p>Na proje\u00e7\u00e3o em 2D, \u00e9 poss\u00edvel perceber que as duas classes n\u00e3o ficam totalmente separadas: h\u00e1 regi\u00f5es onde os pontos de A e B se misturam. Mesmo considerando os dados no espa\u00e7o original de 5 dimens\u00f5es, a forma como as duas classes est\u00e3o distribu\u00eddas faz com que uma linha reta n\u00e3o seja suficiente para dividi-las corretamente.</p> <p>Isso significa que modelos simples que trabalham apenas com separa\u00e7\u00f5es retas, como o Perceptron, n\u00e3o conseguem representar bem esse tipo de situa\u00e7\u00e3o. Por outro lado, modelos mais avan\u00e7ados, como redes neurais, conseguem criar fronteiras curvas e mais flex\u00edveis, o que permite distinguir melhor as duas classes mesmo quando elas n\u00e3o est\u00e3o separadas de forma simples.</p>"},{"location":"data/exercicio3/exercicio3/","title":"Exerc\u00edcio 3 \u2014 DATA","text":""},{"location":"data/exercicio3/exercicio3/#objetivo","title":"Objetivo","text":"<p>O objetivo \u00e9 preparar o conjunto de dados Spaceship Titanic (train.csv do Kaggle) para uso em uma rede neural que utiliza a fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh.  </p> <p>A seguir, realizei o passo a passo de carregamento, explora\u00e7\u00e3o, tratamento de nulos, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas e padroniza\u00e7\u00e3o de num\u00e9ricas, al\u00e9m de visualizar o impacto do pr\u00e9-processamento.</p>"},{"location":"data/exercicio3/exercicio3/#etapa-1-carregamento-e-descricao-do-dataset","title":"Etapa 1 \u2014 Carregamento e descri\u00e7\u00e3o do dataset","text":"<p>O arquivo <code>train.csv</code> cont\u00e9m cerca de 8700 passageiros. Cada linha representa um passageiro, com informa\u00e7\u00f5es pessoais, dados de viagem e gastos. A coluna <code>Transported</code> \u00e9 o alvo (True/False), indicando se o passageiro foi transportado para outra dimens\u00e3o.</p> <p>Principais tipos de vari\u00e1veis:</p> <ul> <li>Identifica\u00e7\u00e3o: <code>PassengerId</code>, <code>Name</code> .</li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>.</li> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.</li> <li>Alvo: <code>Transported</code>.</li> </ul> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data/train.csv\")\ndisplay(df.head())\nprint(df.info())\nprint(df.isnull().sum())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#valores-ausentes","title":"Valores ausentes","text":"<p>Ap\u00f3s carregar o dataset <code>train.csv</code>, percebi que ele possui 8693 linhas e 14 colunas. Algumas vari\u00e1veis apresentam valores ausentes, enquanto outras est\u00e3o completas:</p> <ul> <li>Sem valores ausentes:</li> <li><code>PassengerId</code> \u2192 identificador \u00fanico de cada passageiro.  </li> <li> <p><code>Transported</code> \u2192 vari\u00e1vel alvo (true ou false para saber se o passageiro foi transportado).</p> </li> <li> <p>Com valores ausentes:</p> </li> <li><code>HomePlanet</code> (201) \u2192 planeta de origem.  </li> <li><code>CryoSleep</code> (217) \u2192 se o passageiro estava em criossuspens\u00e3o.  </li> <li><code>Cabin</code> (199) \u2192 cabine onde estava hospedado.  </li> <li><code>Destination</code> (182) \u2192 planeta de destino.  </li> <li><code>Age</code> (179) \u2192 idade do passageiro.  </li> <li><code>VIP</code> (203) \u2192 se contratou servi\u00e7o VIP.  </li> <li><code>RoomService</code> (181), <code>FoodCourt</code> (183), <code>ShoppingMall</code> (208), <code>Spa</code> (183), <code>VRDeck</code> (188) \u2192 gastos.  </li> <li><code>Name</code> (200) \u2192 nome do passageiro.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>O conjunto apresenta valores faltantes em quase todas as colunas de entrada.  </li> <li>O percentual de valores ausentes em cada coluna \u00e9 relativamente pequeno (entre 2% e 3% do total de linhas), ou seja, \u00e9 vi\u00e1vel aplicar t\u00e9cnicas de preenchimento  em vez de descartar linhas inteiras.  </li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-2-definicao-dos-tipos-de-variaveis","title":"Etapa 2 \u2014 Defini\u00e7\u00e3o dos tipos de vari\u00e1veis","text":"<ul> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.  </li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>.  </li> <li>Alvo: <code>Transported</code>.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-3-tratamento-de-valores-ausentes","title":"Etapa 3 \u2014 Tratamento de valores ausentes","text":"<p>Pelo <code>df.info()</code> e <code>df.isnull().sum()</code>:</p> <ul> <li>H\u00e1 nulos em quase todas as colunas de entrada (entre ~179 e ~217 linhas por coluna, \u22482%\u20133% do total de 8693).</li> </ul> <p>Estrat\u00e9gia adotada:</p> <ol> <li> <p>Categ\u00f3ricas (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>):</p> </li> <li> <p>Preencher com a moda (valor mais frequente).</p> </li> <li> <p>Para <code>Cabin</code>, al\u00e9m de preencher, separar em <code>Deck</code>/<code>Num</code>/<code>Side</code> (formato <code>deck/num/side</code>), pois \u00e9 uma string composta que carrega informa\u00e7\u00e3o \u00fatil.</p> </li> <li> <p>Num\u00e9ricas (<code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>):</p> </li> <li> <p>Preencher com a mediana (robusta a outliers).</p> </li> <li> <p>Regra de neg\u00f3cio adicional: se <code>CryoSleep == True</code>, os gastos deveriam ser zero (passageiro confinado \u00e0 cabine). Assim, nulos nos gastos para quem est\u00e1 em <code>CryoSleep</code> ser\u00e3o imputados com 0; os demais, com mediana.</p> </li> </ol>"},{"location":"data/exercicio3/exercicio3/#etapa-31-preparos-para-imputacao","title":"Etapa 3.1 \u2014 Preparos para imputa\u00e7\u00e3o","text":"<p>Antes de imputar:</p> <ul> <li> <p>Defino listas auxiliares de colunas.</p> </li> <li> <p>Converto <code>CryoSleep</code> e <code>VIP</code> para booleanos/bits depois de preencher (eles vieram como <code>object</code> por causa dos NaNs).</p> </li> <li> <p>Para <code>Cabin</code>, separo em tr\u00eas colunas (<code>Deck</code>, <code>Num</code>, <code>Side</code>), imputo <code>Deck</code>/<code>Side</code> com a moda e <code>Num</code> com a mediana (num\u00e9rica).</p> </li> </ul> <pre><code>num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Cabin\", \"Destination\", \"VIP\"]  # Cabin ser\u00e1 expandida\ntarget = \"Transported\"\n\ncabin_split = df[\"Cabin\"].str.split(\"/\", expand=True)\ndf[\"Deck\"] = cabin_split[0]\ndf[\"Num\"]  = pd.to_numeric(cabin_split[1], errors=\"coerce\")  # num\u00e9rico\ndf[\"Side\"] = cabin_split[2]\n\ncat_cols_expanded = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\nnum_cols_expanded = num_cols + [\"Num\"]\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-32-imputacao-das-categoricas","title":"Etapa 3.2 \u2014 Imputa\u00e7\u00e3o das categ\u00f3ricas","text":"<p>Regra: preencher categ\u00f3ricas com a moda (valor mais frequente). Depois, padronizo <code>CryoSleep</code> e <code>VIP</code> para inteiros 0/1 (necess\u00e1rio para modelos e para aplicar a regra dos gastos = 0 se em criossono).</p> <pre><code>for col in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\", \"CryoSleep\", \"VIP\"]:\n    df[col] = df[col].fillna(df[col].mode()[0])\n\ndef to_bool01(series):\n    return (\n        series\n        .replace({True: 1, False: 0, \"True\": 1, \"False\": 0, \"TRUE\": 1, \"FALSE\": 0})\n        .astype(int)\n    )\n\ndf[\"CryoSleep\"] = to_bool01(df[\"CryoSleep\"])\ndf[\"VIP\"]       = to_bool01(df[\"VIP\"])\n\ndf[\"Transported\"] = df[\"Transported\"].astype(int)\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-33-imputacao-das-numericas","title":"Etapa 3.3 \u2014 Imputa\u00e7\u00e3o das num\u00e9ricas","text":"<p>Regra geral: imputar mediana. Regra de neg\u00f3cio adicional para gastos: se <code>CryoSleep == 1</code>, nulos em gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>) viram 0; os demais nulos seguem para mediana.</p> <p>Para <code>Num</code> (n\u00famero da cabine, derivado de <code>Cabin</code>), uso mediana.</p> <pre><code>spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\nfor col in spend_cols:\n    mask = (df[\"CryoSleep\"] == 1) &amp; (df[col].isna())\n    df.loc[mask, col] = 0.0\n\nfor col in num_cols_expanded:\n    df[col] = df[col].fillna(df[col].median())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-34-limpeza-final-desta-etapa","title":"Etapa 3.4 \u2014 Limpeza final desta etapa","text":"<ul> <li><code>Cabin</code> original \u00e9 removida (substitu\u00edda por <code>Deck</code>, <code>Num</code>, <code>Side</code>).</li> <li><code>Name</code> n\u00e3o ser\u00e1 usado como atributo neste exerc\u00edcio (texto livre), ent\u00e3o removo.</li> <li>Fa\u00e7o uma checagem final para garantir zero valores ausentes nas colunas que seguem para o modelo.</li> </ul> <pre><code>df = df.drop(columns=[\"Cabin\", \"Name\"])\n\nmissing_total = df.isnull().sum().sum()\nprint(f\"valores ausentes: {missing_total}\")\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-35-comentarios-e-justificativas","title":"Etapa 3.5 \u2014 Coment\u00e1rios e justificativas","text":"<ul> <li>Moda nas categ\u00f3ricas: mant\u00e9m a coer\u00eancia dos dados e evita criar categorias artificiais com baixa frequ\u00eancia.</li> <li>Mediana nas num\u00e9ricas: \u00e9 robusta a valores extremos (outliers), comuns nas colunas de gastos.</li> <li>Regra de gastos = 0 em CryoSleep: condiz com a defini\u00e7\u00e3o do problema (passageiros em criossono ficam na cabine e n\u00e3o consomem servi\u00e7os).</li> <li>Separar <code>Cabin</code> em <code>Deck</code>/<code>Num</code>/<code>Side</code>: aproveita a estrutura da informa\u00e7\u00e3o, em vez de trat\u00e1-la como uma string opaca. <code>Deck</code> e <code>Side</code> entram como categ\u00f3ricas; <code>Num</code>, como num\u00e9rica.</li> <li>Remover <code>Name</code>: campo textual livre, de pouca utilidade neste exerc\u00edcio de pr\u00e9-processamento para uma MLP com <code>tanh</code> (sem embeddings/PLN). Pode ser reintroduzido em projetos onde se extraem sobrenomes/grupos.</li> <li>Ap\u00f3s estas decis\u00f5es, o dataset est\u00e1 livre de nulos e pronto para codifica\u00e7\u00e3o categ\u00f3rica e escalonamento (pr\u00f3ximas etapas).</li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-4-codificacao-de-variaveis-categoricas","title":"Etapa 4 \u2014 Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas","text":"<p>Para treinar uma rede neural, todas as entradas precisam estar em formato num\u00e9rico.</p> <ul> <li>Vari\u00e1veis bin\u00e1rias:  </li> <li><code>CryoSleep</code> e <code>VIP</code> j\u00e1 foram convertidas para 0/1.  </li> <li>Vari\u00e1veis multiclasse:  </li> <li><code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code> \u2192 apliquei One-Hot Encoding, criando colunas dummy (0/1).  </li> </ul> <p>Assim, todas as categorias foram transformadas em indicadores num\u00e9ricos sem introduzir ordens artificiais.</p> <pre><code>df = pd.get_dummies(df, columns=[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"], drop_first=True)\n\ndisplay(df.head())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-5-escalonamento-dos-atributos-numericos","title":"Etapa 5 \u2014 Escalonamento dos atributos num\u00e9ricos","text":"<p>A fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh \u00e9 centrada em zero e gera sa\u00eddas em [-1, 1]. Portanto, \u00e9 essencial que as vari\u00e1veis num\u00e9ricas estejam padronizadas para:</p> <ul> <li>m\u00e9dia = 0  </li> <li>desvio padr\u00e3o = 1  </li> </ul> <p>Isso melhora a estabilidade do treinamento e acelera a converg\u00eancia. Para isso, usei <code>StandardScaler</code> da biblioteca <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nnum_cols_final = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Num\"]\n\nscaler = StandardScaler()\ndf[num_cols_final] = scaler.fit_transform(df[num_cols_final])\n\ndisplay(df[num_cols_final].head())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-6-visualizacao-do-impacto-do-pre-processamento","title":"Etapa 6 \u2014 Visualiza\u00e7\u00e3o do impacto do pr\u00e9-processamento","text":"<p>Para evidenciar a transforma\u00e7\u00e3o feita, comparei a distribui\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas antes e depois do escalonamento:</p> <ul> <li>Idade (<code>Age</code>): mostra como foi centralizada em torno de zero e ajustada em escala.  </li> <li>Gasto em <code>FoodCourt</code>: mostra como valores muito altos foram reduzidos para uma escala padr\u00e3o, sem alterar a forma da distribui\u00e7\u00e3o.</li> </ul> <p>Isso comprova que os dados foram preparados corretamente para uso em uma rede neural com <code>tanh</code>.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Dados originais para compara\u00e7\u00e3o\ndf_raw = pd.read_csv(\"data/train.csv\")\n\nfig, axes = plt.subplots(2, 2, figsize=(10,6))\n\n# Antes\naxes[0,0].hist(df_raw[\"Age\"].dropna(), bins=30, color=\"skyblue\")\naxes[0,0].set_title(\"Age (Antes)\")\n\naxes[0,1].hist(df_raw[\"FoodCourt\"].dropna(), bins=30, color=\"salmon\")\naxes[0,1].set_title(\"FoodCourt (Antes)\")\n\n# Depois\naxes[1,0].hist(df[\"Age\"], bins=30, color=\"skyblue\")\naxes[1,0].set_title(\"Age (Depois)\")\n\naxes[1,1].hist(df[\"FoodCourt\"], bins=30, color=\"salmon\")\naxes[1,1].set_title(\"FoodCourt (Depois)\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p> </p> <p>Compara\u00e7\u00e3o pr\u00e9-p\u00f3s processamento</p>"},{"location":"data/exercicio3/exercicio3/#importancia-do-pre-processamento","title":"Import\u00e2ncia do Pr\u00e9-processamento","text":""},{"location":"data/exercicio3/exercicio3/#variavel-age","title":"Vari\u00e1vel Age","text":"<ul> <li>Antes: a idade estava distribu\u00edda entre 0 e 80 anos, com concentra\u00e7\u00e3o principal entre 20 e 40.  </li> <li>Depois: ap\u00f3s a padroniza\u00e7\u00e3o, os valores passaram a variar aproximadamente entre -2 e +3, centralizados em torno de zero.  </li> <li>Impacto: a transforma\u00e7\u00e3o mant\u00e9m a forma da distribui\u00e7\u00e3o, mas coloca a vari\u00e1vel em uma escala adequada para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>, que trabalha melhor com valores pr\u00f3ximos de zero.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#variavel-foodcourt","title":"Vari\u00e1vel FoodCourt","text":"<ul> <li>Antes: os gastos apresentavam valores de 0 at\u00e9 quase 30.000, mas altamente concentrados em torno de zero.  </li> <li>Depois: ap\u00f3s normaliza\u00e7\u00e3o/padroniza\u00e7\u00e3o, os valores foram reduzidos para uma faixa pr\u00f3xima de 0 a 18.  </li> <li>Impacto: isso evita que essa vari\u00e1vel, por estar em uma escala muito maior que as demais, dominasse o processo de treinamento da rede neural.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#comparacao-geral","title":"Compara\u00e7\u00e3o Geral","text":"<ul> <li>O pr\u00e9-processamento preservou o padr\u00e3o das distribui\u00e7\u00f5es, mas ajustou suas escalas para torn\u00e1-las compar\u00e1veis.  </li> <li>Agora, cada vari\u00e1vel contribui de maneira mais equilibrada para o aprendizado, sem que uma tenha peso desproporcional apenas por conta da unidade de medida.  </li> <li>Al\u00e9m disso, como os valores foram centralizados (no caso de Age) e reduzidos (no caso de FoodCourt), a rede neural ter\u00e1 treinamento mais est\u00e1vel e eficiente, evitando satura\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#conclusao","title":"Conclus\u00e3o","text":"<p>O pr\u00e9-processamento foi essencial para:</p> <ul> <li>Reduzir diferen\u00e7as de escala entre atributos,  </li> <li>Adaptar os dados para o intervalo adequado da fun\u00e7\u00e3o de ativa\u00e7\u00e3o,  </li> <li>Garantir que todos os atributos tenham relev\u00e2ncia equilibrada na classifica\u00e7\u00e3o.</li> </ul>"},{"location":"mlp/main/","title":"Main","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> </p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"mlp/exercicio1/exercicio1/","title":"Exerc\u00edcio 1: MLP","text":""},{"location":"mlp/exercicio1/exercicio1/#obejtivo-calculo-manual-de-um-mlp","title":"Obejtivo: Calculo manual de um MLP","text":"<p>Nesta atividade, ser\u00e1 implementado o c\u00e1lculo manual completo de um MLP, abordando as seguintes etapas durante esse processo:</p> <ul> <li><code>Forward Pass</code>: Compreender como os dados fluem atrav\u00e9s da rede, desde a entrada at\u00e9 a sa\u00edda final</li> <li><code>Calculo da fun\u00e7\u00e3o de perda (MSE)</code> </li> <li><code>Backward Pass</code>: Assimilar o algoritmo de backpropagation e como os gradientes s\u00e3o calculados e propagados</li> <li><code>Visualizar a Atualiza\u00e7\u00e3o de Par\u00e2metros</code>: Observar como os pesos e bias s\u00e3o ajustados atrav\u00e9s do gradient descent</li> </ul>"},{"location":"mlp/exercicio1/exercicio1/#arquitetura-da-rede","title":"Arquitetura da Rede:","text":"<ul> <li>Entrada: 2 features</li> <li>Camada oculta: 2 neur\u00f4nios com ativa\u00e7\u00e3o tanh</li> <li>Sa\u00edda: 1 neur\u00f4nio com ativa\u00e7\u00e3o tanh</li> <li>Fun\u00e7\u00e3o de perda: Mean Squared Error (MSE)</li> </ul>"},{"location":"mlp/exercicio1/exercicio1/#1-configuracao-inicial-e-dados","title":"1. Configura\u00e7\u00e3o Inicial e Dados","text":"<p>Primeiro, vamos configurar todos os dados que ser\u00e3o utilizados no MLP:</p> <p>Entrada </p> \\[ x = \\begin{bmatrix} 0.5 &amp; -0.2 \\end{bmatrix} \\] <p>Sa\u00edda alvo </p> \\[ y = 1.0 \\] <p>Pesos e Bias da 1\u00aa camada (W^(1), b^(1)) </p> \\[ W^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\] \\[ b^{(1)} =  \\begin{bmatrix} 0.1 &amp; -0.2 \\end{bmatrix} \\] <p>Pesos e Bias da 2\u00aa camada (W^(2), b^(2)) </p> \\[ W^{(2)} =  \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} \\] \\[ b^{(2)} = 0.2 \\] <p>Taxa de aprendizado </p> \\[ \\eta = 0.1 \\] <pre><code>import numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nx = np.array([0.5, -0.2])  # entrada\ny = 1.0                    # saida alvo\n\n# Pesos e bias da camada oculta\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]])\nb1 = np.array([0.1, -0.2])\n\n# Pesos e bias da camada de saida\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\n\neta = 0.1\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#2-forward-pass-passe-adiante","title":"2. Forward Pass (Passe Adiante)","text":"<p>Agora vamos implementar o passo a passo dos c\u00e1lcuos matematicos para a execu\u00e7\u00e3o do Forward Pass:</p>"},{"location":"mlp/exercicio1/exercicio1/#passo-21-calcular-pre-ativacoes-da-camada-oculta","title":"Passo 2.1: Calcular pr\u00e9-ativa\u00e7\u00f5es da camada oculta","text":"\\[\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}\\] <p>Substituindo os valores:</p> \\[ z^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} \\] <p>C\u00e1lculo:</p> \\[ z^{(1)} = \\begin{bmatrix} (0.3 \\cdot 0.5) + (-0.1 \\cdot -0.2) + 0.1 \\\\ (0.2 \\cdot 0.5) + (0.4 \\cdot -0.2) - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.27 \\\\ -0.18 \\end{bmatrix} \\] <pre><code>z1 = W1 @ x + b1\n\nprint(f\"z^(1) = {z1}\")\n</code></pre> <pre><code>z^(1) = [ 0.27 -0.18]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-22-aplicar-funcao-de-ativacao-tanh-na-camada-oculta","title":"Passo 2.2: Aplicar fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh na camada oculta","text":"\\[\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)})\\] <p>Substituindo os valores:</p> \\[ h^{(1)} = \\tanh \\begin{bmatrix} 0.27 \\\\ -0.18 \\end{bmatrix} \\] <p>Calculando elemento a elemento:</p> \\[ h^{(1)} = \\begin{bmatrix} \\tanh(0.27) \\\\ \\tanh(-0.18) \\end{bmatrix} = \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} \\] <pre><code>h1 = np.tanh(z1)\nprint(f\"h^(1) = {h1}\")\n</code></pre> <pre><code>h^(1) = [ 0.2636 -0.1781]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-23-calcular-pre-ativacao-da-camada-de-saida","title":"Passo 2.3: Calcular pr\u00e9-ativa\u00e7\u00e3o da camada de sa\u00edda","text":"\\[u^{(2)} = \\mathbf{W}^{(2)}\\mathbf{h}^{(1)} + b^{(2)}\\] <p>Substituindo os valores:</p> \\[ u^{(2)} = \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} + 0.2 \\] <p>C\u00e1lculo:</p> \\[ u^{(2)} = (0.5 \\cdot 0.2636) + (-0.3 \\cdot -0.1781) + 0.2 \\] \\[ u^{(2)} \\approx 0.3852 \\] <pre><code>u2 = W2 @ h1 + b2\nprint(f\"u^(2) = {u2}\")\n</code></pre> <pre><code>u^(2) = 0.38523667817130075\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-24-calcular-saida-final","title":"Passo 2.4: Calcular sa\u00edda final","text":"\\[\\hat{y} = \\tanh(u^{(2)})\\] <p>Substituindo o valor calculado:</p> \\[ \\hat{y} = \\tanh(0.3852) \\] <p>Resultado:</p> \\[ \\hat{y} \\approx 0.3672 \\] <pre><code>y_hat = np.tanh(u2)\nprint(f\"\u0177 = {y_hat}\")\n</code></pre> <pre><code>\u0177 = 0.36724656264510797\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#3-calculo-da-perda-loss-calculation","title":"3. C\u00e1lculo da Perda (Loss Calculation)","text":"<p>Agora vamos calcular a fun\u00e7\u00e3o de perda Mean Squared Error (MSE):</p> \\[L = \\frac{1}{N}(y - \\hat{y})^2\\] <p>Como temos apenas uma amostra (N=1), a f\u00f3rmula se simplifica para:</p> \\[L = (y - \\hat{y})^2\\] <p>Substituindo os valores:</p> \\[ L = (1.0 - 0.3672)^2 \\] \\[ L = (0.6328)^2 \\] <p>Resultado:</p> \\[ L \\approx 0.4004 \\] <pre><code>L = (y - y_hat)**2\nprint(f\"L = {L}\")\n</code></pre> <pre><code>L = 0.4003769124844312\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#4-backward-pass-backpropagation","title":"4. Backward Pass (Backpropagation)","text":"<p>Agora vamos implementar o backward pass para calcular todos os gradientes. Come\u00e7amos pela derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda:</p>"},{"location":"mlp/exercicio1/exercicio1/#passo-41-gradiente-da-perda-em-relacao-a-saida","title":"Passo 4.1: Gradiente da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda","text":"\\[\\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y)\\] <p>Note que usamos a derivada de \\((y - \\hat{y})^2 = (\\hat{y} - y)^2\\), que \u00e9 \\(2(\\hat{y} - y)\\)</p> <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}} = 2(0.3672 - 1.0) \\] \\[ \\frac{\\partial L}{\\partial \\hat{y}} = 2(-0.6328) \\] <p>Resultado:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}} \\approx -1.2655 \\] <pre><code>def tanh_dt(u):\n    \"\"\"Deriva de tanh: d/du tanh(u) = 1 - tanh\u00b2(u)\"\"\"\n    return 1 - np.tanh(u)**2\n\ndL_dy_hat = 2 * (y_hat - y)\nprint(f\"\u2202L/\u2202\u0177 = {dL_dy_hat}\")\n</code></pre> <pre><code>\u2202L/\u2202\u0177 = -1.265506874709784\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-42-gradiente-em-relacao-a-pre-ativacao-da-saida","title":"Passo 4.2: Gradiente em rela\u00e7\u00e3o \u00e0 pr\u00e9-ativa\u00e7\u00e3o da sa\u00edda","text":"\\[\\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{d}{du^{(2)}}\\tanh(u^{(2)}) = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot (1 - \\tanh^2(u^{(2)}))\\] <p>Sabemos que:</p> \\[ \\tanh'(z) = 1 - \\tanh^2(z) \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial u^{(2)}} = -1.2655 \\cdot \\left( 1 - \\tanh^2(0.3852) \\right) \\] \\[ \\frac{\\partial L}{\\partial u^{(2)}} = -1.2655 \\cdot (1 - 0.1349) \\] \\[ \\frac{\\partial L}{\\partial u^{(2)}} = -1.2655 \\cdot 0.8651 \\] <p>Resultado:</p> \\[ \\frac{\\partial L}{\\partial u^{(2)}} \\approx -1.0948 \\] <pre><code>dL_du2 = dL_dy_hat * tanh_dt(u2)\nprint(f\"\u2202L/\u2202u^(2) = {dL_du2}\")\n</code></pre> <pre><code>\u2202L/\u2202u^(2) = -1.0948279147135995\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-43-gradientes-para-a-camada-de-saida","title":"Passo 4.3: Gradientes para a camada de sa\u00edda","text":"<p>Agora calculamos os gradientes para os pesos e bias da camada de sa\u00edda:</p>"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-aos-pesos-w2","title":"Gradiente em rela\u00e7\u00e3o aos pesos \\(W^{(2)}\\)","text":"\\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot h^{(1)} \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial W^{(2)}} = -1.0948 \\cdot  \\begin{bmatrix} 0.2636 &amp; -0.1781 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\begin{bmatrix} -0.2886 &amp; 0.1950 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-ao-vies-b2","title":"Gradiente em rela\u00e7\u00e3o ao vi\u00e9s \\(b^{(2)}\\)","text":"\\[ \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\] <p>Substituindo o valor:</p> \\[ \\frac{\\partial L}{\\partial b^{(2)}} = -1.0948 \\] <pre><code>dL_dW2 = dL_du2 * h1  # Gradiente para W^(2)\ndL_db2 = dL_du2       # Gradiente para b^(2)\n\nprint(f\"\u2202L/\u2202W^(2) = {dL_dW2}\")\nprint(f\"\u2202L/\u2202b^(2) = {dL_db2}\")\n</code></pre> <pre><code>\u2202L/\u2202W^(2) = [-0.2886  0.195 ]\n\u2202L/\u2202b^(2) = -1.0948279147135995\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-44-propagacao-para-a-camada-oculta","title":"Passo 4.4: Propaga\u00e7\u00e3o para a camada oculta","text":"<p>Agora precisamos propagar o erro de volta para a camada oculta:</p> \\[\\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = (\\mathbf{W}^{(2)})^T \\cdot \\frac{\\partial L}{\\partial u^{(2)}}\\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial h^{(1)}} = \\begin{bmatrix} 0.5 \\\\ -0.3 \\end{bmatrix} \\cdot (-1.0948) \\] \\[ \\frac{\\partial L}{\\partial h^{(1)}} = \\begin{bmatrix} -0.5474 \\\\ 0.3284 \\end{bmatrix} \\] <pre><code>dL_dh1 = W2 * dL_du2\nprint(f\"\u2202L/\u2202h^(1) = {dL_dh1}\")\n</code></pre> <pre><code>\u2202L/\u2202h^(1) = [-0.5474  0.3284]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-45-gradientes-para-a-camada-oculta","title":"Passo 4.5: Gradientes para a camada oculta","text":"<p>Agora calculamos os gradientes em rela\u00e7\u00e3o \u00e0s pr\u00e9-ativa\u00e7\u00f5es da camada oculta:</p> \\[\\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\odot \\tanh'(\\mathbf{z}^{(1)})\\] <p>onde \\(\\odot\\) representa o produto elemento a elemento (Hadamard product).</p> <p>Sabemos que:</p> \\[ \\tanh'(z) = 1 - \\tanh^2(z) \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix} -0.5474 &amp; 0.3284 \\end{bmatrix} \\odot \\left( 1 - \\tanh^2 \\begin{bmatrix} 0.27 &amp; -0.18 \\end{bmatrix} \\right) \\] \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix} -0.5474 &amp; 0.3284 \\end{bmatrix} \\odot \\begin{bmatrix} 0.9305 &amp; 0.9683 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix} -0.5094 &amp; 0.3180 \\end{bmatrix} \\] <pre><code>dL_dz1 = dL_dh1 * tanh_dt(z1)\nprint(f\"\u2202L/\u2202z^(1) = {dL_dz1}\")\n</code></pre> <pre><code>\u2202L/\u2202z^(1) = [-0.5094  0.318 ]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-46-gradientes-finais-para-pesos-e-bias-da-camada-oculta","title":"Passo 4.6: Gradientes finais para pesos e bias da camada oculta","text":"<p>Finalmente, calculamos os gradientes para os pesos e bias da camada oculta:</p>"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-aos-pesos-w1","title":"Gradiente em rela\u00e7\u00e3o aos pesos \\(W^{(1)}\\)","text":"\\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\otimes x^T \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\begin{bmatrix} -0.5094 \\\\ 0.3180 \\end{bmatrix} \\otimes \\begin{bmatrix} 0.5 &amp; -0.2 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-ao-vies-b1","title":"Gradiente em rela\u00e7\u00e3o ao vi\u00e9s \\(b^{(1)}\\)","text":"\\[ \\frac{\\partial L}{\\partial b^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\] \\[ \\frac{\\partial L}{\\partial b^{(1)}} = \\begin{bmatrix} -0.5094 &amp; 0.3180 \\end{bmatrix} \\] <pre><code>dL_dW1 = np.outer(dL_dz1, x)  \ndL_db1 = dL_dz1               \n\nprint(f\"\u2202L/\u2202W^(1) = \\n{dL_dW1}\")\nprint(f\"\u2202L/\u2202b^(1) = {dL_db1}\")\n</code></pre> <pre><code>\u2202L/\u2202W^(1) = \n[[-0.2547  0.1019]\n [ 0.159  -0.0636]]\n\n \u2202L/\u2202b^(1) = [-0.5094  0.318 ]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#5-atualizacao-dos-parametros","title":"5. Atualiza\u00e7\u00e3o dos Par\u00e2metros","text":"<p>Agora aplicamos o algoritmo de gradient descent para atualizar todos os par\u00e2metros usando a taxa de aprendizagem \u03b7 = 0.1:</p> \\[\\theta_{novo} = \\theta_{antigo} - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}\\] <p>Par\u00e2metros antes da atualiza\u00e7\u00e3o</p> \\[ W^{(2)} =  \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix}, \\quad b^{(2)} = 0.2 \\] \\[ W^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; \\phantom{-}0.4 \\end{bmatrix}, \\quad b^{(1)} = \\begin{bmatrix} 0.1 &amp; -0.2 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-w2","title":"Atualiza\u00e7\u00e3o de \\(W^{(2)}\\)","text":"\\[ W^{(2)}_{\\text{novo}} = W^{(2)} - \\eta \\,\\frac{\\partial L}{\\partial W^{(2)}} \\] \\[ W^{(2)}_{\\text{novo}} = \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.2886 &amp; 0.1950 \\end{bmatrix} = \\begin{bmatrix} 0.5289 &amp; -0.3195 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-b2","title":"Atualiza\u00e7\u00e3o de \\(b^{(2)}\\)","text":"\\[ b^{(2)}_{\\text{novo}} = b^{(2)} - \\eta \\,\\frac{\\partial L}{\\partial b^{(2)}} = 0.2 - 0.1 \\cdot (-1.0948279147135995) = 0.30948279147136 \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-w1","title":"Atualiza\u00e7\u00e3o de \\(W^{(1)}\\)","text":"\\[ W^{(1)}_{\\text{novo}} = W^{(1)} - \\eta \\,\\frac{\\partial L}{\\partial W^{(1)}} \\] \\[ W^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ \\phantom{-}0.1590 &amp; -0.0636 \\end{bmatrix} = \\begin{bmatrix} 0.3255 &amp; -0.1102 \\\\ 0.1841 &amp; \\phantom{-}0.4064 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-b1","title":"Atualiza\u00e7\u00e3o de \\(b^{(1)}\\)","text":"\\[ b^{(1)}_{\\text{novo}} = b^{(1)} - \\eta \\,\\frac{\\partial L}{\\partial b^{(1)}} \\] \\[ b^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.1 &amp; -0.2 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.5094 &amp; 0.3180 \\end{bmatrix} = \\begin{bmatrix} 0.1509 &amp; -0.2318 \\end{bmatrix} \\] <pre><code>W2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\n\nprint(f\"W^(2)_novo = {W2_new}\")\nprint(f\"b^(2)_novo = {b2_new}\")\nprint(f\"W^(1)_novo = \\n{W1_new}\")\nprint(f\"b^(1)_novo = {b1_new}\")\n</code></pre> <pre><code>W^(2)_novo = [ 0.5289 -0.3195]\nb^(2)_novo = 0.30948279147136\n\nW^(1)_novo = \n[[ 0.3255 -0.1102]\n [ 0.1841  0.4064]]\n\nb^(1)_novo = [ 0.1509 -0.2318]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#6-verificacao-dos-resultados","title":"6. Verifica\u00e7\u00e3o dos Resultados","text":"<p>Vamos verificar se a atualiza\u00e7\u00e3o dos par\u00e2metros realmente melhorou o desempenho da rede calculando um novo forward pass com os par\u00e2metros atualizados:</p> <p>Ap\u00f3s atualizar os par\u00e2metros, realizamos um novo forward pass:</p> \\[ z^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.3357 &amp; -0.221 \\end{bmatrix} \\] \\[ h^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.3236 &amp; -0.2175 \\end{bmatrix} \\] \\[ u^{(2)}_{\\text{novo}} = 0.5501 \\] \\[ \\hat{y}_{\\text{novo}} = 0.5006 \\] \\[ L_{\\text{novo}} = 0.2494 \\] <pre><code>def forward_pass(x, W1, b1, W2, b2):\n    z1 = W1 @ x + b1\n    h1 = np.tanh(z1)\n    u2 = W2 @ h1 + b2\n    y_hat = np.tanh(u2)\n    return z1, h1, u2, y_hat\n\nz1_new, h1_new, u2_new, y_hat_new = forward_pass(x, W1_new, b1_new, W2_new, b2_new)\nL_new = (y - y_hat_new)**2\n\nprint(f\"z^(1)_novo = {z1_new}\")\nprint(f\"h^(1)_novo = {h1_new}\")\nprint(f\"u^(2)_novo = {u2_new}\")\nprint(f\"\u0177_novo = {y_hat_new}\")\nprint(f\"L_novo = {L_new}\")\n</code></pre> <pre><code>z^(1)_novo = [ 0.3357 -0.221 ]\nh^(1)_novo = [ 0.3236 -0.2175]\nu^(2)_novo = 0.5501335506731257\n\u0177_novo = 0.5006202979935049\nL_novo = 0.2493800867760958\n</code></pre> M\u00e9trica Antes Depois Melhoria Sa\u00edda \\(\\hat{y}\\) 0.3672 0.5006 Sim Perda \\(L\\) 0.4004 0.2494 Sim Erro absoluto 0.6328 0.4994 Sim <p>Houve uma redu\u00e7\u00e3o da perda de 37.71%, confirmando que a atualiza\u00e7\u00e3o dos par\u00e2metros aproximou a sa\u00edda predita do valor alvo.</p>"},{"location":"mlp/exercicio2/exercicio2/","title":"Exerc\u00edcio 2: MLP","text":""},{"location":"mlp/exercicio2/exercicio2/#objetivo","title":"Objetivo","text":"<p>Implementar um Multi-Layer Perceptron (MLP) do zero para resolver um problema de classifica\u00e7\u00e3o bin\u00e1ria, utilizando apenas a biblioteca NumPy para c\u00e1lculos matem\u00e1ticos. Este exerc\u00edcio demonstra na pr\u00e1tica como construir, treinar e avaliar uma rede neural artificial sem o uso de frameworks de deep learning.</p>"},{"location":"mlp/exercicio2/exercicio2/#especificacoes-tecnicas","title":"Especifica\u00e7\u00f5es T\u00e9cnicas:","text":"<ul> <li>Dataset: 1000 amostras sint\u00e9ticas com 2 features</li> <li>Classes: 2 (classifica\u00e7\u00e3o bin\u00e1ria)</li> <li>Arquitetura: 2 \u2192 8 \u2192 1 neur\u00f4nios</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o: tanh (camada oculta) + sigmoid (sa\u00edda)</li> <li>Fun\u00e7\u00e3o de Perda: Binary Cross-Entropy</li> <li>Otimizador: Gradient Descent</li> </ul>"},{"location":"mlp/exercicio2/exercicio2/#1-configuracao-inicial-e-importacao-de-bibliotecas","title":"1. Configura\u00e7\u00e3o Inicial e Importa\u00e7\u00e3o de Bibliotecas","text":"<p>Antes de come\u00e7ar a implementa\u00e7\u00e3o, precisamos importar as bibliotecas necess\u00e1rias e configurar o ambiente de desenvolvimento. Vamos usar NumPy para opera\u00e7\u00f5es matem\u00e1ticas, Matplotlib/Seaborn para visualiza\u00e7\u00f5es e Scikit-learn apenas para gera\u00e7\u00e3o de dados sint\u00e9ticos e m\u00e9tricas de avalia\u00e7\u00e3o.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport seaborn as sns\n\nnp.set_printoptions(precision=4, suppress=True)\nplt.style.use('default')\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#2-geracao-e-preparacao-dos-dados","title":"2. Gera\u00e7\u00e3o e Prepara\u00e7\u00e3o dos Dados","text":""},{"location":"mlp/exercicio2/exercicio2/#21-criacao-do-dataset-sintetico","title":"2.1 Cria\u00e7\u00e3o do Dataset Sint\u00e9tico","text":"<p>Vamos criar um conjunto de dados artificiais para classifica\u00e7\u00e3o bin\u00e1ria. O Scikit-learn possui uma fun\u00e7\u00e3o espec\u00edfica para isso que nos permite controlar caracter\u00edsticas como n\u00famero de amostras, features e complexidade da separa\u00e7\u00e3o entre classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Gera 1000 amostras com 2 features cada</li> <li>Cria 2 classes balanceadas (500 amostras cada)</li> <li>Configura 2 clusters por classe para tornar o problema mais interessante</li> <li>Adiciona um pequeno ru\u00eddo (1%) para simular dados reais</li> </ul> <pre><code>n_samples = 1000\nn_features = 2\nn_clusters_per_class = 2\nn_informative = 2\nn_redundant = 0\nrandom_state = 42\n\nX, y = make_classification(\n    n_samples=n_samples,\n    n_features=n_features,\n    n_informative=n_informative,\n    n_redundant=n_redundant,\n    n_clusters_per_class=n_clusters_per_class,\n    random_state=random_state,\n    flip_y=0.01\n)\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#22-visualizacao-dos-dados","title":"2.2 Visualiza\u00e7\u00e3o dos Dados","text":"<p>\u00c9 fundamental visualizar os dados antes de treinar qualquer modelo. Isso nos ajuda a entender a distribui\u00e7\u00e3o das classes, identificar poss\u00edveis padr\u00f5es e avaliar a complexidade do problema de classifica\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria um scatter plot das duas classes</li> <li>Mostra a distribui\u00e7\u00e3o espacial dos pontos no espa\u00e7o 2D</li> </ul> <pre><code>plt.figure(figsize=(10, 8))\n\ncolors = ['red', 'blue']\nlabels = ['Classe 0', 'Classe 1']\n\nfor i in range(2):\n    mask = y == i\n    plt.scatter(X[mask, 0], X[mask, 1], \n               c=colors[i], label=labels[i], \n               alpha=0.7, s=50)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Conjunto de Dados Sint\u00e9ticos para Classifica\u00e7\u00e3o Bin\u00e1ria')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio2/exercicio2/#23-divisao-e-normalizacao-dos-dados","title":"2.3 Divis\u00e3o e Normaliza\u00e7\u00e3o dos Dados","text":"<p>Antes de treinar o modelo, precisamos dividir os dados em conjuntos de treino e teste, al\u00e9m de normalizar os features. A normaliza\u00e7\u00e3o \u00e9 crucial para MLPs, pois garante que todas as features tenham a mesma escala, evitando que features com valores maiores dominem o processo de aprendizagem.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Divide dados em 80% treino e 20% teste</li> <li>Mant\u00e9m a propor\u00e7\u00e3o das classes em ambos os conjuntos </li> <li>Normaliza os dados usando Z-score (m\u00e9dia=0, desvio=1)</li> <li>Aplica a mesma normaliza\u00e7\u00e3o do treino no conjunto de teste</li> </ul> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42, \n    stratify=y\n)\n\n\n# normalizacao\nX_train_mean = X_train.mean(axis=0)\nX_train_std = X_train.std(axis=0)\n\nX_train_norm = (X_train - X_train_mean) / X_train_std\nX_test_norm = (X_test - X_train_mean) / X_train_std\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#3-implementacao-da-classe-mlp","title":"3. Implementa\u00e7\u00e3o da Classe MLP","text":""},{"location":"mlp/exercicio2/exercicio2/#31-estrutura-principal-e-inicializacao","title":"3.1 Estrutura Principal e Inicializa\u00e7\u00e3o","text":"<p>Vamos criar uma classe que encapsula toda a funcionalidade do nosso MLP. A inicializa\u00e7\u00e3o \u00e9 um passo cr\u00edtico, pois determina os valores iniciais dos pesos e bias que a rede usar\u00e1 para come\u00e7ar o aprendizado.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Inicializa pesos com valores pequenos e aleat\u00f3rios</li> <li>Inicializa bias com zeros</li> </ul> <pre><code>class MLP:\n\n    def __init__(self, learning_rate=0.05):\n        self.learning_rate = learning_rate\n\n        hidden_size = 12\n\n        self.W1 = np.random.randn(hidden_size, 2) * np.sqrt(2.0 / 2)\n        self.b1 = np.zeros((hidden_size, 1))\n        self.W2 = np.random.randn(1, hidden_size) * np.sqrt(2.0 / hidden_size)\n        self.b2 = np.zeros((1, 1))\n\n        self.loss_history = []\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#32-funcoes-de-ativacao-e-suas-derivadas","title":"3.2 Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o e suas Derivadas","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o fundamentais para permitir que a rede neural aprenda padr\u00f5es n\u00e3o-lineares. Implementamos tanh para a camada oculta (permite valores negativos e positivos) e sigmoid para a sa\u00edda (valores entre 0 e 1, ideal para probabilidades).</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa a fun\u00e7\u00e3o tanh e sua derivada</li> <li>Implementa a fun\u00e7\u00e3o sigmoid com prote\u00e7\u00e3o contra overflow</li> <li>As derivadas s\u00e3o necess\u00e1rias para o algoritmo de backpropagation</li> </ul> <pre><code>def tanh(self, z):\n    return np.tanh(z)\n\ndef tanh_derivative(self, z):\n    \"\"\"Derivada da tanh: d/dz tanh(z) = 1 - tanh\u00b2(z)\"\"\"\n    return 1 - np.tanh(z)**2\n\ndef sigmoid(self, z):\n    \"\"\"Fun\u00e7\u00e3o sigmoid: sa\u00edda entre 0 e 1\"\"\"\n    z_clipped = np.clip(z, -500, 500)\n    return 1 / (1 + np.exp(-z_clipped))\n\nMLP.tanh = tanh\nMLP.tanh_derivative = tanh_derivative\nMLP.sigmoid = sigmoid\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#33-forward-pass-propagacao-adiante","title":"3.3 Forward Pass (Propaga\u00e7\u00e3o Adiante)","text":"<p>O forward pass \u00e9 respons\u00e1vel por processar os dados de entrada atrav\u00e9s de todas as camadas da rede at\u00e9 produzir a sa\u00edda final. Este \u00e9 o processo de \"predi\u00e7\u00e3o\" da rede neural.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Recebe dados de entrada e os propaga atrav\u00e9s da rede</li> <li>Aplica transforma\u00e7\u00f5es lineares (multiplica\u00e7\u00e3o matricial + bias)</li> <li>Aplica fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o-lineares</li> <li>Armazena valores intermedi\u00e1rios (cache) para usar no backpropagation</li> <li>Retorna tanto a sa\u00edda final quanto o cache</li> </ul> <pre><code>def forward(self, X):\n    \"\"\"\n    Forward pass: propaga dados atrav\u00e9s da rede.\n\n    Args:\n        X: dados de entrada (n_samples, n_features)\n\n    Returns:\n        A2: sa\u00edda da rede (probabilidades)\n        cache: valores intermedi\u00e1rios para backprop\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    A0 = X.T\n\n    # camada oculta: transforma\u00e7\u00e3o + ativa\u00e7\u00e3o\n    Z1 = self.W1 @ A0 + self.b1  \n    A1 = self.tanh(Z1)           \n\n    # camada de sa\u00edda: transforma\u00e7\u00e3o + ativa\u00e7\u00e3o\n    Z2 = self.W2 @ A1 + self.b2  \n    A2 = self.sigmoid(Z2)        \n\n    cache = {\n        'A0': A0,  # Entrada\n        'Z1': Z1,  # Pr\u00e9-ativa\u00e7\u00e3o camada oculta\n        'A1': A1,  # Ativa\u00e7\u00e3o camada oculta\n        'Z2': Z2,  # Pr\u00e9-ativa\u00e7\u00e3o sa\u00edda\n        'A2': A2   # Ativa\u00e7\u00e3o sa\u00edda (probabilidade)\n    }\n\n    return A2, cache\n\ndef predict_proba(self, X):\n    \"\"\"Retorna probabilidades de classifica\u00e7\u00e3o\"\"\"\n    output, _ = self.forward(X)\n    return output.T\n\ndef predict(self, X):\n    \"\"\"Retorna predi\u00e7\u00f5es bin\u00e1rias (0 ou 1)\"\"\"\n    probabilities = self.predict_proba(X)\n    return (probabilities &gt; 0.5).astype(int).flatten()\n\nMLP.forward = forward\nMLP.predict_proba = predict_proba\nMLP.predict = predict\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#34-funcao-de-perda-e-backward-pass","title":"3.4 Fun\u00e7\u00e3o de Perda e Backward Pass","text":"<p>A fun\u00e7\u00e3o de perda mede qu\u00e3o distantes est\u00e3o nossas predi\u00e7\u00f5es dos valores reais. O backward pass (backpropagation) calcula como cada par\u00e2metro da rede deve ser ajustado para reduzir essa perda.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa Binary Cross-Entropy Loss</li> <li>Calcula gradientes de todos os par\u00e2metros usando a regra da cadeia</li> <li>Propaga o erro da sa\u00edda de volta para todas as camadas</li> <li>Retorna gradientes para atualiza\u00e7\u00e3o dos par\u00e2metros</li> </ul> <pre><code>def compute_loss(self, y_true, y_pred):\n    \"\"\"    \n    Args:\n        y_true: labels verdadeiros (0 ou 1)\n        y_pred: probabilidades preditas (0 a 1)\n\n    Returns:\n        loss: valor da perda (menor \u00e9 melhor)\n    \"\"\"\n    baixo = 1e-15\n    y_pred_clipped = np.clip(y_pred, baixo, 1 - baixo)\n\n    loss = -np.mean(y_true * np.log(y_pred_clipped) + \n                   (1 - y_true) * np.log(1 - y_pred_clipped))\n    return loss\n\ndef backward(self, X, y, cache):\n    \"\"\"    \n    Args:\n        X: dados de entrada\n        y: labels verdadeiros\n        cache: valores do forward pass\n\n    Returns:\n        gradientes de todos os par\u00e2metros\n    \"\"\"\n    m = X.shape[0]\n\n    A0, A1, A2 = cache['A0'], cache['A1'], cache['A2']\n    Z1 = cache['Z1']\n\n    y_reshaped = y.reshape(1, -1)\n    dZ2 = A2 - y_reshaped\n\n    dW2 = (1/m) * np.dot(dZ2, A1.T)  # dL/dW2\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # dL/db2\n\n    dA1 = np.dot(self.W2.T, dZ2)  # dL/dA1\n    dZ1 = dA1 * self.tanh_derivative(Z1)  # dL/dZ1\n\n    dW1 = (1/m) * np.dot(dZ1, A0.T)  # dL/dW1\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)  # dL/db1\n\n    return dW1, db1, dW2, db2\n\nMLP.compute_loss = compute_loss\nMLP.backward = backward\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#35-loop-de-treinamento-principal","title":"3.5 Loop de Treinamento Principal","text":"<p>O m\u00e9todo de treinamento coordena todo o processo de aprendizagem: executa forward passes, calcula perdas, executa backward passes e atualiza par\u00e2metros repetidamente at\u00e9 a rede convergir.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa m\u00faltiplas \u00e9pocas de treinamento</li> <li>Para cada \u00e9poca: forward \u2192 loss \u2192 backward \u2192 update</li> <li>Atualiza par\u00e2metros usando Gradient Descent</li> <li>Monitora progresso (perda e acur\u00e1cia) durante treinamento</li> </ul> <pre><code>def fit(self, X_train, y_train, epochs=150, print_every=30):\n    \"\"\"    \n    Args:\n        X_train: dados de treinamento (n_samples, n_features)\n        y_train: labels de treinamento (n_samples,)\n        epochs: n\u00famero de \u00e9pocas de treinamento\n    \"\"\"\n    self.loss_history = []\n    self.accuracy_history = []\n\n    print(f\"------ Iniciando treino ------\")\n    print(f\"\u00c9pocas: {epochs}\")\n    print(f\"Amostras de treino: {X_train.shape[0]}\")\n    print(f\"Learning rate: {self.learning_rate}\")\n\n    for epoch in range(epochs):\n        # forward pass\n        output, cache = self.forward(X_train)\n        y_pred = output.flatten()\n\n        # perda\n        loss = self.compute_loss(y_train, y_pred)\n        self.loss_history.append(loss)\n\n        # acuracia\n        predictions = (y_pred &gt; 0.5).astype(int)\n        accuracy = np.mean(predictions == y_train)\n        self.accuracy_history.append(accuracy)\n\n        # backward pass\n        dW1, db1, dW2, db2 = self.backward(X_train, y_train, cache)\n\n        # atualiza os parametros\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n\n        if (epoch + 1) % print_every == 0:\n            print(f\"\u00c9poca {epoch+1:3d}/{epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n\n    print(f\"\\n ---------- fim do treino --------\")\n    print(f\"Loss final: {self.loss_history[-1]:.6f}\")\n    print(f\"Acur\u00e1cia final no treino: {self.accuracy_history[-1]:.4f}\")\n\nMLP.fit = fit\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":""},{"location":"mlp/exercicio2/exercicio2/#41-instanciacao-e-configuracao","title":"4.1 Instancia\u00e7\u00e3o e Configura\u00e7\u00e3o","text":"<p>Agora vamos criar uma inst\u00e2ncia da nossa classe MLP e configurar os hiperpar\u00e2metros de treinamento. A escolha do learning rate \u00e9 crucial: muito alto pode causar instabilidade, muito baixo pode tornar o treinamento lento.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria uma inst\u00e2ncia do MLP</li> <li>Inicializa todos os pesos e bias automaticamente</li> <li>Prepara a rede para receber dados de treinamento</li> </ul> <pre><code>model = MLP(learning_rate=0.05)\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#42-execucao-do-treinamento","title":"4.2 Execu\u00e7\u00e3o do Treinamento","text":"<p>Este \u00e9 o momento principal onde nossa rede neural aprende os padr\u00f5es dos dados. O processo pode levar alguns segundos e voc\u00ea ver\u00e1 o progresso da perda e acur\u00e1cia sendo impresso.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa 150 \u00e9pocas de treinamento</li> <li>Mostra progresso a cada 30 \u00e9pocas</li> <li>A perda deve diminuir e a acur\u00e1cia deve aumentar ao longo do tempo</li> <li>Salva hist\u00f3rico para posterior visualiza\u00e7\u00e3o</li> </ul> <pre><code>model.fit(X_train_norm, y_train, epochs=150, print_every=30)\n\nprint(f\"Perda final: {model.loss_history[-1]:.6f}\")\nprint(f\"Acur\u00e1cia final: {model.accuracy_history[-1]:.4f}\")\n</code></pre> <pre><code>------ Iniciando treino ------\n\u00c9pocas: 150\nAmostras de treino: 800\nLearning rate: 0.05\n\u00c9poca  30/150 - Loss: 0.4429 - Acc: 0.8662\n\u00c9poca  60/150 - Loss: 0.3767 - Acc: 0.8662\n\u00c9poca  90/150 - Loss: 0.3540 - Acc: 0.8700\n\u00c9poca 120/150 - Loss: 0.3426 - Acc: 0.8712\n\u00c9poca 150/150 - Loss: 0.3358 - Acc: 0.8725\n\n ---------- fim do treino --------\nLoss final: 0.335833\nAcur\u00e1cia final no treino: 0.8725\nPerda final: 0.335833\nAcur\u00e1cia final: 0.8725\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#5-avaliacao-e-analise-dos-resultados","title":"5. Avalia\u00e7\u00e3o e An\u00e1lise dos Resultados","text":""},{"location":"mlp/exercicio2/exercicio2/#51-metricas-de-desempenho-no-conjunto-de-teste","title":"5.1 M\u00e9tricas de Desempenho no Conjunto de Teste","text":"<p>Ap\u00f3s o treinamento, precisamos avaliar como o modelo se comporta em dados que nunca viu antes (conjunto de teste). Isso nos d\u00e1 uma medida real da capacidade de generaliza\u00e7\u00e3o do modelo.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Faz predi\u00e7\u00f5es no conjunto de teste</li> <li>Calcula m\u00e9tricas abrangentes de classifica\u00e7\u00e3o</li> <li>Compara desempenho entre treino e teste</li> <li>Identifica poss\u00edvel overfitting ou underfitting</li> </ul> <pre><code>y_pred_test = model.predict(X_test_norm)\ny_pred_proba_test = model.predict_proba(X_test_norm).flatten()\n\naccuracy = accuracy_score(y_test, y_pred_test)\nprecision = precision_score(y_test, y_pred_test)\nrecall = recall_score(y_test, y_pred_test)\nf1 = f1_score(y_test, y_pred_test)\ntest_loss = model.compute_loss(y_test, y_pred_proba_test)\n\nprint(f\"Acur\u00e1cia:     {accuracy:.4f}\")\nprint(f\"Precis\u00e3o:     {precision:.4f}\")\nprint(f\"Recall:       {recall:.4f}\")\nprint(f\"F1-Score:     {f1:.4f}\")\nprint(f\"Loss (teste): {test_loss:.4f}\")\n\n# treino vs teste\ntrain_accuracy = model.accuracy_history[-1]\nprint(f\"Acur\u00e1cia Treino: {train_accuracy:.4f}\")\nprint(f\"Acur\u00e1cia Teste:  {accuracy:.4f}\")\nprint(f\"Diferen\u00e7a:       {abs(train_accuracy - accuracy):.4f}\")\n</code></pre> <p><pre><code>Acur\u00e1cia:     0.8700\nPrecis\u00e3o:     0.9022\nRecall:       0.8300\nF1-Score:     0.8646\nLoss (teste): 0.3496\nAcur\u00e1cia Treino: 0.8725\nAcur\u00e1cia Teste:  0.8700\nDiferen\u00e7a:       0.0025\n</code></pre> A diferen\u00e7a entre o treino e o teste foi bem pequena, o que mostra que o modelo esta bem generalizado</p>"},{"location":"mlp/exercicio2/exercicio2/#52-visualizacao-da-curva-de-treinamento","title":"5.2 Visualiza\u00e7\u00e3o da Curva de Treinamento","text":"<p>A visualiza\u00e7\u00e3o da evolu\u00e7\u00e3o da perda durante o treinamento nos ajuda a entender se o modelo convergiu adequadamente e se o learning rate foi apropriado.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Plota a curva de perda ao longo das \u00e9pocas</li> <li>Mostra se houve converg\u00eancia suave</li> <li>Permite identificar problemas como oscila\u00e7\u00f5es ou satura\u00e7\u00e3o</li> <li>Ajuda a determinar se mais \u00e9pocas seriam necess\u00e1rias</li> </ul> <pre><code>plt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(model.loss_history, linewidth=2, color='red', label='Loss')\nplt.title('Evolu\u00e7\u00e3o da Perda Durante o Treinamento')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(model.accuracy_history, linewidth=2, color='blue', label='Accuracy')\nplt.title('Evolu\u00e7\u00e3o da Acur\u00e1cia Durante o Treinamento')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio2/exercicio2/#53-matriz-de-confusao","title":"5.3 Matriz de Confus\u00e3o","text":"<p>A matriz de confus\u00e3o oferece uma vis\u00e3o detalhada de onde o modelo est\u00e1 acertando e errando, permitindo identificar se h\u00e1 vi\u00e9s para alguma classe espec\u00edfica.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria matriz de confus\u00e3o com visualiza\u00e7\u00e3o clara</li> <li>Mostra verdadeiros positivos, falsos positivos, etc.</li> <li>Calcula estat\u00edsticas detalhadas por classe</li> <li>Identifica padr\u00f5es de erro do modelo</li> </ul> <pre><code>cm = confusion_matrix(y_test, y_pred_test)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Classe 0', 'Classe 1'], \n            yticklabels=['Classe 0', 'Classe 1'],\n            cbar_kws={'label': 'N\u00famero de Amostras'})\nplt.title('Matriz de Confus\u00e3o - Conjunto de Teste')\nplt.ylabel('Classe Verdadeira')\nplt.xlabel('Classe Predita')\nplt.show()\n</code></pre> <p></p> <pre><code>Verdadeiros Negativos (TN): 91\nFalsos Positivos (FP):      9\nFalsos Negativos (FN):      17\nVerdadeiros Positivos (TP): 83\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#54-visualizacao-da-fronteira-de-decisao","title":"5.4 Visualiza\u00e7\u00e3o da Fronteira de Decis\u00e3o","text":"<p>Uma das visualiza\u00e7\u00f5es mais importantes \u00e9 a fronteira de decis\u00e3o, que mostra como o modelo separa as classes no espa\u00e7o de features. Isso nos d\u00e1 uma intui\u00e7\u00e3o visual de como a rede neural \"pensa\".</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria um grid denso de pontos no espa\u00e7o de features</li> <li>Calcula a probabilidade de cada ponto pertencer \u00e0 classe 1</li> <li>Visualiza a fronteira de decis\u00e3o como um mapa de calor</li> <li>Sobrep\u00f5e os pontos reais do conjunto de teste</li> </ul> <pre><code>def plot_decision_boundary(model, X, y, title=\"Fronteira de Decis\u00e3o do MLP\"):\n    \"\"\"    \n    Args:\n        model: modelo treinado\n        X: dados para plotar\n        y: labels correspondentes\n    \"\"\"\n    h = 0.02 \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    grid_normalized = (grid_points - X_train_mean) / X_train_std\n\n    Z = model.predict_proba(grid_normalized)\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(12, 8))\n\n    contour = plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.colorbar(contour, label='Probabilidade Classe 1')\n\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, \n                         edgecolors='black', s=60, alpha=0.9)\n\n    plt.xlabel('Feature 1', fontsize=12)\n    plt.ylabel('Feature 2', fontsize=12)\n    plt.title(title, fontsize=14, fontweight='bold')\n\n    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)\n\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\nplot_decision_boundary(model, X_test, y_test, \n                      \"Fronteira de Decis\u00e3o do MLP - Conjunto de Teste\")\n</code></pre> <p></p>"},{"location":"mlp/exercicio2/exercicio2/#6-analise-dos-resultados-e-insights","title":"6. An\u00e1lise dos Resultados e Insights","text":""},{"location":"mlp/exercicio2/exercicio2/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<p>O MLP com tanh nas camadas ocultas e sigmoid na sa\u00edda se adaptou bem ao padr\u00e3o dos dados. A fronteira aprendida \u00e9 suave e n\u00e3o linear, capturando varia\u00e7\u00f5es gradativas entre as classes (caracter\u00edstica da <code>tanh</code>) e entregando probabilidades calibradas via <code>sigmoid</code>.  No conjunto de teste, o modelo apresentou bom desempenho geral (acur\u00e1cia ~87% e F1 ~0.86), com leve vi\u00e9s pr\u00f3-precis\u00e3o (mais conservador para marcar a classe positiva). Em resumo, a arquitetura escolhida foi adequada ao problema e produziu uma separa\u00e7\u00e3o coerente com a distribui\u00e7\u00e3o dos pontos.</p>"},{"location":"mlp/exercicio3/exercicio3/","title":"Exerc\u00edcio 3: MLP Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#objetivo","title":"Objetivo","text":"<p>Implementar um Multi-Layer Perceptron (MLP) do zero para resolver um problema de classifica\u00e7\u00e3o multiclasse (3 classes), utilizando apenas a biblioteca NumPy para c\u00e1lculos matem\u00e1ticos. Este exerc\u00edcio demonstra na pr\u00e1tica como construir, treinar e avaliar uma rede neural artificial para problemas multiclasse sem o uso de frameworks de deep learning.</p>"},{"location":"mlp/exercicio3/exercicio3/#especificacoes-tecnicas","title":"Especifica\u00e7\u00f5es T\u00e9cnicas:","text":"<ul> <li>Dataset: 1500 amostras sint\u00e9ticas com 4 features</li> <li>Classes: 3 (classifica\u00e7\u00e3o multiclasse)</li> <li>Arquitetura: 4 \u2192 16 \u2192 3 neur\u00f4nios</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o: tanh (camada oculta) + softmax (sa\u00edda)</li> <li>Fun\u00e7\u00e3o de Perda: Categorical Cross-Entropy</li> <li>Otimizador: Gradient Descent</li> <li>Clusters por classe: 2, 3 e 4 respectivamente</li> </ul>"},{"location":"mlp/exercicio3/exercicio3/#1-configuracao-inicial-e-importacao-de-bibliotecas","title":"1. Configura\u00e7\u00e3o Inicial e Importa\u00e7\u00e3o de Bibliotecas","text":"<p>Antes de come\u00e7ar a implementa\u00e7\u00e3o, precisamos importar as bibliotecas necess\u00e1rias e configurar o ambiente de desenvolvimento. Vamos usar NumPy para opera\u00e7\u00f5es matem\u00e1ticas, Matplotlib/Seaborn para visualiza\u00e7\u00f5es e Scikit-learn apenas para gera\u00e7\u00e3o de dados sint\u00e9ticos e m\u00e9tricas de avalia\u00e7\u00e3o.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns\n\nnp.set_printoptions(precision=4, suppress=True)\nplt.style.use('default')\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#2-geracao-e-preparacao-dos-dados","title":"2. Gera\u00e7\u00e3o e Prepara\u00e7\u00e3o dos Dados","text":""},{"location":"mlp/exercicio3/exercicio3/#21-criacao-do-dataset-sintetico-multiclasse","title":"2.1 Cria\u00e7\u00e3o do Dataset Sint\u00e9tico Multiclasse","text":"<p>Vamos criar um conjunto de dados artificiais complexo para classifica\u00e7\u00e3o multiclasse. Cada classe ter\u00e1 um n\u00famero diferente de clusters para aumentar a complexidade do problema e testar a capacidade do MLP de aprender fronteiras de decis\u00e3o n\u00e3o-lineares.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Gera 1500 amostras divididas igualmente entre 3 classes (500 cada)</li> <li>Cria clusters diferentes por classe: Classe 0 (2 clusters), Classe 1 (3 clusters), Classe 2 (4 clusters)</li> <li>Desloca cada classe para regi\u00f5es diferentes do espa\u00e7o de features</li> <li>Embaralha os dados para eliminar qualquer ordena\u00e7\u00e3o</li> </ul> <pre><code>n_samples = 1500\nn_classes = 3\nn_features = 4\nn_informative = 4\nn_redundant = 0\nrandom_state = 42\n\nnp.random.seed(random_state)\n\nX_class0, y_class0 = make_classification(\n    n_samples=500, n_features=n_features, n_informative=n_informative,\n    n_redundant=n_redundant, n_clusters_per_class=2, n_classes=1,\n    random_state=random_state, class_sep=1.0\n)\ny_class0 = np.zeros(500)\n\nX_class1, y_class1 = make_classification(\n    n_samples=500, n_features=n_features, n_informative=n_informative,\n    n_redundant=n_redundant, n_clusters_per_class=3, n_classes=1,\n    random_state=random_state+1, class_sep=1.0\n)\nX_class1 = X_class1 + np.array([3, 0, 0, 3])\ny_class1 = np.ones(500)\n\nX_class2, y_class2 = make_classification(\n    n_samples=500, n_features=n_features, n_informative=n_informative,\n    n_redundant=n_redundant, n_clusters_per_class=4, n_classes=1,\n    random_state=random_state+2, class_sep=1.0\n)\nX_class2 = X_class2 + np.array([0, 3, 3, 0])\ny_class2 = np.full(500, 2)\n\nX = np.vstack([X_class0, X_class1, X_class2])\ny = np.hstack([y_class0, y_class1, y_class2])\n\nindices = np.random.permutation(n_samples)\nX, y = X[indices], y[indices]\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#22-visualizacao-dos-dados-multiclasse","title":"2.2 Visualiza\u00e7\u00e3o dos Dados Multiclasse","text":"<p>\u00c9 fundamental visualizar os dados antes de treinar qualquer modelo. Para dados com 4 features, visualizamos proje\u00e7\u00f5es 2D para entender a distribui\u00e7\u00e3o das classes e a complexidade do problema de separa\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria visualiza\u00e7\u00f5es 2D das features mais importantes</li> <li>Mostra a distribui\u00e7\u00e3o espacial das tr\u00eas classes</li> <li>Demonstra a complexidade da separa\u00e7\u00e3o multiclasse</li> </ul> <pre><code>plt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\ncolors = ['red', 'green', 'blue']\nlabels = ['Classe 0', 'Classe 1', 'Classe 2']\n\nfor i in range(3):\n    mask = y == i\n    plt.scatter(X[mask, 0], X[mask, 1], \n               c=colors[i], label=labels[i], alpha=0.7, s=20)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Dataset Multiclasse (Features 1-2)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nfor i in range(3):\n    mask = y == i\n    plt.scatter(X[mask, 2], X[mask, 3], \n               c=colors[i], label=labels[i], alpha=0.7, s=20)\n\nplt.xlabel('Feature 3')\nplt.ylabel('Feature 4')\nplt.title('Dataset Multiclasse (Features 3-4)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio3/exercicio3/#23-divisao-e-normalizacao-dos-dados","title":"2.3 Divis\u00e3o e Normaliza\u00e7\u00e3o dos Dados","text":"<p>Antes de treinar o modelo, precisamos dividir os dados em conjuntos de treino e teste, al\u00e9m de normalizar os features. A normaliza\u00e7\u00e3o \u00e9 ainda mais crucial para problemas multiclasse, pois garante que todas as features contribuam igualmente para a classifica\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Divide dados em 80% treino e 20% teste mantendo propor\u00e7\u00e3o das classes</li> <li>Normaliza os dados usando Z-score (m\u00e9dia=0, desvio=1)</li> <li>Aplica a mesma transforma\u00e7\u00e3o do treino no conjunto de teste</li> <li>Garante que todas as classes estejam representadas nos dois conjuntos</li> </ul> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# normalizacao\nX_train_mean = X_train.mean(axis=0)\nX_train_std = X_train.std(axis=0)\n\nX_train_norm = (X_train - X_train_mean) / X_train_std\nX_test_norm = (X_test - X_train_mean) / X_train_std\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#3-implementacao-da-classe-mlp-multiclasse","title":"3. Implementa\u00e7\u00e3o da Classe MLP Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#31-estrutura-principal-e-inicializacao","title":"3.1 Estrutura Principal e Inicializa\u00e7\u00e3o","text":"<p>Vamos criar uma classe espec\u00edfica para classifica\u00e7\u00e3o multiclasse que encapsula toda a funcionalidade do nosso MLP. A inicializa\u00e7\u00e3o \u00e9 adaptada para lidar com m\u00faltiplas classes de sa\u00edda, utilizando inicializa\u00e7\u00e3o Xavier para melhor converg\u00eancia.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Inicializa pesos usando Xavier/Glorot initialization</li> <li>Configura arquitetura 4\u219216\u21923 para o problema multiclasse</li> <li>Prepara estruturas para armazenar hist\u00f3rico de treinamento</li> </ul> <pre><code>class MultiClassMLP:\n\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=0.1):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n\n        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n        self.b1 = np.zeros((hidden_size, 1))\n        self.W2 = np.random.randn(num_classes, hidden_size) * np.sqrt(2.0 / hidden_size)\n        self.b2 = np.zeros((num_classes, 1))\n\n        self.loss_history = []\n        self.accuracy_history = []\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#32-funcoes-de-ativacao-para-classificacao-multiclasse","title":"3.2 Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o para Classifica\u00e7\u00e3o Multiclasse","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o adaptadas para classifica\u00e7\u00e3o multiclasse. Utilizamos tanh para a camada oculta e softmax para a sa\u00edda, que produz probabilidades normalizadas para cada classe.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa tanh e sua derivada para camada oculta</li> <li>Implementa softmax para sa\u00edda multiclasse com estabilidade num\u00e9rica</li> <li>Implementa one-hot encoding para converter labels categ\u00f3ricos</li> </ul> <pre><code>def tanh(self, z):\n    return np.tanh(z)\n\ndef tanh_derivative(self, z):\n    return 1 - np.tanh(z)**2\n\ndef softmax(self, z):\n    z_shifted = z - np.max(z, axis=0, keepdims=True)\n    exp_z = np.exp(z_shifted)\n    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\ndef one_hot_encode(self, y, num_classes):\n    one_hot = np.zeros((num_classes, len(y)))\n    one_hot[y.astype(int), np.arange(len(y))] = 1\n    return one_hot\n\nMultiClassMLP.tanh = tanh\nMultiClassMLP.tanh_derivative = tanh_derivative\nMultiClassMLP.softmax = softmax\nMultiClassMLP.one_hot_encode = one_hot_encode\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#33-forward-pass-para-classificacao-multiclasse","title":"3.3 Forward Pass para Classifica\u00e7\u00e3o Multiclasse","text":"<p>O forward pass processa os dados atrav\u00e9s da rede at\u00e9 produzir probabilidades para cada uma das tr\u00eas classes. O softmax garante que as probabilidades somem 1.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Propaga dados atrav\u00e9s das camadas da rede</li> <li>Aplica transforma\u00e7\u00f5es lineares e n\u00e3o-lineares</li> <li>Produz distribui\u00e7\u00e3o de probabilidades sobre as classes</li> <li>Armazena valores intermedi\u00e1rios para backpropagation</li> </ul> <pre><code>def forward(self, X):\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    A0 = X.T\n\n    # Camada oculta: linear + tanh\n    Z1 = self.W1 @ A0 + self.b1\n    A1 = self.tanh(Z1)\n\n    # Camada de sa\u00edda: linear + softmax\n    Z2 = self.W2 @ A1 + self.b2\n    A2 = self.softmax(Z2)\n\n    cache = {'A0': A0, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n    return A2, cache\n\ndef predict_proba(self, X):\n    output, _ = self.forward(X)\n    return output.T\n\ndef predict(self, X):\n    probabilities = self.predict_proba(X)\n    return np.argmax(probabilities, axis=1)\n\nMultiClassMLP.forward = forward\nMultiClassMLP.predict_proba = predict_proba\nMultiClassMLP.predict = predict\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#34-funcao-de-perda-e-backward-pass-multiclasse","title":"3.4 Fun\u00e7\u00e3o de Perda e Backward Pass Multiclasse","text":"<p>A fun\u00e7\u00e3o de perda Categorical Cross-Entropy \u00e9 espec\u00edfica para problemas multiclasse. O backward pass calcula gradientes considerando a natureza multiclasse do problema.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa Categorical Cross-Entropy Loss</li> <li>Calcula gradientes espec\u00edficos para classifica\u00e7\u00e3o multiclasse</li> <li>Utiliza a propriedade simplificada do gradiente softmax + cross-entropy</li> <li>Propaga erros de volta atrav\u00e9s de todas as camadas</li> </ul> <pre><code>def compute_loss(self, y_true, y_pred):\n    if y_true.ndim == 1:\n        y_true_onehot = self.one_hot_encode(y_true, self.num_classes)\n    else:\n        y_true_onehot = y_true\n\n    epsilon = 1e-15\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n    loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred_clipped), axis=0))\n    return loss\n\ndef backward(self, X, y, cache):\n    m = X.shape[0]\n    A0, A1, A2 = cache['A0'], cache['A1'], cache['A2']\n    Z1 = cache['Z1']\n\n    y_onehot = self.one_hot_encode(y, self.num_classes)\n\n    dZ2 = A2 - y_onehot\n\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(self.W2.T, dZ2)\n    dZ1 = dA1 * self.tanh_derivative(Z1)\n\n    dW1 = (1/m) * np.dot(dZ1, A0.T)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2\n\nMultiClassMLP.compute_loss = compute_loss\nMultiClassMLP.backward = backward\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#35-loop-de-treinamento-para-classificacao-multiclasse","title":"3.5 Loop de Treinamento para Classifica\u00e7\u00e3o Multiclasse","text":"<p>O m\u00e9todo de treinamento coordena todo o processo de aprendizagem para o problema multiclasse, monitorando tanto a perda quanto a acur\u00e1cia multiclasse durante o treinamento.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa m\u00faltiplas \u00e9pocas de treinamento</li> <li>Calcula acur\u00e1cia multiclasse (classe com maior probabilidade)</li> <li>Atualiza par\u00e2metros usando Gradient Descent</li> <li>Monitora converg\u00eancia durante o processo</li> </ul> <pre><code>def fit(self, X_train, y_train, epochs=200, print_every=50):\n    self.loss_history = []\n    self.accuracy_history = []\n\n    print(f\"Treinando por {epochs} \u00e9pocas...\")\n\n    for epoch in range(epochs):\n        # forward pass\n        output, cache = self.forward(X_train)\n\n        #  perda\n        loss = self.compute_loss(y_train, output)\n        self.loss_history.append(loss)\n\n        #  acur\u00e1cia\n        predictions = np.argmax(output, axis=0)\n        accuracy = np.mean(predictions == y_train)\n        self.accuracy_history.append(accuracy)\n\n        # backward pass\n        dW1, db1, dW2, db2 = self.backward(X_train, y_train, cache)\n\n        # atualiza\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n\n        if (epoch + 1) % print_every == 0:\n            print(f\"\u00c9poca {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n\n\nMultiClassMLP.fit = fit\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#4-treinamento-do-modelo-multiclasse","title":"4. Treinamento do Modelo Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#41-instanciacao-e-configuracao","title":"4.1 Instancia\u00e7\u00e3o e Configura\u00e7\u00e3o","text":"<p>Agora vamos criar uma inst\u00e2ncia da nossa classe MLP multiclasse e configurar os hiperpar\u00e2metros adequados para o problema de tr\u00eas classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria uma inst\u00e2ncia do MLP com arquitetura apropriada</li> <li>Configura 16 neur\u00f4nios na camada oculta para capturar complexidade</li> <li>Inicializa automaticamente todos os pesos e bias</li> </ul> <pre><code>input_size = X_train.shape[1] \nhidden_size = 16  \nnum_classes = 3\nlearning_rate = 0.1\n\nmodel = MultiClassMLP(\n    input_size=input_size,\n    hidden_size=hidden_size,\n    num_classes=num_classes,\n    learning_rate=learning_rate\n)\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#42-execucao-do-treinamento-multiclasse","title":"4.2 Execu\u00e7\u00e3o do Treinamento Multiclasse","text":"<p>Este \u00e9 o momento principal onde nossa rede neural aprende os padr\u00f5es complexos dos dados multiclasse. O treinamento \u00e9 mais desafiador que classifica\u00e7\u00e3o bin\u00e1ria devido \u00e0 natureza das m\u00faltiplas classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa 300 \u00e9pocas de treinamento para garantir converg\u00eancia</li> <li>Mostra progresso a cada 60 \u00e9pocas</li> <li>A perda deve diminuir e a acur\u00e1cia deve aumentar gradualmente</li> <li>Monitora aprendizagem das tr\u00eas classes simultaneamente</li> </ul> <pre><code>model.fit(X_train_norm, y_train, epochs=300, print_every=60)\n\nprint(f\"\\nPerda: {model.loss_history[0]:.4f} \u2192 {model.loss_history[-1]:.4f}\")\nprint(f\"Acur\u00e1cia: {model.accuracy_history[0]:.4f} \u2192 {model.accuracy_history[-1]:.4f}\")\n</code></pre> <pre><code>Treinando por 300 \u00e9pocas...\n\u00c9poca 60/300 - Loss: 0.6234 - Acc: 0.7450\n\u00c9poca 120/300 - Loss: 0.4892 - Acc: 0.8167\n\u00c9poca 180/300 - Loss: 0.4234 - Acc: 0.8425\n\u00c9poca 240/300 - Loss: 0.3876 - Acc: 0.8567\n\u00c9poca 300/300 - Loss: 0.3634 - Acc: 0.8658\nTreinamento conclu\u00eddo: 0.8658\n\nPerda: 1.0923 \u2192 0.3634\nAcur\u00e1cia: 0.3342 \u2192 0.8658\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#5-avaliacao-e-analise-dos-resultados-multiclasse","title":"5. Avalia\u00e7\u00e3o e An\u00e1lise dos Resultados Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#51-metricas-de-desempenho-no-conjunto-de-teste","title":"5.1 M\u00e9tricas de Desempenho no Conjunto de Teste","text":"<p>Ap\u00f3s o treinamento, avaliamos como o modelo se comporta em dados que nunca viu antes. Para problemas multiclasse, analisamos o desempenho geral e espec\u00edfico por classe.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Faz predi\u00e7\u00f5es multiclasse no conjunto de teste</li> <li>Calcula m\u00e9tricas de classifica\u00e7\u00e3o multiclasse</li> <li>Compara desempenho entre treino e teste</li> <li>Verifica capacidade de generaliza\u00e7\u00e3o</li> </ul> <pre><code>y_pred_test = model.predict(X_test_norm)\ny_pred_proba_test = model.predict_proba(X_test_norm)\n\naccuracy = accuracy_score(y_test, y_pred_test)\ntest_loss = model.compute_loss(y_test, y_pred_proba_test.T)\n\nprint(f\"Acur\u00e1cia: {accuracy:.4f} ({accuracy*100:.1f}%)\")\nprint(f\"Loss: {test_loss:.4f}\")\n\ntrain_accuracy = model.accuracy_history[-1]\nprint(f\"\\nTreino: {train_accuracy:.4f} | Teste: {accuracy:.4f}\")\nif abs(train_accuracy - accuracy) &lt; 0.05:\n    print(\"Boa generaliza\u00e7\u00e3o\")\nelse:\n    print(\"talvez overfitting\")\n</code></pre> <pre><code>Acur\u00e1cia: 0.8533 (85.3%)\nLoss: 0.3876\n\nTreino: 0.8658 | Teste: 0.8533\nBoa generaliza\u00e7\u00e3o\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#52-visualizacao-da-evolucao-do-treinamento","title":"5.2 Visualiza\u00e7\u00e3o da Evolu\u00e7\u00e3o do Treinamento","text":"<p>A visualiza\u00e7\u00e3o da evolu\u00e7\u00e3o da perda e acur\u00e1cia durante o treinamento nos ajuda a entender se o modelo convergiu adequadamente para o problema multiclasse.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Plota curvas de perda e acur\u00e1cia multiclasse</li> <li>Mostra converg\u00eancia durante o treinamento</li> <li>Permite identificar se mais \u00e9pocas seriam necess\u00e1rias</li> <li>Demonstra estabilidade do aprendizado</li> </ul> <pre><code>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# perda\nax1.plot(model.loss_history, 'r-', linewidth=2)\nax1.set_title('Evolu\u00e7\u00e3o da Perda')\nax1.set_xlabel('\u00c9poca')\nax1.set_ylabel('Cross-Entropy Loss')\nax1.grid(True, alpha=0.3)\n\n# acuracia\nax2.plot(model.accuracy_history, 'b-', linewidth=2)\nax2.set_title('Evolu\u00e7\u00e3o da Acur\u00e1cia')\nax2.set_xlabel('\u00c9poca')\nax2.set_ylabel('Acur\u00e1cia')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio3/exercicio3/#53-matriz-de-confusao-multiclasse","title":"5.3 Matriz de Confus\u00e3o Multiclasse","text":"<p>A matriz de confus\u00e3o para problemas multiclasse oferece uma vis\u00e3o detalhada de como o modelo distingue entre as tr\u00eas classes, identificando confus\u00f5es espec\u00edficas entre pares de classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria matriz de confus\u00e3o 3x3 para as tr\u00eas classes</li> <li>Mostra erros espec\u00edficos entre cada par de classes</li> <li>Calcula estat\u00edsticas detalhadas por classe</li> <li>Identifica classes mais dif\u00edceis de distinguir</li> </ul> <pre><code>cm = confusion_matrix(y_test, y_pred_test)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'], \n            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\nplt.title('Matriz de Confus\u00e3o')\nplt.ylabel('Verdadeiro')\nplt.xlabel('Predito')\nplt.show()\n\nprint(f\"\\nRelat\u00f3rio por classe:\")\nprint(classification_report(y_test, y_pred_test, \n                          target_names=[f'Classe {i}' for i in range(3)]))\n</code></pre> <p></p> <pre><code>Relat\u00f3rio por classe:\n              precision    recall  f1-score   support\n\n     Classe 0       0.85      0.88      0.86       100\n     Classe 1       0.83      0.81      0.82       100\n     Classe 2       0.88      0.87      0.88       100\n\n     accuracy                           0.85       300\n    macro avg       0.85      0.85      0.85       300\n weighted avg       0.85      0.85      0.85       300\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#54-visualizacao-das-fronteiras-de-decisao-multiclasse","title":"5.4 Visualiza\u00e7\u00e3o das Fronteiras de Decis\u00e3o Multiclasse","text":"<p>Uma das visualiza\u00e7\u00f5es mais importantes \u00e9 mostrar como o modelo separa as tr\u00eas classes no espa\u00e7o de features. Isso demonstra a capacidade do MLP de criar fronteiras de decis\u00e3o complexas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria um grid denso de pontos no espa\u00e7o das primeiras duas features</li> <li>Calcula predi\u00e7\u00f5es para cada ponto do grid</li> <li>Visualiza regi\u00f5es de decis\u00e3o para cada classe</li> <li>Sobrep\u00f5e dados reais para valida\u00e7\u00e3o visual</li> </ul> <pre><code>def plot_decision_boundary(model, X, y, title=\"Fronteiras de Decis\u00e3o\"):\n    h = 0.02\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    other_features = np.tile(X_train_norm[:, 2:].mean(axis=0), (mesh_points.shape[0], 1))\n    mesh_points_full = np.c_[mesh_points, other_features]\n\n    Z = model.predict(mesh_points_full)\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=np.arange(-0.5, 3.5, 1))\n    plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=0.5, levels=np.arange(0.5, 3, 1))\n\n    colors = ['red', 'green', 'blue']\n    for i in range(3):\n        mask = y_train == i\n        plt.scatter(X_train_norm[mask, 0], X_train_norm[mask, 1], \n                   c=colors[i], label=f'Classe {i} (treino)', alpha=0.7, s=30)\n\n    plt.xlabel('Feature 1 (normalizada)')\n    plt.ylabel('Feature 2 (normalizada)')\n    plt.title('Fronteiras de Decis\u00e3o - Dados de Treino')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(1, 2, 2)\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=np.arange(-0.5, 3.5, 1))\n    plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=0.5, levels=np.arange(0.5, 3, 1))\n\n    for i in range(3):\n        mask = y_test == i\n        plt.scatter(X_test_norm[mask, 0], X_test_norm[mask, 1], \n                   c=colors[i], label=f'Classe {i} (teste)', alpha=0.7, s=30, marker='s')\n\n    plt.xlabel('Feature 1 (normalizada)')\n    plt.ylabel('Feature 2 (normalizada)')\n    plt.title('Fronteiras de Decis\u00e3o - Dados de Teste')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_decision_boundary(model, X_train_norm, y_train)\n</code></pre> <p></p>"},{"location":"mlp/exercicio3/exercicio3/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<p>O MLP multiclasse com tanh na camada oculta e softmax na sa\u00edda demonstrou excelente capacidade de adapta\u00e7\u00e3o ao problema de tr\u00eas classes. A arquitetura 4\u219216\u21923 foi adequada para capturar a complexidade dos diferentes clusters por classe, criando fronteiras de decis\u00e3o suaves e n\u00e3o-lineares que separam efetivamente as tr\u00eas classes.</p> <p>O modelo apresentou boa generaliza\u00e7\u00e3o (diferen\u00e7a treino-teste &lt; 2%) e desempenho balanceado entre as classes (precision/recall ~85% para todas). A fun\u00e7\u00e3o softmax produziu distribui\u00e7\u00f5es de probabilidade bem calibradas, permitindo visualizar claramente as regi\u00f5es de confian\u00e7a para cada classe.</p>"},{"location":"mlp/exercicio4/exercicio4/","title":"Exerc\u00edcio 4: MLP","text":""},{"location":"mlp/exercicio4/exercicio4/#objetivo","title":"Objetivo","text":"<p>Implementar uma vers\u00e3o mais profunda do Multi-Layer Perceptron (MLP) com pelo menos 2 camadas escondidas, reutilizando e expandindo o c\u00f3digo do Exerc\u00edcio 3. Este exerc\u00edcio demonstra como redes neurais mais profundas podem capturar representa\u00e7\u00f5es mais complexas e hier\u00e1rquicas dos dados, comparando seu desempenho com arquiteturas mais simples.</p>"},{"location":"mlp/exercicio4/exercicio4/#especificacoes-tecnicas","title":"Especifica\u00e7\u00f5es T\u00e9cnicas:","text":"<ul> <li>Dataset: Mesmo do Exerc\u00edcio 3 (1500 amostras, 4 features, 3 classes)</li> <li>Arquitetura: 4 \u2192 32 \u2192 16 \u2192 3 neur\u00f4nios (2 camadas escondidas)</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o: tanh (camadas ocultas) + softmax (sa\u00edda)</li> <li>Fun\u00e7\u00e3o de Perda: Categorical Cross-Entropy</li> <li>Otimizador: Gradient Descent</li> <li>Learning Rate: 0.05 (reduzido para maior estabilidade)</li> <li>\u00c9pocas: 400 (mais treinamento devido \u00e0 complexidade)</li> </ul>"},{"location":"mlp/exercicio4/exercicio4/#1-motivacao-para-redes-mais-profundas","title":"1. Motiva\u00e7\u00e3o para Redes Mais Profundas","text":"<p>Redes neurais profundas oferecem v\u00e1rias vantagens sobre arquiteturas rasas:</p> <p>Capacidade de Representa\u00e7\u00e3o Hier\u00e1rquica:</p> <ul> <li>Primeira camada escondida (32 neur\u00f4nios): captura caracter\u00edsticas b\u00e1sicas</li> <li>Segunda camada escondida (16 neur\u00f4nios): combina caracter\u00edsticas em padr\u00f5es complexos</li> <li>Camada de sa\u00edda (3 neur\u00f4nios): classifica\u00e7\u00e3o final baseada em representa\u00e7\u00f5es hier\u00e1rquicas</li> </ul> <p>Maior Expressividade:</p> <ul> <li>Mais par\u00e2metros permitem modelar rela\u00e7\u00f5es n\u00e3o-lineares mais complexas</li> <li>Capacidade de aproximar fun\u00e7\u00f5es mais sofisticadas</li> <li>Melhor separa\u00e7\u00e3o de classes com fronteiras de decis\u00e3o mais elaboradas</li> </ul>"},{"location":"mlp/exercicio4/exercicio4/#2-implementacao-da-classe-mlp-profundo","title":"2. Implementa\u00e7\u00e3o da Classe MLP Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#21-estrutura-principal-e-inicializacao-dinamica","title":"2.1 Estrutura Principal e Inicializa\u00e7\u00e3o Din\u00e2mica","text":"<p>A nova classe <code>DeepMultiClassMLP</code> \u00e9 projetada para suportar um n\u00famero arbitr\u00e1rio de camadas escondidas, usando um sistema din\u00e2mico de inicializa\u00e7\u00e3o de par\u00e2metros.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Aceita uma lista de tamanhos para camadas escondidas (<code>hidden_sizes</code>)</li> <li>Inicializa dinamicamente pesos e bias para cada camada</li> <li>Utiliza Xavier/Glorot initialization para todas as camadas</li> <li>Armazena par\u00e2metros em dicion\u00e1rios indexados por camada</li> </ul> <pre><code>class DeepMultiClassMLP:\n\n    def __init__(self, input_size, hidden_sizes, num_classes, learning_rate=0.05):\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.num_layers = len(hidden_sizes) + 1  \n\n        self.weights = {}\n        self.biases = {}\n\n        prev_size = input_size\n        for i, hidden_size in enumerate(hidden_sizes):\n            self.weights[f'W{i+1}'] = np.random.randn(hidden_size, prev_size) * np.sqrt(2.0 / prev_size)\n            self.biases[f'b{i+1}'] = np.zeros((hidden_size, 1))\n            prev_size = hidden_size\n\n        self.weights[f'W{self.num_layers}'] = np.random.randn(num_classes, prev_size) * np.sqrt(2.0 / prev_size)\n        self.biases[f'b{self.num_layers}'] = np.zeros((num_classes, 1))\n\n        self.loss_history = []\n        self.accuracy_history = []\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#22-funcoes-de-ativacao-reutilizadas","title":"2.2 Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Reutilizadas","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o permanecem as mesmas, mas agora s\u00e3o aplicadas a m\u00faltiplas camadas escondidas. A combina\u00e7\u00e3o tanh + softmax continua sendo eficaz para classifica\u00e7\u00e3o multiclasse profunda.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Mant\u00e9m tanh para todas as camadas escondidas (preserva gradientes)</li> <li>Preserva softmax para normaliza\u00e7\u00e3o probabil\u00edstica multiclasse</li> <li>Utiliza one-hot encoding para labels categ\u00f3ricos</li> </ul> <pre><code>def tanh(self, z):\n    return np.tanh(z)\n\ndef tanh_derivative(self, z):\n    return 1 - np.tanh(z)**2\n\ndef softmax(self, z):\n    z_shifted = z - np.max(z, axis=0, keepdims=True)\n    exp_z = np.exp(z_shifted)\n    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\ndef one_hot_encode(self, y, num_classes):\n    one_hot = np.zeros((num_classes, len(y)))\n    one_hot[y.astype(int), np.arange(len(y))] = 1\n    return one_hot\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#3-forward-pass-profundo","title":"3. Forward Pass Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#31-propagacao-atraves-de-multiplas-camadas","title":"3.1 Propaga\u00e7\u00e3o Atrav\u00e9s de M\u00faltiplas Camadas","text":"<p>O forward pass generalizado percorre dinamicamente todas as camadas escondidas, aplicando transforma\u00e7\u00f5es lineares seguidas de ativa\u00e7\u00f5es tanh, antes da classifica\u00e7\u00e3o final com softmax.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Itera atrav\u00e9s de todas as camadas escondidas automaticamente</li> <li>Aplica tanh em cada camada escondida para n\u00e3o-linearidade</li> <li>Armazena todos os valores intermedi\u00e1rios para backpropagation</li> <li>Termina com softmax para distribui\u00e7\u00e3o de probabilidades multiclasse</li> </ul> <pre><code>def deep_forward(self, X):\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    cache = {}\n    A = X.T\n    cache['A0'] = A\n\n    for layer in range(1, self.num_layers):\n        Z = self.weights[f'W{layer}'] @ A + self.biases[f'b{layer}']\n        A = self.tanh(Z)\n        cache[f'Z{layer}'] = Z\n        cache[f'A{layer}'] = A\n\n    Z_out = self.weights[f'W{self.num_layers}'] @ A + self.biases[f'b{self.num_layers}']\n    A_out = self.softmax(Z_out)\n    cache[f'Z{self.num_layers}'] = Z_out\n    cache[f'A{self.num_layers}'] = A_out\n\n    return A_out, cache\n\ndef deep_predict_proba(self, X):\n    output, _ = self.forward(X)\n    return output.T\n\ndef deep_predict(self, X):\n    probabilities = self.predict_proba(X)\n    return np.argmax(probabilities, axis=1)\n\nDeepMultiClassMLP.forward = deep_forward\nDeepMultiClassMLP.predict_proba = deep_predict_proba\nDeepMultiClassMLP.predict = deep_predict\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#4-backward-pass-profundo","title":"4. Backward Pass Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#41-backpropagation-atraves-de-multiplas-camadas","title":"4.1 Backpropagation Atrav\u00e9s de M\u00faltiplas Camadas","text":"<p>O algoritmo de backpropagation \u00e9 generalizado para funcionar com qualquer n\u00famero de camadas escondidas, propagando gradientes de volta atrav\u00e9s de toda a arquitetura profunda.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Calcula gradientes come\u00e7ando pela camada de sa\u00edda</li> <li>Propaga erros de volta atrav\u00e9s de todas as camadas escondidas</li> <li>Utiliza a regra da cadeia para camadas m\u00faltiplas</li> <li>Armazena gradientes para todas as camadas em um dicion\u00e1rio</li> </ul> <pre><code>def deep_compute_loss(self, y_true, y_pred):\n    if y_true.ndim == 1:\n        y_true_onehot = self.one_hot_encode(y_true, self.num_classes)\n    else:\n        y_true_onehot = y_true\n\n    epsilon = 1e-15\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred_clipped), axis=0))\n    return loss\n\ndef deep_backward(self, X, y, cache):\n    m = X.shape[0]\n    gradients = {}\n\n    y_onehot = self.one_hot_encode(y, self.num_classes)\n\n    A_out = cache[f'A{self.num_layers}']\n    dZ = A_out - y_onehot\n\n    for layer in range(self.num_layers, 0, -1):\n        A_prev = cache[f'A{layer-1}']\n\n        gradients[f'dW{layer}'] = (1/m) * np.dot(dZ, A_prev.T)\n        gradients[f'db{layer}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n        if layer &gt; 1:\n            dA = np.dot(self.weights[f'W{layer}'].T, dZ)\n            Z_prev = cache[f'Z{layer-1}']\n            dZ = dA * self.tanh_derivative(Z_prev)\n\n    return gradients\n\nDeepMultiClassMLP.compute_loss = deep_compute_loss\nDeepMultiClassMLP.backward = deep_backward\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#5-treinamento-da-rede-profunda","title":"5. Treinamento da Rede Profunda","text":""},{"location":"mlp/exercicio4/exercicio4/#51-loop-de-treinamento-adaptado","title":"5.1 Loop de Treinamento Adaptado","text":"<p>O m\u00e9todo de treinamento \u00e9 adaptado para lidar com a maior complexidade da rede profunda, incluindo atualiza\u00e7\u00f5es din\u00e2micas de par\u00e2metros para todas as camadas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa mais \u00e9pocas devido \u00e0 maior complexidade da rede</li> <li>Atualiza dinamicamente todos os pesos e bias de todas as camadas</li> <li>Monitora converg\u00eancia com maior frequ\u00eancia</li> <li>Garante estabilidade durante o treinamento profundo</li> </ul> <pre><code>def deep_fit(self, X_train, y_train, epochs=200, print_every=50):\n    self.loss_history = []\n    self.accuracy_history = []\n\n    print(f\"Treinando rede profunda por {epochs} \u00e9pocas...\")\n\n    for epoch in range(epochs):\n        # forward pass\n        output, cache = self.forward(X_train)\n\n        # perda\n        loss = self.compute_loss(y_train, output)\n        self.loss_history.append(loss)\n\n        #  acur\u00e1cia\n        predictions = np.argmax(output, axis=0)\n        accuracy = np.mean(predictions == y_train)\n        self.accuracy_history.append(accuracy)\n\n        # backward pass\n        gradients = self.backward(X_train, y_train, cache)\n\n        # Atualiza\n        for layer in range(1, self.num_layers + 1):\n            self.weights[f'W{layer}'] -= self.learning_rate * gradients[f'dW{layer}']\n            self.biases[f'b{layer}'] -= self.learning_rate * gradients[f'db{layer}']\n\n        if (epoch + 1) % print_every == 0:\n            print(f\"\u00c9poca {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n\nDeepMultiClassMLP.fit = deep_fit\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#6-configuracao-e-treinamento-do-modelo-profundo","title":"6. Configura\u00e7\u00e3o e Treinamento do Modelo Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#61-instanciacao-da-rede-profunda","title":"6.1 Instancia\u00e7\u00e3o da Rede Profunda","text":"<p>Criamos uma inst\u00e2ncia da rede profunda com arquitetura 4\u219232\u219216\u21923, significativamente mais complexa que a vers\u00e3o do Exerc\u00edcio 3.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Define 2 camadas escondidas com 32 e 16 neur\u00f4nios respectivamente</li> <li>Reduz learning rate para 0.05 (maior estabilidade em redes profundas)</li> <li>Inicializa automaticamente todos os par\u00e2metros da arquitetura profunda</li> </ul> <pre><code>deep_hidden_sizes = [32, 16]  \ndeep_learning_rate = 0.05    \n\ndeep_model = DeepMultiClassMLP(\n    input_size=input_size,\n    hidden_sizes=deep_hidden_sizes,\n    num_classes=num_classes,\n    learning_rate=deep_learning_rate\n)\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#62-execucao-do-treinamento-profundo","title":"6.2 Execu\u00e7\u00e3o do Treinamento Profundo","text":"<p>O treinamento da rede profunda requer mais \u00e9pocas e monitoramento cuidadoso devido \u00e0 maior complexidade da otimiza\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa 400 \u00e9pocas (33% mais que o modelo simples)</li> <li>Monitora progresso a cada 80 \u00e9pocas</li> <li>Permite converg\u00eancia mais lenta mas potencialmente melhor</li> <li>Captura representa\u00e7\u00f5es hier\u00e1rquicas complexas</li> </ul> <pre><code>deep_model.fit(X_train_norm, y_train, epochs=400, print_every=80)\n\nprint(f\"  Perda: {deep_model.loss_history[0]:.4f} \u2192 {deep_model.loss_history[-1]:.4f}\")\nprint(f\"  Acur\u00e1cia: {deep_model.accuracy_history[0]:.4f} \u2192 {deep_model.accuracy_history[-1]:.4f}\")\n</code></pre> <pre><code>Perda: 1.3903 \u2192 0.0721\nAcur\u00e1cia: 0.4392 \u2192 0.9775\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#7-avaliacao-e-comparacao-de-desempenho","title":"7. Avalia\u00e7\u00e3o e Compara\u00e7\u00e3o de Desempenho","text":""},{"location":"mlp/exercicio4/exercicio4/#71-metricas-do-modelo-profundo","title":"7.1 M\u00e9tricas do Modelo Profundo","text":"<p>Avaliamos o desempenho da rede profunda no conjunto de teste e comparamos com o modelo simples do Exerc\u00edcio 3.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Calcula acur\u00e1cia e perda no conjunto de teste</li> <li>Compara m\u00e9tricas com o modelo simples</li> <li>Avalia se a complexidade adicional se justifica</li> <li>Verifica generaliza\u00e7\u00e3o da rede profunda</li> </ul> <pre><code>y_pred_deep = deep_model.predict(X_test_norm)\ny_pred_proba_deep = deep_model.predict_proba(X_test_norm)\n\naccuracy_deep = accuracy_score(y_test, y_pred_deep)\ntest_loss_deep = deep_model.compute_loss(y_test, y_pred_proba_deep.T)\n\nprint(f\"  Acur\u00e1cia: {accuracy_deep:.4f} ({accuracy_deep*100:.1f}%)\")\nprint(f\"  Loss: {test_loss_deep:.4f}\")\n</code></pre> <pre><code>Acur\u00e1cia: 0.9833 (98.3%)\nLoss: 0.0648\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#72-comparacao-visual-das-curvas-de-treinamento","title":"7.2 Compara\u00e7\u00e3o Visual das Curvas de Treinamento","text":"<p>A compara\u00e7\u00e3o lado a lado das curvas de treinamento revela as diferen\u00e7as de comportamento entre arquiteturas simples e profundas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Compara evolu\u00e7\u00e3o da perda entre modelos simples e profundo</li> <li>Mostra diferen\u00e7as na converg\u00eancia e estabilidade</li> <li>Analisa comportamento nas \u00faltimas \u00e9pocas</li> <li>Identifica vantagens e desvantagens de cada arquitetura</li> </ul> <pre><code>fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\naxes[0, 0].plot(model.loss_history, 'r-', linewidth=2, label='Exerc\u00edcio 3 (1 camada)')\naxes[0, 0].plot(deep_model.loss_history, 'b-', linewidth=2, label='Exerc\u00edcio 4 (2 camadas)')\naxes[0, 0].set_title('Compara\u00e7\u00e3o: Evolu\u00e7\u00e3o da Perda')\naxes[0, 0].set_xlabel('\u00c9poca')\naxes[0, 0].set_ylabel('Cross-Entropy Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(model.accuracy_history, 'r-', linewidth=2, label='Exerc\u00edcio 3 (1 camada)')\naxes[0, 1].plot(deep_model.accuracy_history, 'b-', linewidth=2, label='Exerc\u00edcio 4 (2 camadas)')\naxes[0, 1].set_title('Compara\u00e7\u00e3o: Evolu\u00e7\u00e3o da Acur\u00e1cia')\naxes[0, 1].set_xlabel('\u00c9poca')\naxes[0, 1].set_ylabel('Acur\u00e1cia')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\nepochs_to_show = min(100, len(model.loss_history))\naxes[1, 0].plot(model.loss_history[-epochs_to_show:], 'r-', linewidth=2, label='Exerc\u00edcio 3')\naxes[1, 0].plot(deep_model.loss_history[-epochs_to_show:], 'b-', linewidth=2, label='Exerc\u00edcio 4')\naxes[1, 0].set_title('Converg\u00eancia Final: Perda')\naxes[1, 0].set_xlabel(f'\u00daltimas {epochs_to_show} \u00c9pocas')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(model.accuracy_history[-epochs_to_show:], 'r-', linewidth=2, label='Exerc\u00edcio 3')\naxes[1, 1].plot(deep_model.accuracy_history[-epochs_to_show:], 'b-', linewidth=2, label='Exerc\u00edcio 4')\naxes[1, 1].set_title('Converg\u00eancia Final: Acur\u00e1cia')\naxes[1, 1].set_xlabel(f'\u00daltimas {epochs_to_show} \u00c9pocas')\naxes[1, 1].set_ylabel('Acur\u00e1cia')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio4/exercicio4/#73-comparacao-de-matrizes-de-confusao","title":"7.3 Compara\u00e7\u00e3o de Matrizes de Confus\u00e3o","text":"<p>A compara\u00e7\u00e3o lado a lado das matrizes de confus\u00e3o mostra como cada arquitetura lida com a confus\u00e3o entre classes espec\u00edficas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria matrizes de confus\u00e3o para ambos os modelos</li> <li>Usa cores diferentes para distinguir visualmente</li> <li>Identifica melhorias espec\u00edficas por classe</li> <li>Mostra onde a profundidade adicional ajuda</li> </ul> <pre><code>cm_deep = confusion_matrix(y_test, y_pred_deep)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'], \n            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\nax1.set_title('Exerc\u00edcio 3: Matriz de Confus\u00e3o\\n(1 camada escondida)')\nax1.set_ylabel('Verdadeiro')\nax1.set_xlabel('Predito')\n\nsns.heatmap(cm_deep, annot=True, fmt='d', cmap='Greens', ax=ax2,\n            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'], \n            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\nax2.set_title('Exerc\u00edcio 4: Matriz de Confus\u00e3o\\n(2 camadas escondidas)')\nax2.set_ylabel('Verdadeiro')\nax2.set_xlabel('Predito')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio4/exercicio4/#8-visualizacao-das-fronteiras-de-decisao-profundas","title":"8. Visualiza\u00e7\u00e3o das Fronteiras de Decis\u00e3o Profundas","text":""},{"location":"mlp/exercicio4/exercicio4/#81-fronteiras-de-decisao-da-rede-profunda","title":"8.1 Fronteiras de Decis\u00e3o da Rede Profunda","text":"<p>A visualiza\u00e7\u00e3o das fronteiras de decis\u00e3o revela como redes mais profundas podem criar separa\u00e7\u00f5es mais sofisticadas entre classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria grid de alta resolu\u00e7\u00e3o para capturar detalhes das fronteiras</li> <li>Calcula predi\u00e7\u00f5es da rede profunda para cada ponto</li> <li>Visualiza regi\u00f5es de decis\u00e3o mais complexas</li> <li>Compara implicitamente com fronteiras mais simples</li> </ul> <pre><code>def plot_deep_decision_boundary(model, X, y, title=\"Fronteiras de Decis\u00e3o\"):\n    h = 0.02  \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    other_features = np.tile(X[:, 2:].mean(axis=0), (mesh_points.shape[0], 1))\n    mesh_points_full = np.c_[mesh_points, other_features]\n\n    Z = model.predict(mesh_points_full).reshape(xx.shape)\n\n    plt.figure(figsize=(10, 6))\n\n    colors = ['red', 'green', 'blue']\n\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=np.arange(-0.5, 3.5, 1))\n    plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=0.5, levels=np.arange(0.5, 3, 1))\n\n    for i in range(3):\n        mask = y == i\n        plt.scatter(X[mask, 0], X[mask, 1], \n                   c=colors[i], label=f'Classe {i}', alpha=0.7, s=30)\n\n    plt.xlabel('Feature 1 (normalizada)')\n    plt.ylabel('Feature 2 (normalizada)')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_deep_decision_boundary(\n    deep_model, X_test_norm, y_test,\n    \"Exerc\u00edcio 4: Fronteiras de Decis\u00e3o (2 camadas escondidas)\"\n)\n</code></pre> <p></p>"},{"location":"mlp/exercicio4/exercicio4/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<p>O MLP profundo com 2 camadas escondidas demonstrou a capacidade das redes neurais profundas de capturar representa\u00e7\u00f5es hier\u00e1rquicas mais sofisticadas. A arquitetura 4\u219232\u219216\u21923 mostrou melhorias potenciais em termos de expressividade e qualidade das fronteiras de decis\u00e3o, embora com custo computacional aumentado.</p> <p>A primeira camada escondida (32 neur\u00f4nios) atua como extrator de caracter\u00edsticas b\u00e1sicas, enquanto a segunda camada (16 neur\u00f4nios) combina essas caracter\u00edsticas em representa\u00e7\u00f5es de n\u00edvel superior. Esta abordagem hier\u00e1rquica permite que o modelo aprenda padr\u00f5es mais complexos do que arquiteturas rasas.</p>"},{"location":"perceptron/main/","title":"Main","text":""},{"location":"perceptron/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"perceptron/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"perceptron/exercicio1/exercicio1/","title":"Exercicio 1 - Perceptron","text":""},{"location":"perceptron/exercicio1/exercicio1/#imports-e-configuracao-inicial","title":"Imports e Configura\u00e7\u00e3o Inicial","text":"<p>Antes de come\u00e7ar, vamos importar as bibliotecas fundamentais que usaremos ao longo de todo o exerc\u00edcio:</p> <ul> <li>NumPy: para opera\u00e7\u00f5es matem\u00e1ticas, \u00e1lgebra linear e gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios</li> <li>Matplotlib: para criar todas as visualiza\u00e7\u00f5es e gr\u00e1ficos</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-1-definicao-dos-parametros","title":"Etapa 1 \u2014 Defini\u00e7\u00e3o dos par\u00e2metros","text":"<p>Aqui eu preparei o gerador de n\u00fameros aleat\u00f3rios e defini os par\u00e2metros das normais multivariadas 2D para as duas classes (\u22121 e +1).</p> <ul> <li> <p><code>rng = np.random.default_rng(42)</code>   Criei um gerador pseudoaleat\u00f3rio com semente 42 para garantir os mesmos dados a cada execu\u00e7\u00e3o.</p> </li> <li> <p><code>mu0</code> e <code>mu1</code>   S\u00e3o os vetores de m\u00e9dias (2 dimens\u00f5es) de cada classe.  </p> </li> <li>Classe \u22121: <code>mu0 = [1.5, 1.5]</code>.  </li> <li> <p>Classe +1: <code>mu1 = [5.0, 5.0]</code>.</p> </li> <li> <p><code>cov0</code> e <code>cov1</code>   S\u00e3o as matrizes de covari\u00e2ncia (2\u00d72) que controlam o espalhamento das nuvens.  </p> </li> <li>Ambas diagonais: <code>[[0.5, 0.0], [0.0, 0.5]]</code> (vari\u00e2ncia 0.5 por eixo).  </li> </ul> <p>Esses par\u00e2metros ser\u00e3o usados na fun\u00e7\u00e3o <code>rng.multivariate_normal</code> para gerar os pontos <code>X0</code> e <code>X1</code> de cada classe na etapa seguinte.</p> <ul> <li> <p><code>y0 = -np.ones(n, dtype=int)</code> e <code>y1 = np.ones(n, dtype=int)</code>   Criei os r\u00f3tulos das classes usando a conven\u00e7\u00e3o \u22121/+1, que facilita a regra de atualiza\u00e7\u00e3o do perceptron.</p> </li> <li> <p>Empilhamento e embaralhamento <code>X = np.vstack([X0, X1])</code> e <code>y = np.hstack([y0, y1])</code> juntam os pontos e r\u00f3tulos das duas classes. <code>idx = rng.permutation(len(X))</code> \u2192 <code>X = X[idx]</code>, <code>y = y[idx]</code> embaralham os pares <code>(X, y)</code> para evitar blocos por classe.</p> </li> </ul> <p>Resultado: dataset <code>X</code> e r\u00f3tulos <code>y</code> prontos, balanceados e embaralhados, para o treino do perceptron.</p> <pre><code>rng = np.random.default_rng(42)\n\nn = 1000\n\nmu0 = np.array([1.5, 1.5], dtype=float)\nmu1 = np.array([5.0, 5.0], dtype=float)\n\ncov0 = np.array([[0.5, 0.0],[0.0, 0.5]], dtype=float)\ncov1 = np.array([[0.5, 0.0],[0.0, 0.5]], dtype=float)\n\nX0 = rng.multivariate_normal(mean=mu0, cov=cov0, size=n)\nX1 = rng.multivariate_normal(mean=mu1, cov=cov1, size=n)\n\ny0 = -np.ones(n, dtype=int)\ny1 =  np.ones(n, dtype=int)\n\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])\n\nidx = rng.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-2-visualizacao-inicial-dos-dados","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o inicial dos dados","text":"<p>Aqui eu plotei a distribui\u00e7\u00e3o dos pontos por classe para inspecionar rapidamente o dataset antes do treino.</p> <ul> <li><code>mask_pos</code> e <code>mask_neg</code> separam os \u00edndices das amostras da classe +1 e classe \u22121.</li> <li>Desenhei dois gr\u00e1ficos de dispers\u00e3o (um por classe), usando marcadores diferentes.</li> </ul> <p>Objetivo: verificar forma, espalhamento e poss\u00edvel sobreposi\u00e7\u00e3o entre as classes, validando a separabilidade linear esperada para o perceptron.</p> <pre><code>mask_pos = y == 1\nmask_neg = y == -1\n\nplt.figure()\nplt.scatter(X[mask_neg, 0], X[mask_neg, 1], label=\"Classe -1\", marker=\"o\", alpha=0.7)\nplt.scatter(X[mask_pos, 0], X[mask_pos, 1], label=\"Classe +1\", marker=\"s\", alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Distribui\u00e7\u00e3o dos dados por classe\")\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-3-idealizacao-do-perceptron","title":"Etapa 3 - Idealiza\u00e7\u00e3o do Perceptron","text":"<p>Resumo: O Perceptron \u00e9 uma rede neural de uma \u00fanica camada (um \u00fanico neur\u00f4nio). Ele recebe um vetor de entradas, calcula uma soma ponderada usando pesos e um vi\u00e9s (bias), e aplica uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o em degrau para produzir uma sa\u00edda bin\u00e1ria.</p> <p></p>"},{"location":"perceptron/exercicio1/exercicio1/#31-componentes-da-figura","title":"3.1 Componentes da figura","text":"<ul> <li>Entradas <code>x\u2081, x\u2082, \u2026, x_m</code> (Input): caracter\u00edsticas do exemplo a classificar.  </li> <li>Pesos <code>w\u2081, w\u2082, \u2026, w_m</code> (Weight): import\u00e2ncia de cada entrada.  </li> <li>Somat\u00f3rio <code>\u03a3</code> (Network input function): calcula a soma ponderada.  </li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o (Activation function): converte o n\u00famero real em decis\u00e3o bin\u00e1ria.  </li> <li>Sa\u00edda (Output): r\u00f3tulo previsto.</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#32-equacoes","title":"3.2 Equa\u00e7\u00f5es","text":"<ol> <li> <p>Soma ponderada (potencial de ativa\u00e7\u00e3o)    $$    a \\;=\\; \\sum_{i=1}^{m} w_i\\,x_i \\;+\\; b \\;=\\; w \\cdot x \\;+\\; b    $$</p> </li> <li> <p>Ativa\u00e7\u00e3o (degrau) </p> </li> <li>Forma 0/1:      $$      \\text{activation}(a) =      \\begin{cases}        1, &amp; a \\ge 0 \\        0, &amp; a &lt; 0      \\end{cases}      $$</li> <li> <p>Forma \u22121/+1 (a que usaremos):      $$      \\hat{y} =      \\begin{cases}        +1, &amp; a \\ge 0 \\        -1, &amp; a &lt; 0      \\end{cases}      $$</p> </li> <li> <p>Regra de decis\u00e3o (equivalente)    $$    \\hat{y} = \\mathrm{sign}(w \\cdot x + b)    $$</p> </li> </ol>"},{"location":"perceptron/exercicio1/exercicio1/#33-interpretacao-geometrica-por-que-e-linear","title":"3.3 Interpreta\u00e7\u00e3o geom\u00e9trica (por que \u00e9 linear)","text":"<ul> <li>A fronteira de decis\u00e3o \u00e9 o conjunto de pontos que satisfaz   $$   w \\cdot x + b = 0.   $$</li> <li>Em 2D, essa fronteira \u00e9 uma reta(nosso caso); em dimens\u00f5es maiores, um hiperplano.  </li> <li>O vetor w \u00e9 perpendicular \u00e0 fronteira; o bias \\(b\\) desloca a reta/hiperplano.</li> </ul> <p>Pr\u00f3xima etapa: veremos como o perceptron aprende \u2014 calculando o erro entre \\(\\hat{y}\\) e o r\u00f3tulo verdadeiro \\(y\\) e ajustando \\(w\\) e \\(b\\) com a regra de atualiza\u00e7\u00e3o.</p>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-4-implementacao-do-perceptron","title":"Etapa 4 \u2014 Implementa\u00e7\u00e3o do Perceptron","text":""},{"location":"perceptron/exercicio1/exercicio1/#41-funcao-degrau-step","title":"4.1 Fun\u00e7\u00e3o degrau (step)","text":"<p>Ideia: Dado um escalar ou vetor <code>z</code>, retornamos +1 quando <code>z \u2265 0</code> e \u22121 quando <code>z &lt; 0</code>. Isso corresponde \u00e0 fun\u00e7\u00e3o de ativa\u00e7\u00e3o do perceptron.</p> <p>F\u00f3rmula:</p> \\[ \\text{sign}(z) = \\begin{cases} +1, &amp; z \\geq 0 \\\\ -1, &amp; z &lt; 0 \\end{cases} \\] <pre><code>def step_sign(z):\n    \"\"\"  \n    Par\u00e2metros:\n    - z: potencial de ativa\u00e7\u00e3o\n\n    Retorna:\n    - +1 para valores \u2265 0, -1 para valores &lt; 0\n    \"\"\"\n    return np.where(z &gt;= 0, 1, -1)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#42-predicao-forward-pass","title":"4.2 Predi\u00e7\u00e3o (forward pass)","text":"<p>Ideia: Para cada amostra <code>x</code>, calculamos o potencial de ativa\u00e7\u00e3o <code>a = w \u00b7 x + b</code> e aplicamos a fun\u00e7\u00e3o degrau. Se <code>a \u2265 0</code> \u2192 classe +1; caso contr\u00e1rio \u2192 \u22121.</p> <p>F\u00f3rmula</p> \\[ \\hat{y} = \\text{sign}(w \\cdot x + b) \\] <p>Par\u00e2metros:</p> <ul> <li> <p>X: <code>matriz de features (n_amostras, n_dimens\u00f5es)</code></p> </li> <li> <p>w: <code>vetor de pesos (n_dimens\u00f5es,)</code></p> </li> <li> <p>b: <code>bias (escalar)</code></p> </li> </ul> <p>Retorna:</p> <ul> <li>vetor de predi\u00e7\u00f5es {-1, +1} para cada amostra</li> </ul> <pre><code>def predict(X, w, b):\n    return step_sign(X @ w + b)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#43-treinamento-aprendizado-com-o-erro","title":"4.3 Treinamento (aprendizado com o erro)","text":"<p>Ideia central: percorrer os dados v\u00e1rias vezes (\u00e9pocas); para cada amostra <code>x_i</code> com r\u00f3tulo <code>y_i \u2208 {\u22121, +1}</code>, se errou (isto \u00e9, <code>y_i (w \u00b7 x_i + b) \u2264 0</code>), ajuste os par\u00e2metros:</p> <p>Regra de atualiza\u00e7\u00e3o</p> \\[ w \\leftarrow w + \\eta y_i x_i, \\quad\\quad b \\leftarrow b + \\eta y_i \\] <ul> <li>eta \u00e9 a taxa de aprendizado.  </li> <li>Embaralhamos as amostras a cada \u00e9poca (opcional) para evitar ciclos.  </li> <li>Guardamos hist\u00f3rico de acur\u00e1cia e n\u00ba de atualiza\u00e7\u00f5es para avaliar a converg\u00eancia.</li> </ul> Fun\u00e7\u00e3o - train_perceptron <pre><code>def train_perceptron(X, y, eta=0.01, max_epochs=100, shuffle=True):\n    \"\"\"    \n    Para cada amostra (x\u1d62, y\u1d62) classificada incorretamente:\n    w \u2190 w + \u03b7\u00b7y\u1d62\u00b7x\u1d62    (atualiza\u00e7\u00e3o dos pesos)\n    b \u2190 b + \u03b7\u00b7y\u1d62      (atualiza\u00e7\u00e3o do bias)\n\n    Uma amostra \u00e9 considerada incorreta se: y\u1d62\u00b7(w\u00b7x\u1d62 + b) \u2264 0\n\n    Par\u00e2metros:\n    - X: matriz de features (n_amostras, n_dimens\u00f5es)\n    - y: vetor de r\u00f3tulos {-1, +1}\n    - eta: taxa de aprendizado\n    - max_epochs: n\u00famero m\u00e1ximo de \u00e9pocas\n    - shuffle: embaralhar amostras a cada \u00e9poca\n\n    Retorna:\n    - w: pesos finais\n    - b: bias final\n    - history: hist\u00f3rico de treinamento (acur\u00e1cia, atualiza\u00e7\u00f5es, par\u00e2metros)\n    - y_hat: predi\u00e7\u00f5es finais\n    \"\"\"\n    n, d = X.shape\n\n    np.random.seed(42)\n    w = np.random.uniform(-2, 2, d)      \n    b = np.random.uniform(-5, 5)        \n\n    history = []\n\n    # percorrer \u00e9pocas\n    for epoch in range(1, max_epochs + 1):\n        updates = 0\n\n        if shuffle:\n            idx = np.random.permutation(n)\n            X_epoch, y_epoch = X[idx], y[idx]\n        else:\n            X_epoch, y_epoch = X, y\n\n        # PERCORRER CADA AMOSTRA NA \u00c9POCA\n        for xi, yi in zip(X_epoch, y_epoch):\n            # potencial de ativa\u00e7\u00e3o: a = w\u00b7x + b\n            z = np.dot(w, xi) + b\n\n            # CRIT\u00c9RIO DE ERRO: yi * z &lt;= 0\n            # Se yi=+1 e z&lt;0: predi\u00e7\u00e3o errada (-1), precisa corrigir\n            # Se yi=-1 e z&gt;0: predi\u00e7\u00e3o errada (+1), precisa corrigir\n            if yi * z &lt;= 0:  \n                # REGRA DE ATUALIZA\u00c7\u00c3O\n                w += eta * yi * xi  # w \u2190 w + \u03b7\u00b7yi\u00b7xi\n                b += eta * yi       # b \u2190 b + \u03b7\u00b7yi\n                updates += 1\n\n        # performance da epoca\n        y_hat = predict(X, w, b)\n        acc = (y_hat == y).mean()  # acuracia\n\n        history.append({\n            \"epoch\": epoch,\n            \"accuracy\": acc,\n            \"updates\": updates,\n            \"w\": w.copy(),\n            \"b\": b\n        })\n\n        # converg\u00eancia\n        if updates == 0:\n            print(f\"Converg\u00eancia alcan\u00e7ada na \u00e9poca {epoch}\")\n            break\n\n    return w, b, history, y_hat\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#44-checagens","title":"4.4 Checagens","text":"<p>Acur\u00e1cia simples:</p> <pre><code>def accuracy(y_true, y_pred):\n    return np.mean(y_true == y_pred)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-5-treinamento-do-perceptron-com-os-dados-do-exercicio","title":"Etapa 5 \u2014 Treinamento do Perceptron com os Dados do Exerc\u00edcio","text":"<p>Objetivo: aplicar o perceptron implementado nos dados gerados na Etapa 1:</p> <ul> <li>Taxa de aprendizado \u03b7 = 0.01</li> <li>M\u00e1ximo de 100 \u00e9pocas</li> <li>Parada antecipada por converg\u00eancia</li> <li>Rastreamento da acur\u00e1cia ap\u00f3s cada \u00e9poca</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#51-treinamento-e-resultados","title":"5.1 Treinamento e resultados","text":"<pre><code>w_final, b_final, history, y_pred_final = train_perceptron(\n    X, y, \n    eta=0.001,       \n    max_epochs=100,  \n    shuffle=True     \n)\n\n\n# metricas\nfinal_accuracy = accuracy(y, y_pred_final)  \nfinal_epoch = history[-1][\"epoch\"]         \ntotal_updates = sum([h[\"updates\"] for h in history]) \n\nprint(\"=== RESULTADOS FINAIS ===\")\nprint(f\"Pesos finais: w = [{w_final[0]:.4f}, {w_final[1]:.4f}]\")\nprint(f\"Vi\u00e9s final: b = {b_final:.4f}\")\nprint(f\"Acur\u00e1cia final: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"Total de atualiza\u00e7\u00f5es durante todo o treinamento: {total_updates}\")\n\n# erros\nmisclassified_mask = (y != y_pred_final)  \nn_misclassified = np.sum(misclassified_mask)\nprint(f\"Pontos mal classificados: {n_misclassified} de {len(y)} ({n_misclassified/len(y)*100:.2f}%)\")\n</code></pre> <pre><code>Converg\u00eancia alcan\u00e7ada na \u00e9poca 16!\n\n=== RESULTADOS FINAIS ===\nPesos finais: w = [0.0205, 0.0451]\nVi\u00e9s final: b = -0.2251\nAcur\u00e1cia final: 1.0000 (100.00%)\nTotal de atualiza\u00e7\u00f5es durante todo o treinamento: 4341\nPontos mal classificados: 0 de 2000 (0.00%)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-6-visualizacao-dos-resultados","title":"Etapa 6 \u2014 Visualiza\u00e7\u00e3o dos Resultados","text":"<p>Objetivo: criar visualiza\u00e7\u00f5es claras e informativas que mostrem:</p> <ol> <li>Evolu\u00e7\u00e3o da acur\u00e1cia durante o treinamento</li> <li>Fronteira de decis\u00e3o aprendida pelo perceptron</li> <li>Pontos mal classificados</li> </ol>"},{"location":"perceptron/exercicio1/exercicio1/#61-curva-de-acuracia-por-epoca","title":"6.1 Curva de acur\u00e1cia por \u00e9poca","text":"<p>A primeira visualiza\u00e7\u00e3o importante \u00e9 observar como a acur\u00e1cia evolui durante o treinamento. Isso nos mostra:</p> <ol> <li>Velocidade de converg\u00eancia: quantas \u00e9pocas o perceptron precisou para aprender</li> <li>Estabilidade: se a acur\u00e1cia melhora consistentemente ou oscila</li> <li>Padr\u00e3o de atualiza\u00e7\u00f5es: como o n\u00famero de corre\u00e7\u00f5es diminui \u00e0 medida que o modelo aprende</li> </ol> <p></p> <p>Os gr\u00e1ficos indicam converg\u00eancia r\u00e1pida e est\u00e1vel do perceptron:</p> <ul> <li>Velocidade de converg\u00eancia: a acur\u00e1cia sai de ~0,4 nas primeiras \u00e9pocas, d\u00e1 um salto por volta da \u00e9poca 4 e atinge 1,0 entre as \u00e9pocas 5\u20136, mantendo-se em plat\u00f4 at\u00e9 o fim.</li> <li>Estabilidade: ap\u00f3s atingir 100%, a acur\u00e1cia permanece constante, sugerindo que n\u00e3o h\u00e1 oscila\u00e7\u00f5es causadas por taxa de aprendizado alta ou por dados ruidosos.</li> <li>Padr\u00e3o de atualiza\u00e7\u00f5es: o n\u00famero de corre\u00e7\u00f5es come\u00e7a muito alto (~1,1k), cai abruptamente ap\u00f3s a \u00e9poca 4 e tende a zero a partir da 6\u00aa \u2014 sinal de que o hiperplano j\u00e1 separa corretamente todas as amostras.</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#62-visualizacao-da-fronteira-de-decisao-final","title":"6.2 Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o final","text":"<p>Agora vamos criar uma visualiza\u00e7\u00e3o detalhada do resultado final, mostrando:</p> <ul> <li>Pontos corretamente classificados: marcados com s\u00edmbolos normais</li> <li>Pontos mal classificados: destacados com um 'X' para f\u00e1cil identifica\u00e7\u00e3o  </li> <li>Fronteira de decis\u00e3o: a linha aprendida pelo perceptron</li> <li>Equa\u00e7\u00e3o da fronteira: mostrando os valores finais de w\u2081, w\u2082 e b</li> </ul> <p>Esta visualiza\u00e7\u00e3o nos permite avaliar visualmente a qualidade da separa\u00e7\u00e3o:</p> <p></p> Fronteira de Decis\u00e3o <pre><code># Fun\u00e7\u00e3o para plotar fronteira de decis\u00e3o\ndef plot_decision_boundary(X, y, w, b, title=\"Fronteira de Decis\u00e3o\"):\n    plt.figure(figsize=(10, 8))\n\n    # Separar pontos por classe\n    mask_pos = y == 1\n    mask_neg = y == -1\n\n    # Plot dos pontos corretamente classificados\n    correct_mask = (y == predict(X, w, b))\n\n    plt.scatter(X[mask_neg &amp; correct_mask, 0], X[mask_neg &amp; correct_mask, 1], \n            c='blue', marker='o', alpha=0.7, s=30, label=\"Classe -1 (correto)\")\n    plt.scatter(X[mask_pos &amp; correct_mask, 0], X[mask_pos &amp; correct_mask, 1], \n            c='red', marker='s', alpha=0.7, s=30, label=\"Classe +1 (correto)\")\n\n    # Plot dos pontos mal classificados (se existirem)\n    if np.any(~correct_mask):\n        plt.scatter(X[mask_neg &amp; ~correct_mask, 0], X[mask_neg &amp; ~correct_mask, 1], \n                c='blue', marker='x', s=100, linewidth=3, label=\"Classe -1 (ERRO)\")\n        plt.scatter(X[mask_pos &amp; ~correct_mask, 0], X[mask_pos &amp; ~correct_mask, 1], \n                c='red', marker='x', s=100, linewidth=3, label=\"Classe +1 (ERRO)\")\n\n    # Calcular e plotar a linha de decis\u00e3o\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n\n    if abs(w[1]) &gt; 1e-10:\n        x1_line = np.array([x1_min, x1_max])\n        x2_line = -(w[0] * x1_line + b) / w[1]\n        plt.plot(x1_line, x2_line, 'k-', linewidth=2, label=f\"Fronteira: {w[0]:.3f}x\u2081 + {w[1]:.3f}x\u2082 + {b:.3f} = 0\")\n\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.axis('equal')\n\n    return plt.gca()\n\nplot_decision_boundary(X, y, w_final, b_final, \n                    f\"Resultado Final do Perceptron (\u00c9poca {final_epoch})\")\nplt.show()\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#63-evolucao-do-perceptron-por-epoca","title":"6.3 Evolu\u00e7\u00e3o do Perceptron por \u00c9poca","text":"<p>O painel mostra, \u00e9poca a \u00e9poca, como o perceptron aprende a separar duas classes (azul = \u22121, vermelho = +1). Em cada subgr\u00e1fico aparece:</p> <ul> <li>Fronteira de decis\u00e3o (reta preta) definida por \\(w\\cdot x + b = 0\\).</li> <li>Pontos das classes e a acur\u00e1cia obtida naquela \u00e9poca.</li> <li>A cada \u00e9poca, os pesos \\(w\\) e o bias \\(b\\) s\u00e3o ajustados a partir das amostras mal classificadas (regra do perceptron), fazendo a reta girar (mudan\u00e7a em \\(w\\)) e transladar (mudan\u00e7a em \\(b\\)) at\u00e9 atingir a separa\u00e7\u00e3o correta.</li> </ul> <p>O que observar:</p> <ul> <li>Orienta\u00e7\u00e3o da reta: o modelo explora diferentes dire\u00e7\u00f5es (sinal e magnitude dos pesos), buscando a que melhor separa os grupos.</li> <li>Posicionamento da reta: o bias desloca a fronteira para cima/baixo, refinando a divis\u00e3o.</li> <li>Acur\u00e1cia por \u00e9poca: indica o progresso do aprendizado e o qu\u00e3o perto o modelo est\u00e1 do resultado esperado.</li> <li>Estabiliza\u00e7\u00e3o: quando n\u00e3o h\u00e1 mais corre\u00e7\u00f5es necess\u00e1rias, a reta para de se mover e a acur\u00e1cia se mant\u00e9m.</li> </ul> <p></p>"},{"location":"perceptron/exercicio1/exercicio1/#64-conclusoes-a-partir-do-grafico","title":"6.4 Conclus\u00f5es a partir do gr\u00e1fico","text":"<p>O painel mostra um aprendizado progressivo e est\u00e1vel do perceptron:</p> <ul> <li>Arranque e salto de desempenho. A acur\u00e1cia parte baixa (\u2248 0,39 na \u00e9poca 1), evolui lentamente nas \u00e9pocas 2\u20133, e d\u00e1 um salto na \u00e9poca 4 (\u2248 0,73).  </li> <li>Quase perfeito cedo. J\u00e1 na \u00e9poca 5 o modelo atinge \u2248 0,9895, e entre as \u00e9pocas 6\u20138 estabiliza acima de 0,994.</li> <li>Ajustes finos com pequenas oscila\u00e7\u00f5es. Entre as \u00e9pocas 9\u201314 h\u00e1 microvaria\u00e7\u00f5es (ex.: \u00e9poca 11 \u2248 0,9950) t\u00edpicas do ajuste online em pontos pr\u00f3ximos \u00e0 fronteira; em seguida, a acur\u00e1cia volta a 0,999+.</li> <li>Converg\u00eancia. A partir das \u00e9pocas 15\u201316 a acur\u00e1cia chega a 1,0000, indicando nenhum erro no conjunto de treino.</li> <li>Geometria da fronteira. A reta inicia com inclina\u00e7\u00e3o positiva, depois gira e assume inclina\u00e7\u00e3o negativa, evidenciando a corre\u00e7\u00e3o do vetor de pesos e o deslocamento via bias at\u00e9 alinhar a separa\u00e7\u00e3o aos dados.</li> <li>Diagn\u00f3stico dos dados. O alcance de 100% sem instabilidades sugere dados linearmente separ\u00e1veis e taxa de aprendizado adequada (sem oscila\u00e7\u00f5es amplas).</li> </ul>"},{"location":"perceptron/exercicio2/exercicio2/","title":"Exerc\u00edcio 2 - Perceptron","text":"<p>O objetivo desse exercicio \u00e9 mostrar como esse modelo de perceptron age para um dataset muito mais complicado. Al\u00e9m disso, vamos refletir sobre poss\u00edveis saidas para esse problema.</p>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-1-geracao-do-conjunto-com-overlap-dados-de-treino","title":"Etapa 1 \u2014 Gera\u00e7\u00e3o do conjunto com overlap (dados de treino)","text":"<p>Objetivo do bloco: criar duas classes 2D com m\u00e9dias pr\u00f3ximas e vari\u00e2ncia alta para induzir overlap (n\u00e3o separabilidade linear perfeita), j\u00e1 com r\u00f3tulos em {-1, +1} e dados embaralhados no final.</p> <p>O que o c\u00f3digo faz:</p> <ul> <li><code>rng2 = np.random.default_rng(42)</code>: fixa a semente para reprodutibilidade.</li> <li><code>n2 = 1000</code>: define 1.000 amostras por classe (total = 2.000).</li> <li><code>mu0_ex2</code>, <code>mu1_ex2</code>: m\u00e9dias das classes [-1] \u2192 [3, 3] e [+1] \u2192 [4, 4].</li> <li><code>sigma2 = 1.5</code> e <code>cov_ex2 = [[1.5, 0], [0, 1.5]]</code>: covari\u00e2ncia isotr\u00f3pica com vari\u00e2ncia 1.5 em cada eixo.</li> <li><code>X0_ex2</code>, <code>X1_ex2</code>: amostragem multivariada normal de cada classe.</li> <li><code>y0_ex2 = -1</code>, <code>y1_ex2 = +1</code>: r\u00f3tulos.</li> <li><code>X_ex2 = vstack(...)</code>, <code>y_ex2 = hstack(...)</code>: concatena as duas classes em um \u00fanico conjunto.</li> <li><code>idx2 = rng2.permutation(...)</code> e reindexa\u00e7\u00e3o: embaralha as amostras para n\u00e3o ficarem em blocos por classe.</li> </ul> <p>Sa\u00eddas esperadas:</p> <ul> <li><code>X_ex2</code> com shape (2000, 2) e <code>y_ex2</code> com shape (2000,).</li> <li>Distribui\u00e7\u00e3o com overlap vis\u00edvel \u2014 ideal para avaliar as limita\u00e7\u00f5es do Perceptron em dados n\u00e3o totalmente separ\u00e1veis.</li> </ul> <pre><code>rng2 = np.random.default_rng(42)\n\nn2 = 1000 \n\nmu0_ex2 = np.array([3.0, 3.0])  \nmu1_ex2 = np.array([4.0, 4.0])\n\nsigma2 = 1.5\ncov_ex2 = np.array([[sigma2, 0.0], [0.0, sigma2]])\n\nX0_ex2 = rng2.multivariate_normal(mu0_ex2, cov_ex2, n2)\nX1_ex2 = rng2.multivariate_normal(mu1_ex2, cov_ex2, n2)\n\ny0_ex2 = -np.ones(n2, dtype=int)\ny1_ex2 = np.ones(n2, dtype=int)\n\nX_ex2 = np.vstack([X0_ex2, X1_ex2])\ny_ex2 = np.hstack([y0_ex2, y1_ex2])\n\nidx2 = rng2.permutation(len(X_ex2))\nX_ex2 = X_ex2[idx2]\ny_ex2 = y_ex2[idx2]\n</code></pre>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-2-visualizacao-inicial-das-classes","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o inicial das classes","text":"<p>Vamos plotar um gr\u00e1fico de dispers\u00e3o (x\u2081 vs x\u2082) separando as amostras por classe (\u22121 em azul, +1 em vermelho). Essa visualiza\u00e7\u00e3o serve para:</p> <ul> <li>Confirmar o overlap entre as distribui\u00e7\u00f5es (as nuvens se interpenetram).</li> <li>Ver forma e dispers\u00e3o dos clusters (vari\u00e2ncia maior, sem correla\u00e7\u00e3o entre eixos).</li> <li>Antecipar o comportamento do Perceptron: como a separa\u00e7\u00e3o n\u00e3o \u00e9 perfeita, a fronteira linear dever\u00e1 resultar em erros residuais mesmo ap\u00f3s o treino.</li> </ul> <p>Esta etapa \u00e9 o \u201cantes do treino\u201d: \u00e9 nossa linha de base visual para comparar depois com a fronteira aprendida.</p> <pre><code>plt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 2)\nmask_pos_ex2 = y_ex2 == 1\nmask_neg_ex2 = y_ex2 == -1\nplt.scatter(X_ex2[mask_neg_ex2, 0], X_ex2[mask_neg_ex2, 1], c='blue', alpha=0.6, s=20, label=\"Classe -1\")\nplt.scatter(X_ex2[mask_pos_ex2, 0], X_ex2[mask_pos_ex2, 1], c='red', alpha=0.6, s=20, label=\"Classe +1\")\nplt.title(\"Dispers\u00e3o das classes -1 e +1\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>An\u00e1lise do scatter</p> <ul> <li>As nuvens azul (\u22121) e vermelha (+1) est\u00e3o fortemente sobrepostas no miolo do plano \u2014 coerente com m\u00e9dias pr\u00f3ximas (\u2248[3,3] e [4,4]) e vari\u00e2ncia alta (1.5).  </li> <li>H\u00e1 uma tend\u00eancia da classe +1 ocupar valores um pouco maiores de \\(x_1\\) e \\(x_2\\), mas sem um \u201cgap\u201d limpo.  </li> <li>Consequ\u00eancia pr\u00e1tica: um limite linear dever\u00e1 separar \u201cem m\u00e9dia\u201d as classes, mas erros residuais s\u00e3o inevit\u00e1veis \u2014 esperamos acur\u00e1cia &lt; 100% e atualiza\u00e7\u00f5es persistentes durante o treino do Perceptron.</li> </ul>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-3-treinamento-do-perceptron","title":"Etapa 3 \u2014 Treinamento do Perceptron","text":"<p>Aqui treinamos o modelo reutilizando exatamente a fun\u00e7\u00e3o <code>train_perceptron</code> do Exerc\u00edcio 1 sobre o conjunto com overlap:</p> <ul> <li><code>w_ex2, b_ex2, history_ex2, y_pred_ex2 = train_perceptron(X_ex2, y_ex2, eta=0.001, max_epochs=100, shuffle=True)</code> </li> <li><code>X_ex2</code>, <code>y_ex2</code>: dados gerados na etapa anterior.  </li> <li><code>eta=0.001</code>: taxa de aprendizado menor para estabilizar a aprendizagem em dados n\u00e3o totalmente separ\u00e1veis.  </li> <li><code>max_epochs=100</code>: limite superior de \u00e9pocas.  </li> <li><code>shuffle=True</code>: embaralha as amostras a cada \u00e9poca, o que ajuda a evitar ciclos em cen\u00e1rios com overlap.  </li> <li>Sa\u00eddas:  <ul> <li><code>w_ex2</code>, <code>b_ex2</code>: pesos e vi\u00e9s aprendidos;  </li> <li><code>history_ex2</code>: lista com m\u00e9tricas por \u00e9poca (<code>accuracy</code>, <code>updates</code>, <code>w</code>, <code>b</code>);  </li> <li><code>y_pred_ex2</code>: predi\u00e7\u00f5es finais no treino ({-1, +1}).</li> </ul> </li> </ul> <p>Em seguida calculamos as m\u00e9tricas:</p> <ul> <li><code>final_acc_ex2 = accuracy(y_ex2, y_pred_ex2)</code>: acur\u00e1cia final no conjunto de treino (mesma fun\u00e7\u00e3o de utilit\u00e1rio usada no Exerc\u00edcio 1).  </li> <li><code>final_epoch_ex2 = history_ex2[-1][\"epoch\"]</code>: \u00faltima \u00e9poca executada (serve para inferir se bateu o limite de 100).  </li> <li><code>total_updates_ex2 = sum(h[\"updates\"] for h in history_ex2)</code>: total de atualiza\u00e7\u00f5es (quantas corre\u00e7\u00f5es ocorreram ao longo de todo o treinamento).</li> </ul> <p>Lembrando: como h\u00e1 overlap, \u00e9 esperado que a acur\u00e1cia n\u00e3o atinja 100% e que possamos n\u00e3o zerar atualiza\u00e7\u00f5es antes de chegar a <code>max_epochs</code>.</p> <pre><code>w_ex2, b_ex2, history_ex2, y_pred_ex2 = train_perceptron(\n    X_ex2, y_ex2, \n    eta=0.001,      \n    max_epochs=100, \n    shuffle=True\n)\n\nfinal_acc_ex2 = accuracy(y_ex2, y_pred_ex2)\nfinal_epoch_ex2 = history_ex2[-1][\"epoch\"]\ntotal_updates_ex2 = sum([h[\"updates\"] for h in history_ex2])\n\nprint(\"RESULTADOS:\")\nprint(f\"   Convergiu? {'N\u00c3O' if final_epoch_ex2 &gt;= 100 else 'SIM'}\")\nprint(f\"   \u00c9pocas usadas: {final_epoch_ex2}/100\")\nprint(f\"   Acur\u00e1cia final: {final_acc_ex2:.3f} ({final_acc_ex2*100:.1f}%)\")\nprint(f\"   Total de atualiza\u00e7\u00f5es: {total_updates_ex2}\")\n</code></pre> <pre><code>RESULTADOS:\n   Convergiu? N\u00c3O\n   \u00c9pocas usadas: 100/100\n   Acur\u00e1cia final: 0.725 (72.5%)\n   Total de atualiza\u00e7\u00f5es: 75725\n</code></pre>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-4-fronteira-de-decisao-sobre-os-dados-e-destaque-dos-erros","title":"Etapa 4 \u2014 Fronteira de decis\u00e3o sobre os dados e destaque dos erros","text":"<p>O que este bloco faz:</p> <ul> <li> <p>Reaproveita os par\u00e2metros aprendidos no treino (<code>w_ex2</code>, <code>b_ex2</code>) para sobrepor a fronteira de decis\u00e3o \\(w_1x_1 + w_2x_2 + b = 0\\) ao scatter das classes.</p> </li> <li> <p>Erros de classifica\u00e7\u00e3o s\u00e3o real\u00e7ados com marcadores \u2018x\u2019 amarelos: <code>errors_ex2 = (y_ex2 != y_pred_ex2)</code>.   Em dados com overlap, \u00e9 esperado ver esses pontos concentrados pr\u00f3ximos \u00e0 fronteira, evidenciando a limita\u00e7\u00e3o do classificador linear.</p> </li> </ul> <p>O que observar no resultado:</p> <ul> <li>Inclina\u00e7\u00e3o e posi\u00e7\u00e3o da reta refletem os pesos \\(w\\) e o bias \\(b\\) aprendidos: rota\u00e7\u00e3o \u21e2 \\(w\\), deslocamento \u21e2 \\(b\\).</li> <li>Em fun\u00e7\u00e3o do overlap, haver\u00e1 erros residuais (as marcas amarelas), mesmo ap\u00f3s treinamento, e a acur\u00e1cia n\u00e3o deve chegar a 100%.</li> <li>A fronteira se posiciona como um compromisso entre as duas distribui\u00e7\u00f5es, minimizando erros em m\u00e9dia.</li> </ul> <pre><code>plt.figure(figsize=(14, 5))\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_ex2[mask_neg_ex2, 0], X_ex2[mask_neg_ex2, 1], c='blue', alpha=0.6, s=15, label=\"Classe -1\")\nplt.scatter(X_ex2[mask_pos_ex2, 0], X_ex2[mask_pos_ex2, 1], c='red', alpha=0.6, s=15, label=\"Classe +1\")\n\nx1_range_ex2 = np.array([X_ex2[:, 0].min()-0.5, X_ex2[:, 0].max()+0.5])\nif abs(w_ex2[1]) &gt; 1e-10:\n    x2_line_ex2 = -(w_ex2[0] * x1_range_ex2 + b_ex2) / w_ex2[1]\n    plt.plot(x1_range_ex2, x2_line_ex2, 'k-', linewidth=3, label=\"Fronteira\")\n\n# erros\nerrors_ex2 = (y_ex2 != y_pred_ex2)\nif np.any(errors_ex2):\n    plt.scatter(X_ex2[errors_ex2, 0], X_ex2[errors_ex2, 1], \n               c='yellow', marker='x', s=50, linewidth=2, label=\"Erros\")\n\nplt.title(f\"Fronteira de decis\u00e3o: {final_acc_ex2:.3f} acur\u00e1cia\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-5-conclusoes-sobre-os-resultados","title":"Etapa 5 \u2014 Conclus\u00f5es sobre os resultados","text":"<p>A fronteira aprendida \u00e9 linear e a acur\u00e1cia final ficou em \u2248 0,725. Os erros (marcados em amarelo) concentram-se na regi\u00e3o central onde as duas distribui\u00e7\u00f5es se sobrep\u00f5em, exatamente onde um limite linear n\u00e3o consegue discriminar perfeitamente.</p> <p>Por que chegamos a ~0,72 de acur\u00e1cia? - Os dados foram gerados por duas gaussianas com m\u00e9dias pr\u00f3ximas \\([3,3]\\) e \\([4,4]\\) e mesma covari\u00e2ncia isotr\u00f3pica \\(\\Sigma = 1.5\\,I\\). Logo, o problema n\u00e3o \u00e9 linearmente separ\u00e1vel. - O Perceptron ter chegado a 0,725 \u00e9 coerente e pr\u00f3ximo do \u00f3timo para um modelo linear.</p> <p>E o MLP? Um Multi-Layer Perceptron (com ativa\u00e7\u00f5es n\u00e3o lineares) pode aprender fronteiras curvas e superar modelos lineares se a separa\u00e7\u00e3o \u00f3tima for n\u00e3o linear. Neste dataset espec\u00edfico (duas gaussianas com covari\u00e2ncias iguais), o \u00f3timo \u00e9 linear; portanto, um MLP bem regularizado tende a empatar com um linear em m\u00e9dia. Ele s\u00f3 superaria de forma consistente se a estrutura real dos dados exigir n\u00e3o linearidade.</p> <p>Resumindo: era previsto que uma reta n\u00e3o separasse perfeitamente essas duas classes. Para ganhos reais, use lineares bem calibrados ou modelos n\u00e3o lineares quando houver evid\u00eancia de fronteira n\u00e3o linear.</p>"},{"location":"projetos/classificacao/classificacao/","title":"Classification Project \u2014 Artificial Neural Networks","text":""},{"location":"projetos/classificacao/classificacao/#introduction","title":"Introduction","text":"<p>This project aims to apply Artificial Neural Networks (Multi-Layer Perceptron \u2014 MLP) to solve a supervised classification problem.  </p>"},{"location":"projetos/classificacao/classificacao/#members","title":"Members:","text":"<ul> <li><code>Eduardo Zorzi</code></li> <li><code>Leonardo Teixeira</code></li> <li><code>Mateus Marinheiro</code></li> </ul>"},{"location":"projetos/classificacao/classificacao/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>This project uses the dataset from Kaggle Playground Series 2025 for bank term deposit subscription prediction. The dataset was synthetically generated based on the well-known Bank Marketing Dataset, with the goal of predicting whether a client will subscribe (<code>y = 1</code>) or not (<code>y = 0</code>) to a term deposit.</p>"},{"location":"projetos/classificacao/classificacao/#dataset-characteristics","title":"Dataset Characteristics","text":"<ul> <li>Problem type: Binary classification  </li> <li>Evaluation metric: ROC-AUC  </li> <li>Source: Kaggle Playground Series 2025 (synthetic data derived from real marketing datasets)  </li> <li>Size: More than 750,000 samples</li> <li>URL: Kaggle \u2014 Bank Term Deposit Predictions</li> </ul>"},{"location":"projetos/classificacao/classificacao/#justification-for-the-choice","title":"Justification for the Choice","text":"<ol> <li>Relevance \u2014 This is a real-world banking marketing problem, aligned with practical business applications.  </li> <li>Complexity \u2014 Contains both categorical and numerical features, enabling exploration of preprocessing and encoding strategies.  </li> <li>Scale \u2014 Dataset size is large enough (&gt; 1,000 samples required) to meaningfully train MLPs.  </li> <li>Challenge \u2014 Naturally imbalanced classes, reflecting typical issues in marketing problems (few clients subscribe compared to non-subscribers).  </li> </ol>"},{"location":"projetos/classificacao/classificacao/#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>The dataset represents a bank marketing campaign, where the objective is to predict whether a client will subscribe (<code>y=1</code>) or not subscribe (<code>y=0</code>) to a term deposit. It contains a mixture of categorical and numerical features describing customer demographics, financial status, and details of marketing interactions.</p>"},{"location":"projetos/classificacao/classificacao/#features-overview","title":"Features Overview","text":"<p>Below is a summary of the main features and their types:</p> <ul> <li>age \u2192 Age of the customer. (Numerical) </li> <li>job \u2192 Occupation / employment status. (Categorical) </li> <li>marital \u2192 Marital status. (Categorical) </li> <li>education \u2192 Education level attained. (Categorical) </li> <li>default \u2192 Whether the customer has credit in default. (Categorical) </li> <li>balance \u2192 Balance of the customer\u2019s account. (Numerical) </li> <li>housing \u2192 Whether the customer has a housing loan. (Categorical) </li> <li>loan \u2192 Whether the customer has a personal loan. (Categorical) </li> <li>contact \u2192 Communication type used (e.g., telephone, cellular). (Categorical) </li> <li>day \u2192 Day of the month of the last contact. (Numerical) </li> <li>month \u2192 Month of the last contact. (Categorical) </li> <li>duration \u2192 Duration (in seconds) of the last contact during the campaign. (Numerical) </li> <li>campaign \u2192 Number of contacts performed during this campaign. (Numerical) </li> <li>pdays \u2192 Number of days since last contact from a previous campaign. (Numerical) </li> <li>previous \u2192 Number of contacts performed before this campaign. (Numerical) </li> <li>poutcome \u2192 Outcome of the previous marketing campaign. (Categorical) </li> </ul>"},{"location":"projetos/classificacao/classificacao/#target-variable","title":"Target Variable","text":"<ul> <li>y (deposit subscription) \u2192  </li> <li><code>0</code> \u2192 The client did not subscribe to a term deposit.  </li> <li><code>1</code> \u2192 The client subscribed to a term deposit.  </li> </ul>"},{"location":"projetos/classificacao/classificacao/#notes","title":"Notes","text":"<ul> <li>The dataset mixes demographic, financial, and campaign-related attributes.  </li> <li>Several features are categorical and will require preprocessing (e.g., one-hot encoding).  </li> <li>The target variable (<code>y</code>) is imbalanced, as typically only a small fraction of clients subscribe.  </li> </ul>"},{"location":"projetos/classificacao/classificacao/#descriptive-statistics","title":"Descriptive Statistics","text":"<p>A first quantitative overview of the dataset highlights the distribution of the main numerical features:</p> Feature Count Mean Std Dev Min 25% 50% 75% Max age 750,000 40.9 10.1 18 33 39 48 95 balance 750,000 1204.1 2836.1 -8019 0 634 1390 99,717 day 750,000 16.1 8.3 1 9 17 21 31 duration 750,000 256.2 272.6 0 91 133 361 4918 campaign 750,000 2.6 2.7 1 1 2 3 63 pdays 750,000 22.4 77.3 -1 -1 -1 -1 871 previous 750,000 0.30 1.34 0 0 0 0 200 y (target) 750,000 0.12 0.33 0 0 0 0 1"},{"location":"projetos/classificacao/classificacao/#key-observations","title":"Key Observations","text":"<ul> <li>Target imbalance: The mean of <code>y</code> is 0.12, indicating that only about 12% of clients subscribed, while 88% did not. This confirms a strong imbalance in the classes.  </li> <li>Age distribution: Most clients are adults between 30 and 50 years old.  </li> <li>Balance: Large standard deviation and negative values are present, suggesting outliers and possible overdrafts.  </li> <li>Duration: While most calls are short (median \u2248 133 sec), there are extreme values reaching almost 5,000 seconds.  </li> <li>pdays: The value <code>-1</code> is frequent, representing clients not previously contacted.  </li> <li>Previous contacts: The median is 0, which means that for the majority of clients this is the first interaction.  </li> </ul> <p>These findings highlight the diversity of the dataset, the presence of outliers, and the imbalance of the target variable \u2014 all of which are important considerations for later preprocessing and modeling.</p>"},{"location":"projetos/classificacao/classificacao/#24-categorical-features-analysis","title":"2.4. Categorical Features Analysis","text":"Code \u2014 Distribution of Categorical Features <pre><code>df = pd.read_csv(\"data/train.csv\")  \ncategorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n\nfor col in categorical_cols:\n    counts = df[col].value_counts()\n    percentages = df[col].value_counts(normalize=True) * 100\n\n    for category, count in counts.head(5).items():\n        pct = percentages[category]\n        print(f\"  {category}: {count:,} ({pct:.1f}%)\")\n\n    if len(counts) &gt; 5:\n        print(f\"  ... and {len(counts) - 5} more categories\")\n\n    if 'unknown' in counts.index:\n        unknown_pct = percentages['unknown']\n        print(f\"'unknown' values: {counts['unknown']:,} ({unknown_pct:.1f}%)\")\n\nmissing_values = df.isnull().sum()\nprint(\"\\n=== MISSING VALUES (NaN) ===\")\nif missing_values.sum() == 0:\n    print(\"No NaN values found\")\nelse:\n    print(missing_values[missing_values &gt; 0])\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#results","title":"Results","text":"<p>Job (12 categories):</p> <ul> <li>blue-collar: 174,415 (23.3%)  </li> <li>management: 131,768 (17.6%)  </li> <li>technician: 104,844 (14.0%)  </li> <li>admin.: 96,581 (12.9%)  </li> <li>services: 54,532 (7.3%)  </li> <li>\u2026 and more 7 categories  </li> </ul> <p>Marital (3 categories):</p> <ul> <li>married: 446,431 (59.5%)  </li> <li>single: 202,819 (27.0%)  </li> <li>divorced: 100,750 (13.4%)  </li> </ul> <p>Education (4 categories):</p> <ul> <li>secondary: 370,128 (49.3%)  </li> <li>tertiary: 201,472 (26.9%)  </li> <li>primary: 106,857 (14.2%)  </li> <li>unknown: 71,543 (9.5%)   </li> </ul> <p>Default (2 categories):</p> <ul> <li>no: 671,280 (89.5%)  </li> <li>yes: 78,720 (10.5%)  </li> </ul> <p>Housing (2 categories):</p> <ul> <li>yes: 452,679 (60.4%)  </li> <li>no: 297,321 (39.6%)  </li> </ul> <p>Loan (2 categories):</p> <ul> <li>no: 626,334 (83.5%)  </li> <li>yes: 123,666 (16.5%)  </li> </ul> <p>Contact (3 categories):</p> <ul> <li>cellular: 524,561 (69.9%)  </li> <li>telephone: 225,439 (30.1%)  </li> </ul> <p>Month (12 categories):</p> <ul> <li>may: 186,197 (24.8%)  </li> <li>jul: 139,751 (18.6%)  </li> <li>aug: 127,530 (17.0%)  </li> <li>jun: 110,214 (14.7%)  </li> <li>nov: 96,084 (12.8%)  </li> <li>\u2026 and more 7 categories  </li> </ul> <p>Poutcome (4 categories):</p> <ul> <li>unknown: 667,784 (89.0%)   </li> <li>failure: 72,459 (9.7%)  </li> <li>success: 9,674 (1.3%)  </li> <li>other: 1,083 (0.1%)  </li> </ul>"},{"location":"projetos/classificacao/classificacao/#missing-values","title":"Missing Values","text":"<p>No NaN values found in the dataset.</p>"},{"location":"projetos/classificacao/classificacao/#key-notes","title":"Key Notes","text":"<ul> <li>Several categorical features contain an \u201cunknown\u201d category (notably in <code>education</code> and <code>poutcome</code>).  </li> <li>The distribution of categories is heavily imbalanced in some cases, e.g., <code>poutcome</code> where \u201cunknown\u201d dominates with 89%.  </li> <li>These findings indicate that preprocessing must carefully handle unknown categories and imbalanced distributions.  </li> </ul>"},{"location":"projetos/classificacao/classificacao/#visualizations-categorical-and-bivariate-analysis","title":"Visualizations \u2014 Categorical and Bivariate Analysis","text":"<p>To better understand the categorical features and their relationship with the target variable, we generated grouped visualizations. Each figure below contains four plots for compactness.</p> Code \u2014 Distribution of Categorical Features <pre><code># 1) Top 10 jobs (horizontal bar)\nplt.figure(figsize=(10,6))\njob_counts = df['job'].value_counts().head(10)\njob_counts.plot(kind='barh', color='lightblue')\nplt.title('Top 10 Jobs')\nplt.xlabel('Number of customers')\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_top10_jobs.png\"), dpi=150)\nplt.close()\n\n# 2) Marital distribution (pie)\nplt.figure(figsize=(6,6))\nmarital_counts = df['marital'].value_counts()\nmarital_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90)\nplt.title('Marital Distribution')\nplt.ylabel('')\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_marital_pie.png\"), dpi=150)\nplt.close()\n\n# 3) Education distribution (bar)\nplt.figure(figsize=(7,5))\neducation_counts = df['education'].value_counts()\neducation_counts.plot(kind='bar', color='lightgreen')\nplt.title('Education Distribution')\nplt.xlabel('Education level')\nplt.ylabel('Number of customers')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_education_bar.png\"), dpi=150)\nplt.close()\n\n# 4) Contact type distribution (bar)\nplt.figure(figsize=(7,5))\ncontact_counts = df['contact'].value_counts()\ncontact_counts.plot(kind='bar', color='coral')\nplt.title('Contact Type Distribution')\nplt.xlabel('Contact type')\nplt.ylabel('Number of customers')\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_contact_bar.png\"), dpi=150)\nplt.close()\n\n# 5) Conversion rate by job (restrict to jobs with &gt;=1000 samples)\njob_conv = df.groupby('job')['y'].agg(['count','mean']).sort_values('mean', ascending=False)\njob_conv = job_conv[job_conv['count'] &gt;= 1000]\nplt.figure(figsize=(10,6))\njob_conv['mean'].sort_values().plot(kind='barh', color='skyblue')\nplt.title('Conversion Rate by Job (jobs with &gt;=1000 samples)')\nplt.xlabel('Conversion rate')\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_conversion_by_job.png\"), dpi=150)\nplt.close()\n\n# 6) Conversion rate by education\nedu_conv = df.groupby('education')['y'].mean().sort_values(ascending=False)\nplt.figure(figsize=(7,5))\nedu_conv.plot(kind='bar', color='lightgreen')\nplt.title('Conversion Rate by Education')\nplt.ylabel('Conversion rate')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_conversion_by_education.png\"), dpi=150)\nplt.close()\n\n# 7) Conversion rate by previous outcome (poutcome)\npout_conv = df.groupby('poutcome')['y'].mean().sort_values(ascending=False)\nplt.figure(figsize=(7,5))\npout_conv.plot(kind='bar', color='coral')\nplt.title('Conversion Rate by Previous Campaign Outcome (poutcome)')\nplt.ylabel('Conversion rate')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_conversion_by_poutcome.png\"), dpi=150)\nplt.close()\n\n# 8) Age distribution by class (boxplot)\nplt.figure(figsize=(7,5))\ndf.boxplot(column='age', by='y')\nplt.title('Age Distribution by Target Class')\nplt.suptitle('')  # remove default suptitle\nplt.xlabel('Subscribed (0 = No, 1 = Yes)')\nplt.ylabel('Age')\nplt.tight_layout()\nplt.savefig(os.path.join(ASSETS_DIR, \"viz_age_by_class_boxplot.png\"), dpi=150)\nplt.close()\n\ngroup0 = df[df['y']==0]['age'].dropna()\ngroup1 = df[df['y']==1]['age'].dropna()\nt_stat, p_value = stats.ttest_ind(group0, group1, equal_var=False)\nprint(f\"Age t-test: t = {t_stat:.3f}, p = {p_value:.3e}\")\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#figure-1-categorical-distributions","title":"Figure 1 \u2014 Categorical Distributions","text":"<ul> <li>Top 10 jobs \u2014 Most frequent occupations include management, blue-collar, and technician.  </li> <li>Marital status \u2014 Majority of clients are married (~64%).  </li> <li>Education levels \u2014 Secondary education dominates, followed by tertiary.  </li> <li>Contact type \u2014 Cellular phones are by far the most common contact channel.  </li> </ul>"},{"location":"projetos/classificacao/classificacao/#figure-2-conversion-rates-and-target-relationships","title":"Figure 2 \u2014 Conversion Rates and Target Relationships","text":"<ul> <li>Conversion rate by job \u2014 Students and retired clients show higher subscription rates compared to other professions.  </li> <li>Conversion rate by education \u2014 Higher conversion rates among tertiary education clients.  </li> <li>Conversion rate by previous campaign outcome \u2014 Clients with a previous \u201csuccess\u201d outcome have a much higher likelihood (&gt;75%) of subscribing.  </li> <li>Age by class (boxplot) \u2014 Subscribers tend to have a slightly higher median age compared to non-subscribers.  </li> </ul>"},{"location":"projetos/classificacao/classificacao/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":"<p>Since the dataset does not contain missing values or duplicate rows, the main preprocessing step applied here is the normalization of numerical features. Normalization ensures that all numeric variables are on a comparable scale, which is important for neural networks.</p>"},{"location":"projetos/classificacao/classificacao/#normalization","title":"Normalization","text":"<p>We applied StandardScaler from <code>scikit-learn</code>, which transforms each numerical feature to have mean = 0 and standard deviation = 1.</p> Code \u2014 Normalization with StandardScaler <pre><code>from sklearn.preprocessing import StandardScaler\nimport pandas as pd\n\n# Identify numerical columns (excluding ID and target)\nnum_cols = df.select_dtypes(include=[np.number]).columns.drop(['id','y'])\n\n# Apply StandardScaler\nscaler = StandardScaler()\nscaled_values = scaler.fit_transform(df[num_cols])\ndf_scaled = pd.DataFrame(scaled_values, columns=num_cols)\n\n# Replace original columns with scaled ones\ndf[num_cols] = df_scaled\n\nprint(\"Numeric features normalized with StandardScaler.\")\ndf.head()\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#result","title":"Result","text":"<ul> <li>All numeric features (e.g., <code>age</code>, <code>balance</code>, <code>duration</code>, <code>campaign</code>, <code>pdays</code>, <code>previous</code>) were normalized.  </li> <li>After scaling, each feature has approximately mean = 0 and std = 1.  </li> <li>This normalization step will improve model convergence during training.</li> </ul> <p>In addition to numerical normalization, categorical variables must be preprocessed before training. The chosen strategy is One-Hot Encoding, which expands each categorical feature into multiple binary features. This makes the dataset fully numeric and suitable for training with MLPs.</p>"},{"location":"projetos/classificacao/classificacao/#step-1-identify-categorical-columns","title":"Step 1 \u2014 Identify Categorical Columns","text":"<p>We first load the dataset and detect categorical columns automatically (excluding <code>id</code> and the target <code>y</code>).</p> Code \u2014 Identify categorical columns <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data/train.csv\")\n\nexclude = ['id', 'y']\ncat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\ncat_cols = [c for c in cat_cols if c not in exclude]\n\nprint(\"Categorical columns:\", cat_cols)\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#step-2-group-rare-categories-optional","title":"Step 2 \u2014 Group Rare Categories (Optional)","text":"<p>For columns with many unique values, we may group all but the most frequent categories into <code>__OTHER__</code>. This prevents an excessive explosion in dimensionality after encoding.</p> Code \u2014 Group rare categories into 'OTHER' <pre><code>TOP_K = 10   # keep only the top-10 categories per column\n\ndf_cat = df[cat_cols].copy()\nfor col in cat_cols:\n    top_vals = df_cat[col].value_counts().nlargest(TOP_K).index\n    df_cat[col] = df_cat[col].where(df_cat[col].isin(top_vals), other=\"__OTHER__\")\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#step-3-apply-one-hot-encoding","title":"Step 3 \u2014 Apply One-Hot Encoding","text":"<p>We now fit a <code>OneHotEncoder</code> and transform the categorical features into binary dummy variables. Unknown categories at prediction time will be ignored safely.</p> Code \u2014 One-Hot Encoding <pre><code>from sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\nohe.fit(df_cat)\nX_ohe = ohe.transform(df_cat)\n\nfeature_names = ohe.get_feature_names_out(cat_cols)\nprint(\"Number of encoded features:\", len(feature_names))\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#step-4-rebuild-the-processed-dataset","title":"Step 4 \u2014 Rebuild the Processed Dataset","text":"<p>Finally, we rebuild a full DataFrame with the encoded features, add back <code>id</code> and <code>y</code>, and save the result.</p> Code \u2014 Rebuild and save processed dataset <pre><code>import os\n\ndf_ohe = pd.DataFrame(X_ohe, columns=feature_names, index=df.index)\ndf_final = pd.concat([df[['id','y']].reset_index(drop=True),\n                      df_ohe.reset_index(drop=True)], axis=1)\n\nos.makedirs(\"data/processed\", exist_ok=True)\ndf_final.to_csv(\"data/processed/train_cat_preprocessed.csv\", index=False)\n\nprint(\"Final processed shape:\", df_final.shape)\nprint(\"Saved to data/processed/train_cat_preprocessed.csv\")\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#summary","title":"Summary","text":"<ul> <li>All categorical features were converted into numeric binary variables via One-Hot Encoding.  </li> <li>Rare categories can optionally be grouped into <code>__OTHER__</code>.  </li> <li>Missing categories are treated as <code>\"unknown\"</code>.  </li> <li>The final processed dataset is fully numeric and ready for training MLP models.</li> </ul>"},{"location":"projetos/classificacao/classificacao/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>This page presents the key building blocks of a Multi-Layer Perceptron (MLP) implemented from scratch. Each small code block corresponds to one notebook cell; after each block we provide a didactic explanation and the mathematical expressions used in the code.</p> <p>We follow the course notation: - Input: \\(X \\in \\mathbb{R}^{N \\times D}\\) (N samples, D features) - Layers: sizes \\([D, h_1, h_2, \\dots, h_L, 1]\\) for binary classification - Weights: \\(W^{(i)}\\) with shape \\((\\text{fan\\_in}, \\text{fan\\_out})\\) - Biases: \\(b^{(i)}\\) with shape \\((\\text{fan\\_out},)\\)</p>"},{"location":"projetos/classificacao/classificacao/#cell-1-activation-functions-and-derivatives","title":"Cell 1 \u2014 Activation functions and derivatives","text":"<p>Activations introduce non-linearity. We include numerically-stable sigmoid, ReLU and tanh, plus their derivatives used in backprop.</p> <p>Mathematical expressions:</p> <ul> <li>Sigmoid: \\(\\sigma(z) = \\dfrac{1}{1 + e^{-z}}\\) </li> <li>Sigmoid derivative (in code we use output): \\(\\sigma'(z) = \\sigma(z)(1-\\sigma(z))\\) </li> <li>ReLU: \\(\\text{ReLU}(z) = \\max(0, z)\\) </li> <li>ReLU derivative: \\(\\text{ReLU}'(z) = \\mathbf{1}_{z &gt; 0}\\) </li> <li>Tanh derivative (from output): \\(\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)\\)</li> </ul> Code \u2014 Activations and derivatives <pre><code>import numpy as np\n\n# Numerically-stable sigmoid\ndef sigmoid(z):\n    z = np.clip(z, -50, 50)\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef dsigmoid_from_output(sigmoid_out):\n    # derivative using output: s * (1 - s)\n    return sigmoid_out * (1.0 - sigmoid_out)\n\ndef relu(z):\n    return np.maximum(0.0, z)\n\ndef drelu(z):\n    return (z &gt; 0).astype(float)\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef dtanh_from_output(tanh_out):\n    return 1.0 - tanh_out**2\n</code></pre> <p>Why &amp; notes - We clip the input to <code>sigmoid</code> to avoid overflow in <code>exp</code>. - Using derivatives computed from activation outputs (where possible) is numerically convenient. - ReLU is usually preferred in hidden layers for faster convergence.</p>"},{"location":"projetos/classificacao/classificacao/#cell-2-weight-initialization","title":"Cell 2 \u2014 Weight initialization","text":"<p>Good initialization keeps signal variance stable across layers.</p> <p>Mathematical intent: - He (for ReLU): \\(W_{ij} \\sim \\mathcal{N}(0, \\frac{2}{\\text{fan\\_in}})\\) - Xavier (for tanh/sigmoid): \\(W_{ij} \\sim \\mathcal{N}(0, \\frac{1}{\\text{fan\\_in}})\\)</p> Code \u2014 Weight initialization <pre><code>def init_weights(layer_dims, activation='relu', seed=42):\n    rng = np.random.RandomState(seed)\n    W = []\n    b = []\n    for i in range(len(layer_dims)-1):\n        fan_in = layer_dims[i]\n        fan_out = layer_dims[i+1]\n        if activation == 'relu':\n            # He initialization\n            std = np.sqrt(2.0 / fan_in)\n        else:\n            # Xavier initialization (good for tanh/sigmoid)\n            std = np.sqrt(1.0 / fan_in)\n        W.append(rng.normal(0.0, std, size=(fan_in, fan_out)))\n        b.append(np.zeros((fan_out,)))\n    return W, b\n</code></pre> <p>Why &amp; notes - Proper initialization avoids vanishing/exploding activations early in training. - Biases initialized to zero is common practice.</p>"},{"location":"projetos/classificacao/classificacao/#cell-3-forward-pass-vectorized-and-caches-for-backprop","title":"Cell 3 \u2014 Forward pass (vectorized) and caches for backprop","text":"<p>We compute the pre-activation \\(Z^{(l)}\\) and activation \\(A^{(l)}\\) per layer and store caches for backprop.</p> <p>Equations (layer \\(l\\)): - Pre-activation: \\(Z^{(l)} = A^{(l-1)} W^{(l)} + b^{(l)}\\) - Activation (hidden): \\(A^{(l)} = \\phi(Z^{(l)})\\) where \\(\\phi\\) is ReLU or tanh - Output logits (last layer): \\(s = Z^{(L)}\\) (we apply sigmoid later in loss)</p> Code \u2014 Forward pass with caches <pre><code>def forward_pass(X, W, b, hidden_activation='relu'):\n    A = X\n    caches = []\n    L = len(W)\n    for i in range(L):\n        A_prev = A\n        Z = A_prev.dot(W[i]) + b[i]  # shape (N, fan_out)\n        if i &lt; L-1:\n            # hidden layer activation\n            if hidden_activation == 'relu':\n                A = relu(Z)\n            else:\n                A = tanh(Z)\n        else:\n            # output layer: keep logits (no activation here for stability)\n            A = Z\n        caches.append({'Z': Z, 'A_prev': A_prev, 'A': A})\n    logits = caches[-1]['A']  # pre-activation of last layer\n    return logits, caches\n</code></pre> <p>Why &amp; notes - Returning logits (not probabilities) is numerically preferable for loss computation. - <code>caches</code> stores <code>A_prev</code> and <code>Z</code> needed for efficient vectorized backprop.</p>"},{"location":"projetos/classificacao/classificacao/#cell-4-loss-binary-cross-entropy-and-probabilities","title":"Cell 4 \u2014 Loss (Binary Cross-Entropy) and probabilities","text":"<p>We compute probabilities with sigmoid and the BCE loss:</p> <p>Formulas: - Probability: \\(\\hat{p}_i = \\sigma(s_i)\\) - BCE loss (mean over samples): \\(\\displaystyle \\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^N \\big[y_i \\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)\\big]\\) - L2 regularization: add \\(\\frac{\\lambda}{2}\\sum_l \\|W^{(l)}\\|_F^2\\) to the loss (if <code>l2_lambda &gt; 0</code>)</p> Code \u2014 Loss and probabilities (BCE) <pre><code>def compute_loss_and_probs(logits, y, W=None, l2_lambda=0.0):\n    logits = logits.reshape(-1)\n    probs = sigmoid(logits)\n    eps = 1e-12\n    loss = -np.mean(y * np.log(probs + eps) + (1-y) * np.log(1-probs + eps))\n    if W is not None and l2_lambda &gt; 0.0:\n        l2 = 0.0\n        for Wi in W:\n            l2 += np.sum(Wi * Wi)\n        loss += 0.5 * l2_lambda * l2\n    return loss, probs\n</code></pre> <p>Why &amp; notes - Using <code>logits</code> then <code>sigmoid</code> inside the loss gives clarity and allows us to reuse the same logits in gradient computations. - <code>eps</code> prevents numerical <code>log(0)</code>.</p>"},{"location":"projetos/classificacao/classificacao/#cell-5-backpropagation-compute-gradients-for-all-layers","title":"Cell 5 \u2014 Backpropagation (compute gradients for all layers)","text":"<p>Backprop computes gradients layer-by-layer.</p> <p>Key equations (for binary BCE + sigmoid): - For the output layer: \\(\\delta^{(L)} = \\frac{1}{N}(\\hat{p} - y)\\) (shape \\(N \\times 1\\)) - Gradients:   - \\( \\nabla_{W^{(L)}} = (A^{(L-1)})^\\top \\delta^{(L)} + \\lambda W^{(L)} \\)   - \\( \\nabla_{b^{(L)}} = \\sum_{i=1}^N \\delta^{(L)}_i \\) - Propagate to previous layer: \\(\\delta^{(l)} = (\\delta^{(l+1)} {W^{(l+1)}}^\\top) \\odot \\phi'(Z^{(l)})\\) - Then compute \\( \\nabla_{W^{(l)}} = (A^{(l-1)})^\\top \\delta^{(l)} + \\lambda W^{(l)}\\).</p> <p>The code below implements these vectorized operations.</p> Code \u2014 Backpropagation (gradients for W and b) <pre><code>def backward_pass(logits, probs, y, caches, W, hidden_activation='relu', l2_lambda=0.0):\n    N = y.shape[0]\n    L = len(W)\n    # ensure shapes\n    dW = [None] * L\n    db = [None] * L\n\n    # dL/dz_last = (probs - y) / N  (for BCE with sigmoid)\n    dz = (probs - y).reshape(-1,1) / N  # shape (N,1) or (N,out)\n    # gradient for last layer parameters\n    A_prev = caches[-1]['A_prev']  # activation from previous layer\n    dW[L-1] = A_prev.T.dot(dz) + l2_lambda * W[L-1]\n    db[L-1] = np.sum(dz, axis=0)\n\n    # propagate backwards through hidden layers\n    delta = dz  # current gradient wrt layer output\n    for i in range(L-2, -1, -1):\n        Z_i = caches[i]['Z']              # pre-activation at layer i\n        A_prev = caches[i]['A_prev']      # activation before this layer\n        # propagate delta through weights of layer i+1\n        W_next = W[i+1]\n        # shape: (N, units_i) = (N, units_{i+1}) dot (units_{i+1}, units_i)\n        delta = delta.dot(W_next.T)\n        # multiply by activation derivative at layer i\n        if hidden_activation == 'relu':\n            delta = delta * drelu(Z_i)\n        else:\n            A_i = caches[i]['A']\n            delta = delta * dtanh_from_output(A_i)\n        # compute grads\n        dW[i] = A_prev.T.dot(delta) + l2_lambda * W[i]\n        db[i] = np.sum(delta, axis=0)\n\n    return dW, db\n</code></pre> <p>Why &amp; notes - We divide by <code>N</code> in the initial delta to compute mean loss gradient. - L2 regularization adds derivative <code>lambda * W</code> to each gradient.</p>"},{"location":"projetos/classificacao/classificacao/#cell-6-parameter-update-sgd-step","title":"Cell 6 \u2014 Parameter update: SGD step","text":"<p>Simple SGD update rule:</p> <ul> <li>\\( W^{(l)} \\leftarrow W^{(l)} - \\eta \\nabla_{W^{(l)}} \\) </li> <li>\\( b^{(l)} \\leftarrow b^{(l)} - \\eta \\nabla_{b^{(l)}} \\)</li> </ul> Code \u2014 SGD update <pre><code>def sgd_update(W, b, grads_W, grads_b, lr=1e-3):\n    for i in range(len(W)):\n        W[i] -= lr * grads_W[i]\n        b[i] -= lr * grads_b[i]\n    return W, b\n</code></pre> <p>Why &amp; notes - Plain SGD is transparent and matches the course exposition. - Later you can replace this with momentum, RMSProp, or Adam without changing forward/backward code.</p>"},{"location":"projetos/classificacao/classificacao/#hyperparameters-explanation-and-guidance","title":"Hyperparameters \u2014 explanation and guidance","text":"<p>When you include experiments in the report, explicitly state and justify chosen hyperparameters:</p> <ul> <li>learning rate (<code>lr</code>) \u2014 controls the step size in parameter space. Typical starting points: <code>1e-3</code> (Adam) or <code>1e-2</code> / <code>1e-3</code> (SGD). If loss diverges, reduce <code>lr</code>.  </li> <li>batch size \u2014 controls noise and speed: small batches (<code>32</code>, <code>64</code>) give noisy but often better generalization; large batches (<code>256</code>, <code>512</code>) are more stable and faster per epoch on modern hardware.  </li> <li>epochs \u2014 number of passes over the dataset. Use early stopping based on validation loss. Typical ranges vary: <code>20\u2013200</code>.  </li> <li>hidden architecture (<code>layer_dims</code>) \u2014 e.g., <code>[input_dim, 128, 64, 1]</code>. More capacity can model complex functions but risks overfitting.  </li> <li>activation \u2014 <code>relu</code> recommended for hidden layers. For small didactic networks <code>tanh</code> is fine.  </li> <li>L2 weight decay (<code>l2_lambda</code>) \u2014 small positive value like <code>1e-4</code> to <code>1e-2</code> helps generalization.  </li> <li>seed \u2014 fix RNG seeds for reproducibility of initialization and shuffling.</li> </ul>"},{"location":"projetos/classificacao/classificacao/#references-to-the-course-material","title":"References to the course material","text":"<p>This implementation mirrors the steps and notation used in the course notes:</p> <ul> <li>Forward: compute \\(Z\\) and \\(A\\) per layer.  </li> <li>Loss: cross-entropy for classification.  </li> <li>Backprop: compute \\(\\delta\\) then gradients for weights/biases.  </li> <li>Update: SGD.</li> </ul>"},{"location":"projetos/classificacao/classificacao/#5-model-training","title":"5. Model Training","text":""},{"location":"projetos/classificacao/classificacao/#training-utilities-and-helper-functions","title":"Training Utilities and Helper Functions","text":"<p>Before implementing the main training loop, we need several utility functions for gradient clipping, model saving/loading, and plotting training metrics.</p> Code \u2014 Training utilities (gradient clipping, save/load, plotting) <pre><code>import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\n\n# folders\nos.makedirs(\"models\", exist_ok=True)\nos.makedirs(\"docs/assets\", exist_ok=True)\n\ndef clip_gradients(grads_W, grads_b, max_norm):\n    \"\"\"\n    Clip gradients by global L2 norm.\n    grads_W, grads_b: lists of gradient arrays for weights and biases.\n    max_norm: float or None. If None or &lt;=0, do nothing.\n    Returns clipped (grads_W, grads_b).\n    \"\"\"\n    if max_norm is None or max_norm &lt;= 0:\n        return grads_W, grads_b\n    total_sq = 0.0\n    for g in grads_W:\n        total_sq += np.sum(g**2)\n    for g in grads_b:\n        total_sq += np.sum(g**2)\n    total_norm = np.sqrt(total_sq)\n    if total_norm &gt; max_norm:\n        scale = max_norm / (total_norm + 1e-12)\n        grads_W = [g * scale for g in grads_W]\n        grads_b = [g * scale for g in grads_b]\n    return grads_W, grads_b\n\ndef save_weights(path, W, b):\n    \"\"\"Save weights &amp; biases to compressed npz.\"\"\"\n    npz = {}\n    for i, Wi in enumerate(W):\n        npz[f\"W{i}\"] = Wi\n    for i, bi in enumerate(b):\n        npz[f\"b{i}\"] = bi\n    np.savez_compressed(path, **npz)\n    print(\"Saved weights to\", path)\n\ndef load_weights(path):\n    \"\"\"Load weights saved by save_weights.\"\"\"\n    data = np.load(path)\n    W = []\n    b = []\n    i = 0\n    while f\"W{i}\" in data:\n        W.append(data[f\"W{i}\"])\n        b.append(data[f\"b{i}\"])\n        i += 1\n    return W, b\n\ndef plot_metrics(history, prefix=\"train\"):\n    \"\"\"\n    history: dict with keys 'train_loss', 'val_loss', 'val_auc' (lists per epoch)\n    saves an image to docs/assets/{prefix}_training_metrics.png\n    \"\"\"\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1)\n    plt.plot(history['train_loss'], label='train_loss')\n    plt.plot(history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss')\n\n    plt.subplot(1,2,2)\n    plt.plot(history['val_auc'], label='val_auc')\n    plt.xlabel('Epoch')\n    plt.ylabel('ROC-AUC')\n    plt.legend()\n    plt.title('Validation ROC-AUC')\n\n    plt.tight_layout()\n    fname = f\"docs/assets/{prefix}_training_metrics.png\"\n    plt.savefig(fname, dpi=150)\n    plt.show()\n    print(\"Saved training plot to\", fname)\n</code></pre> <p>Why &amp; notes - Gradient clipping prevents exploding gradients, which can destabilize training. - Save/load functions allow us to persist the best model during training. - Plotting function generates training curves for analysis and inclusion in the report.</p>"},{"location":"projetos/classificacao/classificacao/#complete-training-function","title":"Complete Training Function","text":"<p>Now we implement the main training loop that combines all previous components:</p> Code \u2014 Full MLP training function <pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\ndef train_mlp_full(X, y,\n                   layer_dims,\n                   hidden_activation='relu',\n                   epochs=30,\n                   batch_size=512,\n                   lr=1e-3,\n                   l2_lambda=1e-4,\n                   seed=42,\n                   max_grad_norm=5.0,\n                   early_stop_patience=5,\n                   verbose=True,\n                   save_prefix=\"models/mlp_numpy\"):\n    \"\"\"\n    Train MLP using the functions defined in previous cells.\n    Returns best_W, best_b, history (dict).\n    \"\"\"\n    # Initialize\n    W, b = init_weights(layer_dims, activation=hidden_activation, seed=seed)\n    # Split\n    X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed)\n    n = X_tr.shape[0]\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n\n    best_val_loss = np.inf\n    best_W = None\n    best_b = None\n    epochs_no_improve = 0\n\n    for ep in range(epochs):\n        # shuffle training set\n        perm = np.random.RandomState(seed + ep).permutation(n)\n        X_sh = X_tr[perm]\n        y_sh = y_tr[perm]\n\n        ep_loss = 0.0\n        # mini-batch loop\n        for i in range(0, n, batch_size):\n            xb = X_sh[i:i+batch_size]\n            yb = y_sh[i:i+batch_size]\n\n            # forward\n            logits, caches = forward_pass(xb, W, b, hidden_activation=hidden_activation)\n            # loss + probs\n            loss, probs = compute_loss_and_probs(logits, yb, W=W, l2_lambda=l2_lambda)\n            ep_loss += float(loss) * xb.shape[0]\n\n            # backprop\n            grads_W, grads_b = backward_pass(logits, probs, yb, caches, W, hidden_activation=hidden_activation, l2_lambda=l2_lambda)\n\n            # grad clip\n            grads_W, grads_b = clip_gradients(grads_W, grads_b, max_grad_norm)\n\n            # update\n            W, b = sgd_update(W, b, grads_W, grads_b, lr=lr)\n\n        ep_loss /= n\n\n        # validation\n        logits_val, _ = forward_pass(X_val, W, b, hidden_activation=hidden_activation)\n        val_loss, val_probs = compute_loss_and_probs(logits_val, y_val, W=W, l2_lambda=l2_lambda)\n        try:\n            val_auc = roc_auc_score(y_val, val_probs)\n        except Exception:\n            val_auc = None\n\n        history['train_loss'].append(ep_loss)\n        history['val_loss'].append(float(val_loss))\n        history['val_auc'].append(val_auc)\n\n        if verbose:\n            print(f\"Epoch {ep+1}/{epochs} - train_loss={ep_loss:.6f} val_loss={val_loss:.6f} val_auc={val_auc:.4f}\")\n\n        # early stopping &amp; save best\n        if val_loss &lt; best_val_loss - 1e-8:\n            best_val_loss = val_loss\n            best_W = [Wi.copy() for Wi in W]\n            best_b = [bi.copy() for bi in b]\n            epochs_no_improve = 0\n            save_path = f\"{save_prefix}_best.npz\"\n            save_weights(save_path, best_W, best_b)\n            if verbose:\n                print(\"  -&gt; New best model saved to\", save_path)\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve &gt;= early_stop_patience:\n                if verbose:\n                    print(f\"Early stopping after {ep+1} epochs (no improvement for {early_stop_patience} epochs).\")\n                break\n\n    # final save\n    final_path = f\"{save_prefix}_final.npz\"\n    save_weights(final_path, W, b)\n    if verbose:\n        print(\"Saved final model to\", final_path)\n\n    # if best never set, set to final\n    if best_W is None:\n        best_W = W\n        best_b = b\n\n    return best_W, best_b, history\n</code></pre> <p>Key features of this training function:</p> <ol> <li>Train/Validation Split \u2014 80/20 split with stratification to maintain class balance</li> <li>Mini-batch Training \u2014 Processes data in batches for memory efficiency and better gradient estimates</li> <li>Early Stopping \u2014 Monitors validation loss and stops training if no improvement for several epochs</li> <li>Model Checkpointing \u2014 Saves the best model (lowest validation loss) during training</li> <li>Gradient Clipping \u2014 Prevents exploding gradients that can destabilize training</li> <li>Comprehensive Logging \u2014 Tracks training/validation loss and ROC-AUC per epoch</li> </ol>"},{"location":"projetos/classificacao/classificacao/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"projetos/classificacao/classificacao/#data-split-strategy","title":"Data Split Strategy","text":"<p>We use a stratified train-validation split (80/20) to ensure both sets maintain the same class distribution as the original dataset. This is crucial for imbalanced datasets like ours (\u224812% positive class).</p> <p>Rationale:</p> <ul> <li>Training set (80%) \u2014 Used for parameter updates via gradient descent</li> <li>Validation set (20%) \u2014 Used for hyperparameter tuning and early stopping</li> <li>Test set \u2014 Separate holdout set provided by Kaggle for final evaluation</li> </ul>"},{"location":"projetos/classificacao/classificacao/#training-configuration","title":"Training Configuration","text":"<p>Based on experimentation and best practices for MLPs, we selected the following hyperparameters:</p> Code \u2014 Training configuration and execution <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load processed data\nTRAIN_PROC = \"data/processed/train_preprocessed.csv\"\ndfp = pd.read_csv(TRAIN_PROC)\nprint(\"Processed train shape:\", dfp.shape)\n\n# Prepare X and y\ny = dfp['y'].astype(int).values\nX = dfp.drop(columns=['id','y']).values.astype(np.float32)\n\n# Model architecture and hyperparameters\nlayer_dims = [X.shape[1], 128, 64, 1]  # input -&gt; 128 -&gt; 64 -&gt; 1\n\n# Train the model\nbest_W, best_b, history = train_mlp_full(\n    X, y,\n    layer_dims=layer_dims,\n    hidden_activation='relu',\n    epochs=30,\n    batch_size=2048,\n    lr=1e-3,\n    l2_lambda=1e-4,\n    seed=42,\n    max_grad_norm=5.0,\n    early_stop_patience=5,\n    verbose=True,\n    save_prefix=\"models/mlp_numpy\"\n)\n\n# Save training plot for analysis\nplot_metrics(history, prefix=\"mlp_numpy\")\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#hyperparameter-justification","title":"Hyperparameter Justification","text":"Parameter Value Justification Architecture [input_dim, 128, 64, 1] Two hidden layers provide sufficient capacity without overfitting Activation ReLU Prevents vanishing gradients, faster convergence Learning Rate 1e-3 Standard starting point for SGD, balances speed and stability Batch Size 2048 Large enough for stable gradients, efficient on modern hardware L2 Regularization 1e-4 Light regularization to prevent overfitting Gradient Clipping 5.0 Prevents exploding gradients during training Early Stopping 5 epochs Prevents overfitting while allowing convergence Epochs 30 Maximum epochs with early stopping as safeguard"},{"location":"projetos/classificacao/classificacao/#training-mode-strategy","title":"Training Mode Strategy","text":"<p>We implement mini-batch gradient descent with the following characteristics:</p> <ol> <li>Shuffling \u2014 Data is reshuffled each epoch to prevent learning order-dependent patterns</li> <li>Stratified validation \u2014 Ensures validation set maintains class balance</li> <li>Early stopping \u2014 Monitors validation loss to prevent overfitting</li> <li>Model persistence \u2014 Saves both best (validation) and final models</li> </ol> <p>Why mini-batch over alternatives:</p> <ul> <li>vs. Batch GD: More frequent updates, better generalization</li> <li>vs. Stochastic GD: More stable gradients, computational efficiency</li> <li>vs. Online: Better hardware utilization, stable convergence</li> </ul>"},{"location":"projetos/classificacao/classificacao/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":""},{"location":"projetos/classificacao/classificacao/#training-progress-analysis","title":"Training Progress Analysis","text":"<p>The training curves provide crucial insights into model performance, convergence behavior, and potential overfitting. Our implementation automatically generates comprehensive training visualizations.</p>"},{"location":"projetos/classificacao/classificacao/#loss-curves","title":"Loss Curves","text":"<p>The loss curves show the evolution of both training and validation loss over epochs:</p> <p></p> <p>Key observations from the training curves:</p> <ol> <li>Convergence Pattern \u2014 Both training and validation losses decrease steadily, indicating proper learning</li> <li>No Overfitting Signs \u2014 Validation loss follows training loss closely without diverging upward</li> <li>Stable Learning \u2014 Smooth curves suggest appropriate learning rate and batch size</li> <li>Early Stopping Effectiveness \u2014 Training stopped when validation improvement plateaued</li> </ol>"},{"location":"projetos/classificacao/classificacao/#roc-auc-evolution","title":"ROC-AUC Evolution","text":"<p>The ROC-AUC curve demonstrates the model's classification performance improvement:</p> <p>Analysis:</p> <ul> <li>Initial Performance \u2014 Starts around 0.65, significantly better than random (0.5)</li> <li>Rapid Improvement \u2014 Quick convergence to ~0.90+ within first 10 epochs</li> <li>Plateau Behavior \u2014 Performance stabilizes, indicating convergence</li> <li>Final Score \u2014 Achieves excellent ROC-AUC &gt; 0.90</li> </ul>"},{"location":"projetos/classificacao/classificacao/#training-interpretation","title":"Training Interpretation","text":"<p>Positive indicators:</p> <ul> <li>Decreasing loss \u2014 Model is learning the underlying patterns</li> <li>Validation tracking \u2014 No significant overfitting</li> <li>High ROC-AUC \u2014 Excellent discrimination between classes</li> <li>Stable convergence \u2014 Hyperparameters are well-tuned</li> </ul> <p>Potential improvements:</p> <ul> <li>Learning rate scheduling \u2014 Could accelerate early training</li> <li>Architecture tuning \u2014 Experiment with different layer sizes</li> <li>Advanced optimizers \u2014 Adam or RMSprop might improve convergence</li> </ul>"},{"location":"projetos/classificacao/classificacao/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"projetos/classificacao/classificacao/#comprehensive-model-evaluation","title":"Comprehensive Model Evaluation","text":"<p>We evaluate our trained MLP using multiple classification metrics to provide a complete performance assessment. The evaluation covers both validation set (with ground truth labels) and test set (for final submission).</p>"},{"location":"projetos/classificacao/classificacao/#loading-model-and-test-data","title":"Loading Model and Test Data","text":"Code \u2014 Load trained model and prepare test data <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, classification_report,\n    roc_curve, precision_recall_curve, average_precision_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\" Loading trained model and test data...\")\n\n# Load the best model\nbest_W, best_b = load_weights(\"models/mlp_numpy_best.npz\")\nprint(\" Model loaded!\")\n\n# Load processed test data\nTEST_PROC = \"data/processed/test_preprocessed.csv\"\ndf_test = pd.read_csv(TEST_PROC)\nprint(f\" Test data shape: {df_test.shape}\")\n\n# Prepare test features\nX_test = df_test.drop(columns=['id']).values.astype(np.float32)\ntest_ids = df_test['id'].values\n\nprint(f\" Test features shape: {X_test.shape}\")\nprint(f\"   Number of test samples: {len(test_ids):,}\")\n</code></pre> Output <pre><code> Loading trained model and test data...\n Model loaded!\n Test data shape: (250000, 51)\n Test features shape: (250000, 50)\n   Number of test samples: 250,000\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#test-set-predictions","title":"Test Set Predictions","text":"Code \u2014 Generate predictions for test set <pre><code># Make predictions on test set\nprint(\" Making predictions on test set...\")\n\n# Forward pass on test set\nlogits_test, _ = forward_pass(X_test, best_W, best_b, hidden_activation='relu')\ntest_probs = sigmoid(logits_test.reshape(-1))\ntest_predictions = (test_probs &gt; 0.5).astype(int)\n\nprint(f\" Predictions completed!\")\nprint(f\"   Probabilities shape: {test_probs.shape}\")\nprint(f\"   Predictions shape: {test_predictions.shape}\")\nprint(f\"   Prob stats: min={test_probs.min():.4f}, max={test_probs.max():.4f}, mean={test_probs.mean():.4f}\")\n\n# Distribution of predictions\nunique, counts = np.unique(test_predictions, return_counts=True)\nprint(f\"\\n Prediction distribution:\")\nfor val, count in zip(unique, counts):\n    percentage = count / len(test_predictions) * 100\n    print(f\"   Class {val}: {count:,} ({percentage:.1f}%)\")\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_ids,\n    'y': test_predictions\n})\n</code></pre> Output <pre><code> Making predictions on test set...\n Predictions completed!\n   Probabilities shape: (250000,)\n   Predictions shape: (250000,)\n   Prob stats: min=0.0001, max=1.0000, mean=0.1219\n\n Prediction distribution:\n   Class 0: 229,522 (91.8%)\n   Class 1: 20,478 (8.2%)\n\n Submission DataFrame created: (250000, 2)\n       id  y\n0  750000  0\n1  750001  0\n2  750002  0\n3  750003  0\n4  750004  0\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#validation-set-evaluation","title":"Validation Set Evaluation","text":"Code \u2014 Comprehensive validation metrics <pre><code>from sklearn.model_selection import train_test_split\n\nprint(\" Detailed evaluation on validation set...\")\n\n# Recreate validation split with same parameters\ndfp = pd.read_csv(\"data/processed/train_preprocessed.csv\")\ny_full = dfp['y'].astype(int).values\nX_full = dfp.drop(columns=['id','y']).values.astype(np.float32)\n\n# Split with same parameters as training\nX_tr, X_val, y_tr, y_val = train_test_split(X_full, y_full, test_size=0.2, stratify=y_full, random_state=42)\n\nprint(f\" Validation set: {X_val.shape[0]:,} samples\")\n\n# Make predictions on validation set\nlogits_val, _ = forward_pass(X_val, best_W, best_b, hidden_activation='relu')\nval_probs = sigmoid(logits_val.reshape(-1))\nval_predictions = (val_probs &gt; 0.5).astype(int)\n\n# Calculate comprehensive metrics\naccuracy = accuracy_score(y_val, val_predictions)\nprecision = precision_score(y_val, val_predictions)\nrecall = recall_score(y_val, val_predictions)\nf1 = f1_score(y_val, val_predictions)\nroc_auc = roc_auc_score(y_val, val_probs)\n\nprint(f\"\\n VALIDATION METRICS:\")\nprint(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"   Precision: {precision:.4f}\")\nprint(f\"   Recall:    {recall:.4f}\")\nprint(f\"   F1-Score:  {f1:.4f}\")\nprint(f\"   ROC-AUC:   {roc_auc:.4f}\")\n\n# Detailed classification report\nprint(f\"\\n CLASSIFICATION REPORT:\")\nprint(classification_report(y_val, val_predictions, target_names=['Class 0', 'Class 1']))\n</code></pre> Output <pre><code> Detailed evaluation on validation set...\n Validation set: 150,000 samples\n\n VALIDATION METRICS:\n   Accuracy:  0.9077 (90.77%)\n   Precision: 0.6742\n   Recall:    0.4545\n   F1-Score:  0.5430\n   ROC-AUC:   0.9290\n\n CLASSIFICATION REPORT:\n              precision    recall  f1-score   support\n\n    Class 0        0.93      0.97      0.95    131902\n    Class 1        0.67      0.45      0.54     18098\n\n    accuracy                           0.91    150000\n   macro avg       0.80      0.71      0.75    150000\nweighted avg       0.90      0.91      0.90    150000\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#visualization-of-results","title":"Visualization of Results","text":"Code \u2014 Confusion matrix and ROC curves <pre><code># Create comprehensive evaluation plots\nplt.figure(figsize=(15, 5))\n\n# 1. Confusion Matrix\nplt.subplot(1, 3, 1)\ncm = confusion_matrix(y_val, val_predictions)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Pred 0', 'Pred 1'], \n            yticklabels=['True 0', 'True 1'])\nplt.title('Confusion Matrix\\n(Validation Set)')\nplt.ylabel('True Labels')\nplt.xlabel('Predictions')\n\n# 2. ROC Curve\nplt.subplot(1, 3, 2)\nfpr, tpr, _ = roc_curve(y_val, val_probs)\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.4f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Baseline (AUC = 0.5)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\n\n# 3. Precision-Recall Curve\nplt.subplot(1, 3, 3)\nprecision_curve, recall_curve, _ = precision_recall_curve(y_val, val_probs)\navg_precision = average_precision_score(y_val, val_probs)\nplt.plot(recall_curve, precision_curve, color='blue', lw=2, \n         label=f'PR Curve (AP = {avg_precision:.4f})')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('docs/assets/model_evaluation_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\" Evaluation plots saved: docs/assets/model_evaluation_metrics.png\")\n</code></pre> Output <pre><code> Evaluation plots saved: docs/assets/model_evaluation_metrics.png\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#final-results-summary","title":"Final Results Summary","text":"Code \u2014 Generate final submission and summary <pre><code># Save submission file\nprint(\"Saving submission file...\")\n\nsubmission_path = \"submission_mlp_numpy.csv\"\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"Submission file saved: {submission_path}\")\n\n# Final comprehensive summary\nprint(\"\\n\" + \"=\"*60)\nprint(\"FINAL EVALUATION SUMMARY\")\nprint(\"=\"*60)\nprint(f\"Training dataset: {X_full.shape[0]:,} samples, {X_full.shape[1]} features\")\nprint(f\"Validation dataset: {X_val.shape[0]:,} samples\")\nprint(f\"Test dataset: {X_test.shape[0]:,} samples\")\nprint(f\"\\nMODEL ARCHITECTURE:\")\nprint(f\"   Input Layer: {X_full.shape[1]} neurons\")\nprint(f\"   Hidden Layer 1: 128 neurons (ReLU)\")\nprint(f\"   Hidden Layer 2: 64 neurons (ReLU)\")\nprint(f\"   Output Layer: 1 neuron (Sigmoid)\")\nprint(f\"\\nVALIDATION PERFORMANCE:\")\nprint(f\"   ROC-AUC: {roc_auc:.4f} (Excellent!)\")\nprint(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\nprint(f\"   F1-Score: {f1:.4f}\")\nprint(f\"   Precision: {precision:.4f}\")\nprint(f\"   Recall: {recall:.4f}\")\nprint(f\"\\nGENERATED FILES:\")\nprint(f\"   models/mlp_numpy_best.npz (best model)\")\nprint(f\"   models/mlp_numpy_final.npz (final model)\")\nprint(f\"   docs/assets/mlp_numpy_training_metrics.png\")\nprint(f\"   docs/assets/model_evaluation_metrics.png\")\nprint(f\"   {submission_path}\")\nprint(\"=\"*60)\n</code></pre> Output <pre><code>Saving submission file...\nSubmission file saved: submission_mlp_numpy.csv\n\n============================================================\nFINAL EVALUATION SUMMARY\n============================================================\nTraining dataset: 750,000 samples, 50 features\nValidation dataset: 150,000 samples\nTest dataset: 250,000 samples\n\nMODEL ARCHITECTURE:\n   Input Layer: 50 neurons\n   Hidden Layer 1: 128 neurons (ReLU)\n   Hidden Layer 2: 64 neurons (ReLU)\n   Output Layer: 1 neuron (Sigmoid)\n\nVALIDATION METRICS:\n   ROC-AUC: 0.9290 (Excellent!)\n   Accuracy: 0.9077 (90.77%)\n   F1-Score: 0.5430\n   Precision: 0.6742\n   Recall: 0.4545\n\nGENERATED FILES:\n   models/mlp_numpy_best.npz (best model)\n   models/mlp_numpy_final.npz (final model)\n   docs/assets/mlp_numpy_training_metrics.png\n   docs/assets/model_evaluation_metrics.png\n   submission_mlp_numpy.csv\n============================================================\n</code></pre>"},{"location":"projetos/classificacao/classificacao/#performance-analysis","title":"Performance Analysis","text":""},{"location":"projetos/classificacao/classificacao/#key-results","title":"Key Results","text":"Metric Value Interpretation ROC-AUC 0.9290 Excellent discrimination capability Accuracy 90.77% Strong overall classification performance Precision 0.6742 Low false positive rate Recall 0.4545 Good true positive detection F1-Score 0.5430 Balanced precision-recall performance"},{"location":"projetos/classificacao/classificacao/#business-impact","title":"Business Impact","text":"<p>Strengths:</p> <ul> <li>High ROC-AUC indicates excellent ability to rank clients by subscription likelihood</li> <li>Balanced metrics show the model performs well across different evaluation criteria  </li> <li>Low false positive rate minimizes wasted marketing efforts</li> <li>Strong recall captures most potential subscribers</li> </ul> <p>Considerations for deployment:</p> <ul> <li>Model shows robust performance on validation data</li> <li>Confusion matrix reveals class-specific performance patterns</li> <li>ROC and PR curves demonstrate strong discriminative power across thresholds</li> </ul>"},{"location":"projetos/classificacao/classificacao/#conclusion","title":"Conclusion","text":"<p>This project successfully implemented a Multi-Layer Perceptron (MLP) from scratch to solve a real-world binary classification problem. The model achieved excellent performance with ROC-AUC &gt; 0.90 on the bank marketing dataset.</p>"},{"location":"projetos/classificacao/classificacao/#key-achievements","title":"Key Achievements","text":"<ol> <li>Complete MLP Implementation \u2014 Built all components from scratch: forward pass, backpropagation, training loop</li> <li>Robust Data Pipeline \u2014 Handled categorical encoding, normalization, and missing values</li> <li>Professional Training Strategy \u2014 Implemented early stopping, model checkpointing, and comprehensive evaluation</li> <li>Excellent Performance \u2014 Achieved strong metrics across multiple evaluation criteria</li> </ol>"},{"location":"projetos/classificacao/classificacao/#limitations-and-future-work","title":"Limitations and Future Work","text":"<p>Current limitations:</p> <ul> <li>Basic SGD optimizer (could benefit from Adam/RMSprop)</li> <li>Fixed architecture (could explore automated architecture search)</li> <li>No advanced regularization techniques (dropout, batch normalization)</li> </ul> <p>Potential improvements:</p> <ul> <li>Implement advanced optimizers for faster convergence</li> <li>Explore ensemble methods for improved robustness</li> <li>Apply cross-validation for more robust performance estimates</li> <li>Investigate feature engineering and selection techniques</li> </ul>"},{"location":"projetos/classificacao/classificacao/#references","title":"References","text":"<ol> <li>Dataset: Kaggle Playground Series 2025 - Bank Marketing Dataset</li> <li>Course Material: Artificial Neural Networks - Insper 2025</li> <li>Libraries: NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn</li> </ol> <p>This implementation demonstrates a deep understanding of neural network fundamentals while achieving practical, real-world performance on a challenging classification task.</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"},{"location":"vae/main/","title":"Exerc\u00edcio 4 - Variational Autoencoder (VAE)","text":"<p>Este relatorio implementa um Variational Autoencoder (VAE) no dataset MNIST para compreender e explorar modelos generativos baseados em aprendizado profundo.</p>"},{"location":"vae/main/#objetivos","title":"Objetivos:","text":""},{"location":"vae/main/#objetivo-geral","title":"Objetivo Geral","text":"<p>Implementar e avaliar um Variational Autoencoder (VAE) no dataset MNIST, compreendendo sua arquitetura, processo de treinamento e capacidades generativas.</p>"},{"location":"vae/main/#objetivos-especificos","title":"Objetivos Espec\u00edficos","text":"<p>Prepara\u00e7\u00e3o de Dados - Carregar e normalizar o dataset MNIST - Dividir em conjuntos de treino e valida\u00e7\u00e3o - Criar DataLoaders para processamento eficiente em batches</p> <p>Implementa\u00e7\u00e3o do Modelo</p> <ul> <li>Definir a arquitetura do VAE (encoder e decoder)</li> <li>Implementar o reparameterization trick</li> <li>Construir a fun\u00e7\u00e3o de loss combinando reconstruction loss e KL divergence</li> </ul> <p>Treinamento</p> <ul> <li>Treinar o VAE no dataset MNIST</li> <li>Monitorar m\u00e9tricas de loss durante o treinamento</li> <li>Gerar reconstru\u00e7\u00f5es peri\u00f3dicas para avaliar progresso</li> </ul> <p>Avalia\u00e7\u00e3o</p> <ul> <li>Avaliar performance no conjunto de valida\u00e7\u00e3o</li> <li>Gerar novas amostras a partir do espa\u00e7o latente aprendido</li> <li>Analisar a qualidade das reconstru\u00e7\u00f5es</li> </ul> <p>Visualiza\u00e7\u00e3o</p> <ul> <li>Comparar imagens originais com reconstru\u00e7\u00f5es</li> <li>Visualizar o espa\u00e7o latente usando t\u00e9cnicas de redu\u00e7\u00e3o de dimensionalidade (t-SNE, PCA)</li> <li>Explorar a organiza\u00e7\u00e3o sem\u00e2ntica do espa\u00e7o latente</li> </ul>"},{"location":"vae/main/#1-importacao-das-bibliotecas","title":"1. Importa\u00e7\u00e3o das Bibliotecas","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport warnings\n</code></pre>"},{"location":"vae/main/#2-preparacao-dos-dados","title":"2. Prepara\u00e7\u00e3o dos dados","text":"<p>Nesta se\u00e7\u00e3o, realizamos as seguintes opera\u00e7\u00f5es:</p> <ol> <li>Carregamento do dataset MNIST</li> <li>Normaliza\u00e7\u00e3o das imagens para o intervalo [0, 1]</li> <li>Divis\u00e3o em conjuntos de treino e valida\u00e7\u00e3o</li> <li>Cria\u00e7\u00e3o dos DataLoaders para processamento em batches</li> </ol> <p>O MNIST cont\u00e9m 60.000 imagens de treino e 10.000 de teste, cada uma com dimens\u00e3o 28x28 pixels em escala de cinza.</p> <pre><code>torch.manual_seed(42)\nnp.random.seed(42)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nBATCH_SIZE = 128\nIMAGE_SIZE = 28 * 28\n\ntransform = transforms.Compose([\n    transforms.ToTensor(), \n])\n\ntrain_dataset = datasets.MNIST(\n    root='./data', \n    train=True, \n    download=True, \n    transform=transform\n)\n\ntest_dataset = datasets.MNIST(\n    root='./data', \n    train=False, \n    download=True, \n    transform=transform\n)\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=False,\n    num_workers=2\n)\n\nprint(f\"Tamanho do dataset de treino: {len(train_dataset)}\")\nprint(f\"Tamanho do dataset de teste: {len(test_dataset)}\")\nprint(f\"Tamanho do batch: {BATCH_SIZE}\")\n\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfig.suptitle('Exemplos do Dataset MNIST', fontsize=16)\n\nfor i, ax in enumerate(axes.flat):\n    img, label = train_dataset[i]\n    ax.imshow(img.squeeze(), cmap='gray')\n    ax.set_title(f'Label: {label}')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"vae/main/#3-arquitetura-do-vae","title":"3. Arquitetura do VAE","text":"<p>Um Variational Autoencoder (VAE) \u00e9 composto por tr\u00eas componentes principais:</p> <ol> <li> <p>Encoder (q(z|x)): Mapeia a entrada x para os par\u00e2metros da distribui\u00e7\u00e3o    latente (m\u00e9dia \u03bc e log-vari\u00e2ncia log(\u03c3\u00b2))</p> </li> <li> <p>Reparameterization Trick: Permite backpropagation atrav\u00e9s de sampling    z = \u03bc + \u03c3 * \u03b5, onde \u03b5 ~ N(0, I)</p> </li> <li> <p>Decoder (p(x|z)): Reconstr\u00f3i a entrada a partir da representa\u00e7\u00e3o latente z</p> </li> </ol>"},{"location":"vae/main/#loss-function","title":"Loss Function","text":"<p>A loss do VAE combina dois termos:</p> <ul> <li>Reconstruction Loss: Mede qu\u00e3o bem o decoder reconstr\u00f3i a entrada</li> <li>KL Divergence: Regulariza o espa\u00e7o latente para ser similar a N(0, I)</li> </ul> <p>L = E[log p(x|z)] - KL[q(z|x) || p(z)]</p> <pre><code>class VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n        \"\"\"\n        Inicializa o Variational Autoencoder.\n\n        Args:\n            input_dim (int): Dimens\u00e3o da entrada (784 para MNIST 28x28)\n            hidden_dim (int): Dimens\u00e3o da camada oculta\n            latent_dim (int): Dimens\u00e3o do espa\u00e7o latente\n        \"\"\"\n        super(VAE, self).__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n\n        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, input_dim)\n\n    def encode(self, x):\n        \"\"\"\n        Encoder: mapeia entrada x para par\u00e2metros da distribui\u00e7\u00e3o latente.\n\n        Args:\n            x (torch.Tensor): Entrada com shape (batch_size, input_dim)\n\n        Returns:\n            mu (torch.Tensor): M\u00e9dia da distribui\u00e7\u00e3o latente\n            logvar (torch.Tensor): Log-vari\u00e2ncia da distribui\u00e7\u00e3o latente\n        \"\"\"\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        \"\"\"\n        Reparameterization Trick: z = \u03bc + \u03c3 * \u03b5\n\n        Esta t\u00e9cnica permite backpropagation atrav\u00e9s da opera\u00e7\u00e3o de sampling.\n        Ao inv\u00e9s de samplear diretamente de N(\u03bc, \u03c3\u00b2), sampleamos \u03b5 ~ N(0, 1)\n        e calculamos z = \u03bc + \u03c3 * \u03b5.\n\n        Args:\n            mu (torch.Tensor): M\u00e9dia\n            logvar (torch.Tensor): Log-vari\u00e2ncia\n\n        Returns:\n            z (torch.Tensor): Amostra do espa\u00e7o latente\n        \"\"\"\n        std = torch.exp(0.5 * logvar)  \n        eps = torch.randn_like(std)     \n        z = mu + eps * std            \n        return z\n\n    def decode(self, z):\n        \"\"\"\n        Decoder: reconstr\u00f3i a entrada a partir da representa\u00e7\u00e3o latente.\n\n        Args:\n            z (torch.Tensor): Representa\u00e7\u00e3o latente\n\n        Returns:\n            reconstruction (torch.Tensor): Imagem reconstru\u00edda\n        \"\"\"\n        h = F.relu(self.fc3(z))\n        reconstruction = torch.sigmoid(self.fc4(h)) \n        return reconstruction\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass completo do VAE.\n\n        Args:\n            x (torch.Tensor): Entrada\n\n        Returns:\n            reconstruction (torch.Tensor): Reconstru\u00e7\u00e3o\n            mu (torch.Tensor): M\u00e9dia da distribui\u00e7\u00e3o latente\n            logvar (torch.Tensor): Log-vari\u00e2ncia da distribui\u00e7\u00e3o latente\n        \"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        reconstruction = self.decode(z)\n        return reconstruction, mu, logvar\n\nLATENT_DIM = 20\nHIDDEN_DIM = 400\n\nmodel = VAE(input_dim=IMAGE_SIZE, hidden_dim=HIDDEN_DIM, latent_dim=LATENT_DIM)\nmodel = model.to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Arquitetura do VAE:\")\nprint(f\"{'='*60}\")\nprint(f\"Dimens\u00e3o de entrada: {IMAGE_SIZE}\")\nprint(f\"Dimens\u00e3o da camada oculta: {HIDDEN_DIM}\")\nprint(f\"Dimens\u00e3o do espa\u00e7o latente: {LATENT_DIM}\")\nprint(f\"Total de par\u00e2metros: {total_params:,}\")\nprint(f\"Par\u00e2metros trein\u00e1veis: {trainable_params:,}\")\nprint(f\"{'='*60}\\n\")\n\nprint(model)\n</code></pre> <pre><code>============================================================\nArquitetura do VAE:\n============================================================\nDimens\u00e3o de entrada: 784\nDimens\u00e3o da camada oculta: 400\nDimens\u00e3o do espa\u00e7o latente: 20\nTotal de par\u00e2metros: 652,824\nPar\u00e2metros trein\u00e1veis: 652,824\n============================================================\n\nVAE(\n  (fc1): Linear(in_features=784, out_features=400, bias=True)\n  (fc_mu): Linear(in_features=400, out_features=20, bias=True)\n  (fc_logvar): Linear(in_features=400, out_features=20, bias=True)\n  (fc3): Linear(in_features=20, out_features=400, bias=True)\n  (fc4): Linear(in_features=400, out_features=784, bias=True)\n)\n</code></pre>"},{"location":"vae/main/#4-loss-function","title":"4. Loss Function","text":"<p>A fun\u00e7\u00e3o de loss do VAE \u00e9 composta por dois termos:</p> <p>Reconstruction Loss (BCE): </p> <ul> <li>Mede a qualidade da reconstru\u00e7\u00e3o</li> <li>Usamos Binary Cross-Entropy pois os pixels est\u00e3o em [0, 1]</li> </ul> <p>KL Divergence Loss:</p> <ul> <li>Regulariza o espa\u00e7o latente</li> <li>For\u00e7a a distribui\u00e7\u00e3o q(z|x) a ser pr\u00f3xima de N(0, I)</li> <li>KL = -0.5 * \u03a3(1 + log(\u03c3\u00b2) - \u03bc\u00b2 - \u03c3\u00b2)</li> </ul> <p>A loss total \u00e9: L = Reconstruction Loss + KL Divergence</p> <pre><code>def vae_loss(reconstruction, x, mu, logvar):\n    \"\"\"\n    Calcula a loss do VAE.\n\n    Args:\n        reconstruction (torch.Tensor): Imagem reconstru\u00edda\n        x (torch.Tensor): Imagem original\n        mu (torch.Tensor): M\u00e9dia da distribui\u00e7\u00e3o latente\n        logvar (torch.Tensor): Log-vari\u00e2ncia da distribui\u00e7\u00e3o latente\n\n    Returns:\n        total_loss (torch.Tensor): Loss total\n        bce_loss (torch.Tensor): Reconstruction loss\n        kld_loss (torch.Tensor): KL divergence loss\n    \"\"\"\n    BCE = F.binary_cross_entropy(reconstruction, x, reduction='sum')\n\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    total_loss = BCE + KLD\n\n    return total_loss, BCE, KLD\n</code></pre>"},{"location":"vae/main/#5-treinamento","title":"5. Treinamento","text":"<p>Nesta se\u00e7\u00e3o, treinamos o VAE monitorando:</p> <ul> <li>Loss total</li> <li>Reconstruction loss</li> <li>KL divergence loss</li> </ul> <p>Salvamos tamb\u00e9m algumas reconstru\u00e7\u00f5es durante o treinamento para visualizar o progresso.</p> <pre><code>LEARNING_RATE = 1e-3\nNUM_EPOCHS = 20\n\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\ntrain_losses = []\ntrain_bce_losses = []\ntrain_kld_losses = []\n\ntest_losses = []\ntest_bce_losses = []\ntest_kld_losses = []\n\ndef train_epoch(model, train_loader, optimizer, epoch):\n    \"\"\"\n    Treina o modelo por uma \u00e9poca.\n    \"\"\"\n    model.train()\n    train_loss = 0\n    train_bce = 0\n    train_kld = 0\n\n    for batch_idx, (data, _) in enumerate(train_loader):\n        data = data.view(-1, IMAGE_SIZE).to(device)\n\n        optimizer.zero_grad()\n\n        reconstruction, mu, logvar = model(data)\n\n        loss, bce, kld = vae_loss(reconstruction, data, mu, logvar)\n\n        loss.backward()\n\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_bce += bce.item()\n        train_kld += kld.item()\n\n        if batch_idx % 100 == 0:\n            print(f'Epoch [{epoch}/{NUM_EPOCHS}] Batch [{batch_idx}/{len(train_loader)}] '\n                  f'Loss: {loss.item()/len(data):.4f} '\n                  f'BCE: {bce.item()/len(data):.4f} '\n                  f'KLD: {kld.item()/len(data):.4f}')\n\n    avg_loss = train_loss / len(train_loader.dataset)\n    avg_bce = train_bce / len(train_loader.dataset)\n    avg_kld = train_kld / len(train_loader.dataset)\n\n    return avg_loss, avg_bce, avg_kld\n\ndef test_epoch(model, test_loader):\n    \"\"\"\n    Avalia o modelo no conjunto de teste.\n    \"\"\"\n    model.eval()\n    test_loss = 0\n    test_bce = 0\n    test_kld = 0\n\n    with torch.no_grad():\n        for data, _ in test_loader:\n            data = data.view(-1, IMAGE_SIZE).to(device)\n            reconstruction, mu, logvar = model(data)\n            loss, bce, kld = vae_loss(reconstruction, data, mu, logvar)\n\n            test_loss += loss.item()\n            test_bce += bce.item()\n            test_kld += kld.item()\n\n    avg_loss = test_loss / len(test_loader.dataset)\n    avg_bce = test_bce / len(test_loader.dataset)\n    avg_kld = test_kld / len(test_loader.dataset)\n\n    return avg_loss, avg_bce, avg_kld\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Iniciando Treinamento\")\nprint(f\"{'='*60}\\n\")\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    train_loss, train_bce, train_kld = train_epoch(model, train_loader, optimizer, epoch)\n    train_losses.append(train_loss)\n    train_bce_losses.append(train_bce)\n    train_kld_losses.append(train_kld)\n\n    test_loss, test_bce, test_kld = test_epoch(model, test_loader)\n    test_losses.append(test_loss)\n    test_bce_losses.append(test_bce)\n    test_kld_losses.append(test_kld)\n\n    print(f'\\n==&gt; Epoch {epoch}/{NUM_EPOCHS}:')\n    print(f'Train Loss: {train_loss:.4f} (BCE: {train_bce:.4f}, KLD: {train_kld:.4f})')\n    print(f'Test Loss: {test_loss:.4f} (BCE: {test_bce:.4f}, KLD: {test_kld:.4f})\\n')\n\nprint(f\"{'='*60}\")\nprint(f\"Treinamento Conclu\u00eddo!\")\nprint(f\"{'='*60}\\n\")\n</code></pre> Saida do Treinamento <pre><code>============================================================\nIniciando Treinamento\n============================================================\n\nEpoch [1/20] Batch [0/469] Loss: 547.7181 BCE: 547.6080 KLD: 0.1102\nEpoch [1/20] Batch [100/469] Loss: 190.9158 BCE: 180.3392 KLD: 10.5767\nEpoch [1/20] Batch [200/469] Loss: 153.1178 BCE: 137.9201 KLD: 15.1977\nEpoch [1/20] Batch [300/469] Loss: 137.8311 BCE: 119.5325 KLD: 18.2987\nEpoch [1/20] Batch [400/469] Loss: 123.4127 BCE: 103.6798 KLD: 19.7329\n\n==&gt; Epoch 1/20:\nTrain Loss: 165.6817 (BCE: 150.3774, KLD: 15.3043)\nTest Loss: 128.3517 (BCE: 107.1814, KLD: 21.1703)\n\nEpoch [2/20] Batch [0/469] Loss: 131.5599 BCE: 110.3438 KLD: 21.2161\nEpoch [2/20] Batch [100/469] Loss: 119.8561 BCE: 98.7648 KLD: 21.0913\nEpoch [2/20] Batch [200/469] Loss: 122.9955 BCE: 99.6710 KLD: 23.3245\nEpoch [2/20] Batch [300/469] Loss: 123.4292 BCE: 99.8748 KLD: 23.5544\nEpoch [2/20] Batch [400/469] Loss: 112.8817 BCE: 88.2455 KLD: 24.6362\n\n==&gt; Epoch 2/20:\nTrain Loss: 121.7666 (BCE: 99.0999, KLD: 22.6667)\nTest Loss: 116.1320 (BCE: 91.9835, KLD: 24.1484)\n\nEpoch [3/20] Batch [0/469] Loss: 118.2178 BCE: 94.1602 KLD: 24.0577\nEpoch [3/20] Batch [100/469] Loss: 113.5944 BCE: 89.3379 KLD: 24.2566\nEpoch [3/20] Batch [200/469] Loss: 120.5640 BCE: 95.0131 KLD: 25.5509\nEpoch [3/20] Batch [300/469] Loss: 116.4105 BCE: 92.0074 KLD: 24.4031\nEpoch [3/20] Batch [400/469] Loss: 114.3827 BCE: 88.7905 KLD: 25.5922\n\n==&gt; Epoch 3/20:\nTrain Loss: 114.8643 (BCE: 90.5292, KLD: 24.3351)\nTest Loss: 112.5289 (BCE: 87.6411, KLD: 24.8878)\n\nEpoch [4/20] Batch [0/469] Loss: 116.6102 BCE: 91.3340 KLD: 25.2762\nEpoch [4/20] Batch [100/469] Loss: 119.1978 BCE: 93.0574 KLD: 26.1404\nEpoch [4/20] Batch [200/469] Loss: 111.8201 BCE: 87.2685 KLD: 24.5516\nEpoch [4/20] Batch [300/469] Loss: 109.3109 BCE: 85.4205 KLD: 23.8904\nEpoch [4/20] Batch [400/469] Loss: 115.0183 BCE: 89.1727 KLD: 25.8456\n\n==&gt; Epoch 4/20:\nTrain Loss: 111.9254 (BCE: 87.0481, KLD: 24.8772)\nTest Loss: 110.1254 (BCE: 84.9148, KLD: 25.2106)\n\nEpoch [5/20] Batch [0/469] Loss: 111.2665 BCE: 86.2037 KLD: 25.0628\nEpoch [5/20] Batch [100/469] Loss: 107.6143 BCE: 83.1450 KLD: 24.4693\nEpoch [5/20] Batch [200/469] Loss: 110.9458 BCE: 85.5570 KLD: 25.3887\nEpoch [5/20] Batch [300/469] Loss: 108.6978 BCE: 83.6044 KLD: 25.0934\nEpoch [5/20] Batch [400/469] Loss: 109.5588 BCE: 84.9173 KLD: 24.6415\n\n==&gt; Epoch 5/20:\nTrain Loss: 110.1515 (BCE: 85.0090, KLD: 25.1425)\nTest Loss: 108.6508 (BCE: 83.2855, KLD: 25.3654)\n\nEpoch [6/20] Batch [0/469] Loss: 110.0747 BCE: 84.8577 KLD: 25.2170\nEpoch [6/20] Batch [100/469] Loss: 110.2866 BCE: 84.2476 KLD: 26.0389\nEpoch [6/20] Batch [200/469] Loss: 108.9948 BCE: 84.0330 KLD: 24.9618\nEpoch [6/20] Batch [300/469] Loss: 105.8406 BCE: 80.2418 KLD: 25.5988\nEpoch [6/20] Batch [400/469] Loss: 110.3239 BCE: 84.0532 KLD: 26.2707\n\n==&gt; Epoch 6/20:\nTrain Loss: 108.9805 (BCE: 83.6647, KLD: 25.3157)\nTest Loss: 107.7639 (BCE: 82.5061, KLD: 25.2578)\n\nEpoch [7/20] Batch [0/469] Loss: 110.3818 BCE: 84.9648 KLD: 25.4171\nEpoch [7/20] Batch [100/469] Loss: 112.2786 BCE: 86.1578 KLD: 26.1208\nEpoch [7/20] Batch [200/469] Loss: 111.4767 BCE: 85.2840 KLD: 26.1927\nEpoch [7/20] Batch [300/469] Loss: 105.9573 BCE: 81.0529 KLD: 24.9044\nEpoch [7/20] Batch [400/469] Loss: 108.9869 BCE: 83.2294 KLD: 25.7575\n\n==&gt; Epoch 7/20:\nTrain Loss: 108.0676 (BCE: 82.6637, KLD: 25.4039)\nTest Loss: 107.1843 (BCE: 81.3881, KLD: 25.7962)\n\nEpoch [8/20] Batch [0/469] Loss: 102.7414 BCE: 77.2972 KLD: 25.4443\nEpoch [8/20] Batch [100/469] Loss: 105.0379 BCE: 80.0685 KLD: 24.9694\nEpoch [8/20] Batch [200/469] Loss: 108.6506 BCE: 82.5692 KLD: 26.0814\nEpoch [8/20] Batch [300/469] Loss: 110.8654 BCE: 84.6671 KLD: 26.1982\nEpoch [8/20] Batch [400/469] Loss: 106.9389 BCE: 80.9872 KLD: 25.9517\n\n==&gt; Epoch 8/20:\nTrain Loss: 107.4442 (BCE: 81.9674, KLD: 25.4767)\nTest Loss: 106.5399 (BCE: 81.4333, KLD: 25.1065)\n\nEpoch [9/20] Batch [0/469] Loss: 108.7018 BCE: 83.4785 KLD: 25.2234\nEpoch [9/20] Batch [100/469] Loss: 106.1763 BCE: 80.7188 KLD: 25.4576\nEpoch [9/20] Batch [200/469] Loss: 109.2445 BCE: 83.2185 KLD: 26.0260\nEpoch [9/20] Batch [300/469] Loss: 105.6352 BCE: 80.0195 KLD: 25.6156\nEpoch [9/20] Batch [400/469] Loss: 103.6626 BCE: 78.6496 KLD: 25.0130\n\n==&gt; Epoch 9/20:\nTrain Loss: 106.9027 (BCE: 81.3975, KLD: 25.5052)\nTest Loss: 106.0948 (BCE: 80.1591, KLD: 25.9357)\n\nEpoch [10/20] Batch [0/469] Loss: 107.6751 BCE: 81.6907 KLD: 25.9844\nEpoch [10/20] Batch [100/469] Loss: 106.8302 BCE: 80.7514 KLD: 26.0788\nEpoch [10/20] Batch [200/469] Loss: 106.7897 BCE: 81.8621 KLD: 24.9276\nEpoch [10/20] Batch [300/469] Loss: 106.3996 BCE: 80.6347 KLD: 25.7649\nEpoch [10/20] Batch [400/469] Loss: 105.7111 BCE: 79.8186 KLD: 25.8925\n\n==&gt; Epoch 10/20:\nTrain Loss: 106.4829 (BCE: 80.9227, KLD: 25.5602)\nTest Loss: 105.8488 (BCE: 80.2044, KLD: 25.6444)\n\nEpoch [11/20] Batch [0/469] Loss: 101.8705 BCE: 77.0654 KLD: 24.8051\nEpoch [11/20] Batch [100/469] Loss: 107.2353 BCE: 81.6814 KLD: 25.5539\nEpoch [11/20] Batch [200/469] Loss: 105.0089 BCE: 79.7209 KLD: 25.2879\nEpoch [11/20] Batch [300/469] Loss: 105.7223 BCE: 80.6989 KLD: 25.0234\nEpoch [11/20] Batch [400/469] Loss: 104.2558 BCE: 78.5459 KLD: 25.7100\n\n==&gt; Epoch 11/20:\nTrain Loss: 106.1213 (BCE: 80.5491, KLD: 25.5723)\nTest Loss: 105.4185 (BCE: 79.9769, KLD: 25.4415)\n\nEpoch [12/20] Batch [0/469] Loss: 104.6272 BCE: 79.3034 KLD: 25.3238\nEpoch [12/20] Batch [100/469] Loss: 103.0406 BCE: 77.8178 KLD: 25.2228\nEpoch [12/20] Batch [200/469] Loss: 104.3270 BCE: 78.5069 KLD: 25.8201\nEpoch [12/20] Batch [300/469] Loss: 102.3923 BCE: 76.3363 KLD: 26.0559\nEpoch [12/20] Batch [400/469] Loss: 108.7696 BCE: 82.4958 KLD: 26.2737\n\n==&gt; Epoch 12/20:\nTrain Loss: 105.8151 (BCE: 80.2293, KLD: 25.5858)\nTest Loss: 105.1260 (BCE: 79.3993, KLD: 25.7267)\n\nEpoch [13/20] Batch [0/469] Loss: 102.8754 BCE: 77.5261 KLD: 25.3492\nEpoch [13/20] Batch [100/469] Loss: 103.4130 BCE: 78.1033 KLD: 25.3097\nEpoch [13/20] Batch [200/469] Loss: 107.2884 BCE: 81.3277 KLD: 25.9607\nEpoch [13/20] Batch [300/469] Loss: 105.8446 BCE: 80.2443 KLD: 25.6003\nEpoch [13/20] Batch [400/469] Loss: 106.8965 BCE: 81.1839 KLD: 25.7126\n\n==&gt; Epoch 13/20:\nTrain Loss: 105.4760 (BCE: 79.8937, KLD: 25.5823)\nTest Loss: 105.0972 (BCE: 79.1370, KLD: 25.9602)\n\nEpoch [14/20] Batch [0/469] Loss: 106.5575 BCE: 80.9546 KLD: 25.6029\nEpoch [14/20] Batch [100/469] Loss: 104.4511 BCE: 78.6633 KLD: 25.7878\nEpoch [14/20] Batch [200/469] Loss: 102.9126 BCE: 77.0436 KLD: 25.8691\nEpoch [14/20] Batch [300/469] Loss: 104.6314 BCE: 79.6544 KLD: 24.9769\nEpoch [14/20] Batch [400/469] Loss: 102.4218 BCE: 77.2318 KLD: 25.1900\n\n==&gt; Epoch 14/20:\nTrain Loss: 105.2852 (BCE: 79.6551, KLD: 25.6301)\nTest Loss: 104.8909 (BCE: 79.1134, KLD: 25.7775)\n\nEpoch [15/20] Batch [0/469] Loss: 104.8589 BCE: 79.2806 KLD: 25.5784\nEpoch [15/20] Batch [100/469] Loss: 104.0095 BCE: 77.9567 KLD: 26.0528\nEpoch [15/20] Batch [200/469] Loss: 108.7142 BCE: 82.3233 KLD: 26.3909\nEpoch [15/20] Batch [300/469] Loss: 107.1299 BCE: 80.6516 KLD: 26.4783\nEpoch [15/20] Batch [400/469] Loss: 102.4967 BCE: 77.3740 KLD: 25.1227\n\n==&gt; Epoch 15/20:\nTrain Loss: 105.0008 (BCE: 79.3890, KLD: 25.6118)\nTest Loss: 104.6754 (BCE: 78.9176, KLD: 25.7578)\n\nEpoch [16/20] Batch [0/469] Loss: 103.8095 BCE: 78.5377 KLD: 25.2719\nEpoch [16/20] Batch [100/469] Loss: 105.6003 BCE: 79.8592 KLD: 25.7412\nEpoch [16/20] Batch [200/469] Loss: 102.7085 BCE: 77.2939 KLD: 25.4146\nEpoch [16/20] Batch [300/469] Loss: 103.4191 BCE: 78.3936 KLD: 25.0255\nEpoch [16/20] Batch [400/469] Loss: 102.1266 BCE: 77.2972 KLD: 24.8294\n\n==&gt; Epoch 16/20:\nTrain Loss: 104.8295 (BCE: 79.1741, KLD: 25.6554)\nTest Loss: 104.5653 (BCE: 79.5856, KLD: 24.9798)\n\nEpoch [17/20] Batch [0/469] Loss: 102.1342 BCE: 77.5616 KLD: 24.5726\nEpoch [17/20] Batch [100/469] Loss: 110.1351 BCE: 83.2960 KLD: 26.8391\nEpoch [17/20] Batch [200/469] Loss: 100.4841 BCE: 74.5434 KLD: 25.9407\nEpoch [17/20] Batch [300/469] Loss: 104.9085 BCE: 79.6293 KLD: 25.2792\nEpoch [17/20] Batch [400/469] Loss: 104.9729 BCE: 79.0714 KLD: 25.9015\n\n==&gt; Epoch 17/20:\nTrain Loss: 104.6250 (BCE: 79.0165, KLD: 25.6084)\nTest Loss: 104.1926 (BCE: 78.7366, KLD: 25.4559)\n\nEpoch [18/20] Batch [0/469] Loss: 105.9602 BCE: 80.0777 KLD: 25.8824\nEpoch [18/20] Batch [100/469] Loss: 105.1729 BCE: 79.5517 KLD: 25.6212\nEpoch [18/20] Batch [200/469] Loss: 106.0764 BCE: 80.7828 KLD: 25.2936\nEpoch [18/20] Batch [300/469] Loss: 102.0675 BCE: 77.0342 KLD: 25.0333\nEpoch [18/20] Batch [400/469] Loss: 99.6716 BCE: 74.4532 KLD: 25.2183\n\n==&gt; Epoch 18/20:\nTrain Loss: 104.4636 (BCE: 78.8489, KLD: 25.6148)\nTest Loss: 104.1558 (BCE: 78.5155, KLD: 25.6403)\n\nEpoch [19/20] Batch [0/469] Loss: 104.8492 BCE: 78.9228 KLD: 25.9265\nEpoch [19/20] Batch [100/469] Loss: 104.5966 BCE: 79.1284 KLD: 25.4682\nEpoch [19/20] Batch [200/469] Loss: 100.7931 BCE: 75.3777 KLD: 25.4154\nEpoch [19/20] Batch [300/469] Loss: 102.8889 BCE: 77.4148 KLD: 25.4741\nEpoch [19/20] Batch [400/469] Loss: 105.5467 BCE: 79.8940 KLD: 25.6527\n\n==&gt; Epoch 19/20:\nTrain Loss: 104.3119 (BCE: 78.6938, KLD: 25.6181)\nTest Loss: 104.2359 (BCE: 78.7243, KLD: 25.5117)\n\nEpoch [20/20] Batch [0/469] Loss: 110.6339 BCE: 84.1829 KLD: 26.4509\nEpoch [20/20] Batch [100/469] Loss: 106.6272 BCE: 80.3633 KLD: 26.2639\nEpoch [20/20] Batch [200/469] Loss: 102.1574 BCE: 76.8002 KLD: 25.3573\nEpoch [20/20] Batch [300/469] Loss: 101.8267 BCE: 76.5448 KLD: 25.2819\nEpoch [20/20] Batch [400/469] Loss: 104.8664 BCE: 79.3323 KLD: 25.5341\n\n==&gt; Epoch 20/20:\nTrain Loss: 104.1215 (BCE: 78.5025, KLD: 25.6190)\nTest Loss: 103.8884 (BCE: 78.2692, KLD: 25.6192)\n\n============================================================\nTreinamento Conclu\u00eddo!\n============================================================\n</code></pre>"},{"location":"vae/main/#6-visualizacao-do-treinamento","title":"6. Visualiza\u00e7\u00e3o do Treinamento","text":"<p>Visualizamos as curvas de loss durante o treinamento para avaliar:</p> <ul> <li>Converg\u00eancia do modelo</li> <li>Comportamento da reconstruction loss e KL divergence</li> <li>Poss\u00edvel overfitting</li> </ul> <pre><code>fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].plot(train_losses, label='Train', linewidth=2)\naxes[0].plot(test_losses, label='Test', linewidth=2)\naxes[0].set_xlabel('\u00c9poca', fontsize=12)\naxes[0].set_ylabel('Loss Total', fontsize=12)\naxes[0].set_title('Loss Total ao Longo do Treinamento', fontsize=14, fontweight='bold')\naxes[0].legend(fontsize=11)\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(train_bce_losses, label='Train BCE', linewidth=2, color='orange')\naxes[1].plot(test_bce_losses, label='Test BCE', linewidth=2, color='red')\naxes[1].set_xlabel('\u00c9poca', fontsize=12)\naxes[1].set_ylabel('Reconstruction Loss (BCE)', fontsize=12)\naxes[1].set_title('Reconstruction Loss', fontsize=14, fontweight='bold')\naxes[1].legend(fontsize=11)\naxes[1].grid(True, alpha=0.3)\n\naxes[2].plot(train_kld_losses, label='Train KLD', linewidth=2, color='green')\naxes[2].plot(test_kld_losses, label='Test KLD', linewidth=2, color='purple')\naxes[2].set_xlabel('\u00c9poca', fontsize=12)\naxes[2].set_ylabel('KL Divergence', fontsize=12)\naxes[2].set_title('KL Divergence', fontsize=14, fontweight='bold')\naxes[2].legend(fontsize=11)\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>An\u00e1lise das Curvas de Loss:</p> <ul> <li>Loss final de treino: 104.1215</li> <li>Loss final de teste: 103.8884</li> <li>BCE final (teste): 78.2692</li> <li>KLD final (teste): 25.6192</li> </ul>"},{"location":"vae/main/#7-visualizacao-de-reconstrucoes","title":"7. Visualiza\u00e7\u00e3o de Reconstru\u00e7\u00f5es","text":"<p>Comparamos imagens originais com suas reconstru\u00e7\u00f5es para avaliar a qualidade do decoder.</p> <pre><code>model.eval()\n\nwith torch.no_grad():\n    test_images, test_labels = next(iter(test_loader))\n    test_images = test_images.view(-1, IMAGE_SIZE).to(device)\n\n    reconstructions, mu, logvar = model(test_images)\n\n    test_images = test_images.cpu()\n    reconstructions = reconstructions.cpu()\n\nn_images = 10\nfig, axes = plt.subplots(2, n_images, figsize=(20, 4))\nfig.suptitle('Compara\u00e7\u00e3o: Imagens Originais vs Reconstru\u00e7\u00f5es', fontsize=16, fontweight='bold')\n\nfor i in range(n_images):\n    axes[0, i].imshow(test_images[i].view(28, 28), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_ylabel('Original', fontsize=12, fontweight='bold')\n\n    axes[1, i].imshow(reconstructions[i].view(28, 28), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_ylabel('Reconstru\u00edda', fontsize=12, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"vae/main/#8-sampling-gerando-novas-imagens","title":"8. Sampling: Gerando Novas Imagens","text":"<p>Uma das principais vantagens do VAE \u00e9 a capacidade de gerar novas amostras. Sampleamos pontos aleat\u00f3rios do espa\u00e7o latente N(0, I) e decodificamos para gerar novos d\u00edgitos.</p> <pre><code>model.eval()\n\nwith torch.no_grad():\n    z = torch.randn(64, LATENT_DIM).to(device)\n\n    generated_images = model.decode(z).cpu()\n\nfig, axes = plt.subplots(8, 8, figsize=(12, 12))\nfig.suptitle('Amostras Geradas pelo VAE', fontsize=16, fontweight='bold')\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(generated_images[i].view(28, 28), cmap='gray')\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"vae/main/#9-visualizacao-do-espaco-latente","title":"9. Visualiza\u00e7\u00e3o do espa\u00e7o latente","text":"<p>Visualizamos o espa\u00e7o latente aprendido pelo encoder usando t\u00e9cnicas de redu\u00e7\u00e3o de dimensionalidade (t-SNE e PCA).</p> <p>Como o espa\u00e7o latente tem 20 dimens\u00f5es, usamos t-SNE ou PCA para reduzir para 2D e visualizar.</p> <pre><code>model.eval()\n\nlatent_vectors = []\nlabels_list = []\n\nwith torch.no_grad():\n    for data, labels in test_loader:\n        data = data.view(-1, IMAGE_SIZE).to(device)\n        mu, logvar = model.encode(data)\n        latent_vectors.append(mu.cpu().numpy())\n        labels_list.append(labels.numpy())\n\nlatent_vectors = np.concatenate(latent_vectors, axis=0)\nlabels_array = np.concatenate(labels_list, axis=0)\n\nprint(f\"Shape do espa\u00e7o latente: {latent_vectors.shape}\")\nprint(f\"Shape dos labels: {labels_array.shape}\")\n\nprint(\"\\nAplicando t-SNE\")\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nlatent_2d_tsne = tsne.fit_transform(latent_vectors)\n\nprint(\"Aplicando PCA\")\npca = PCA(n_components=2, random_state=42)\nlatent_2d_pca = pca.fit_transform(latent_vectors)\n\nprint(f\"Vari\u00e2ncia explicada pelo PCA: {pca.explained_variance_ratio_.sum():.2%}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 7))\n\nscatter1 = axes[0].scatter(latent_2d_tsne[:, 0], latent_2d_tsne[:, 1], \n                           c=labels_array, cmap='tab10', s=5, alpha=0.6)\naxes[0].set_title('Espa\u00e7o Latente Visualizado com t-SNE', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Componente 1', fontsize=12)\naxes[0].set_ylabel('Componente 2', fontsize=12)\naxes[0].grid(True, alpha=0.3)\ncbar1 = plt.colorbar(scatter1, ax=axes[0])\ncbar1.set_label('D\u00edgito', fontsize=11)\n\nscatter2 = axes[1].scatter(latent_2d_pca[:, 0], latent_2d_pca[:, 1], \n                           c=labels_array, cmap='tab10', s=5, alpha=0.6)\naxes[1].set_title('Espa\u00e7o Latente Visualizado com PCA', fontsize=14, fontweight='bold')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} var.)', fontsize=12)\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} var.)', fontsize=12)\naxes[1].grid(True, alpha=0.3)\ncbar2 = plt.colorbar(scatter2, ax=axes[1])\ncbar2.set_label('D\u00edgito', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <pre><code>Shape do espa\u00e7o latente: (10000, 20)\nShape dos labels: (10000,)\n\nAplicando t-SNE\nAplicando PCA\nVari\u00e2ncia explicada pelo PCA: 16.17%\n</code></pre> <p></p> <p>An\u00e1lise do Espa\u00e7o Latente:</p> <ul> <li>O espa\u00e7o latente mostra como o VAE organizou as diferentes classes.</li> <li>Clusters bem definidos indicam que o modelo aprendeu representa\u00e7\u00f5es significativas que separam diferentes d\u00edgitos.</li> </ul>"},{"location":"vae/main/#10-conclusao-e-analise-final","title":"10. Conclus\u00e3o e An\u00e1lise Final","text":""},{"location":"vae/main/#resumo-da-implementacao","title":"Resumo da Implementa\u00e7\u00e3o","text":"<p>Neste exerc\u00edcio, implementamos com sucesso um Variational Autoencoder (VAE) para o dataset MNIST, explorando todos os componentes fundamentais desta arquitetura generativa. O VAE foi constru\u00eddo com:</p> <ul> <li>Espa\u00e7o latente de 20 dimens\u00f5es</li> <li>Camada oculta de 400 neur\u00f4nios</li> <li>Arquitetura totalmente conectada (Fully Connected)</li> <li>Treinamento por 20 \u00e9pocas com otimizador Adam</li> </ul>"},{"location":"vae/main/#principais-resultados","title":"Principais Resultados","text":""},{"location":"vae/main/#1-convergencia-do-treinamento","title":"1. Converg\u00eancia do Treinamento","text":"<p>O modelo demonstrou converg\u00eancia adequada durante o treinamento:</p> <ul> <li>A loss total diminuiu consistentemente ao longo das \u00e9pocas</li> <li>As curvas de treino e teste permaneceram pr\u00f3ximas, indicando aus\u00eancia de overfitting significativo</li> <li>A Reconstruction Loss (BCE) capturou a qualidade das reconstru\u00e7\u00f5es</li> <li>A KL Divergence manteve-se est\u00e1vel, regularizando o espa\u00e7o latente</li> </ul>"},{"location":"vae/main/#2-qualidade-das-reconstrucoes","title":"2. Qualidade das Reconstru\u00e7\u00f5es","text":"<p>As reconstru\u00e7\u00f5es geradas pelo decoder mostraram:</p> <ul> <li>Capacidade de preservar as caracter\u00edsticas principais dos d\u00edgitos</li> <li>Algumas perdas de detalhes finos devido \u00e0 compress\u00e3o no espa\u00e7o latente de 20 dimens\u00f5es</li> <li>Trade-off entre fidelidade e capacidade generativa</li> </ul>"},{"location":"vae/main/#3-geracao-de-novas-amostras","title":"3. Gera\u00e7\u00e3o de Novas Amostras","text":"<p>O VAE demonstrou capacidade generativa ao samplear do prior N(0, I):</p> <ul> <li>Gera\u00e7\u00e3o de d\u00edgitos plaus\u00edveis a partir de vetores aleat\u00f3rios</li> <li>Diversidade nas amostras geradas</li> <li>Alguns exemplos apresentaram caracter\u00edsticas \"misturadas\" entre classes, evidenciando a natureza cont\u00ednua do espa\u00e7o latente</li> </ul>"},{"location":"vae/main/#4-organizacao-do-espaco-latente","title":"4. Organiza\u00e7\u00e3o do Espa\u00e7o Latente","text":"<p>As visualiza\u00e7\u00f5es com t-SNE e PCA revelaram:</p> <ul> <li>Clusters parcialmente separados para diferentes d\u00edgitos</li> <li>Organiza\u00e7\u00e3o sem\u00e2ntica onde d\u00edgitos similares (ex: 3 e 8, 4 e 9) aparecem pr\u00f3ximos</li> <li>O espa\u00e7o latente aprendeu representa\u00e7\u00f5es significativas que capturam varia\u00e7\u00f5es intra-classe e inter-classe</li> </ul>"},{"location":"vae/main/#conclusao-final","title":"Conclus\u00e3o Final","text":"<p>Este exerc\u00edcio demonstrou com sucesso a implementa\u00e7\u00e3o e an\u00e1lise de um Variational Autoencoder, explorando seus componentes fundamentais: encoder, decoder, reparameterization trick, e fun\u00e7\u00e3o de loss dual. O VAE mostrou-se capaz de aprender representa\u00e7\u00f5es latentes significativas do MNIST, com capacidade tanto de reconstru\u00e7\u00e3o quanto de gera\u00e7\u00e3o de novas amostras.</p>"}]}