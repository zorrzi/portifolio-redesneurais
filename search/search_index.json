{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Redes Neurais e Deep Learning \u2014 Insper","text":"<p>Este portf\u00f3lio re\u00fane as atividades e projetos desenvolvidos na disciplina de Redes Neurais e Deep Learning do Insper. O objetivo \u00e9 documentar de forma clara e organizada cada etapa do aprendizado, desde os primeiros conceitos de separabilidade de dados at\u00e9 a implementa\u00e7\u00e3o de modelos mais avan\u00e7ados, como redes neurais multicamadas e modelos generativos.</p> <p>Ao longo deste material, est\u00e3o descritos:</p> <ul> <li>A motiva\u00e7\u00e3o de cada exerc\u00edcio ou projeto,</li> <li>O passo a passo de implementa\u00e7\u00e3o,</li> <li>As an\u00e1lises dos resultados obtidos,</li> <li>E as conclus\u00f5es em rela\u00e7\u00e3o ao uso de redes neurais para diferentes problemas.</li> </ul>"},{"location":"#estrutura-do-portfolio","title":"Estrutura do Portf\u00f3lio","text":""},{"location":"#exercicios","title":"\ud83d\udcdd Exerc\u00edcios","text":"<p>Os exerc\u00edcios pr\u00e1ticos t\u00eam como foco a explora\u00e7\u00e3o de conceitos fundamentais de redes neurais.  </p> <ol> <li>Data \u2014 Gera\u00e7\u00e3o e an\u00e1lise de dados sint\u00e9ticos para explorar separabilidade de classes e limites de decis\u00e3o.  </li> <li>Perceptron \u2014 Implementa\u00e7\u00e3o e avalia\u00e7\u00e3o de um perceptron simples para problemas linearmente separ\u00e1veis.  </li> <li>MLP (Multi-Layer Perceptron) \u2014 Constru\u00e7\u00e3o e treinamento de uma rede neural multicamadas para lidar com problemas n\u00e3o lineares.  </li> <li>Metrics \u2014 An\u00e1lise de m\u00e9tricas de avalia\u00e7\u00e3o, discutindo acur\u00e1cia, precis\u00e3o, recall e F1-score no contexto de classifica\u00e7\u00e3o.</li> </ol>"},{"location":"#projetos","title":"\ud83d\ude80 Projetos","text":"<p>Os projetos aplicam os conceitos estudados em problemas mais complexos e realistas.  </p> <ol> <li>Classification \u2014 Modelos de classifica\u00e7\u00e3o em diferentes conjuntos de dados, explorando arquiteturas de redes neurais.  </li> <li>Regression \u2014 Aplica\u00e7\u00e3o de redes neurais em tarefas de regress\u00e3o, analisando desempenho e capacidade de generaliza\u00e7\u00e3o.  </li> <li>Generative Models \u2014 Estudo e implementa\u00e7\u00e3o de modelos generativos (como autoencoders ou GANs), avaliando seu potencial em criar ou reconstruir dados.</li> </ol>"},{"location":"#status-de-desenvolvimento","title":"\ud83d\udccc Status de Desenvolvimento","text":"<ul> <li> Exerc\u00edcio 1 (Data) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 2 (Perceptron) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 3 (MLP) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 4 (Metrics) conclu\u00eddo e documentado.  </li> <li>[] Projeto 1 (Classification) em andamento.  </li> <li>[] Projeto 2 (Regression) em andamento.  </li> <li>[] Projeto 3 (Generative Models) planejado para pr\u00f3xima etapa.  </li> </ul>"},{"location":"#conclusao","title":"\ud83c\udfaf Conclus\u00e3o","text":"<p>Este portf\u00f3lio funciona como um di\u00e1rio de bordo da disciplina, mostrando n\u00e3o apenas os c\u00f3digos implementados, mas tamb\u00e9m as reflex\u00f5es sobre os resultados e os aprendizados obtidos. A ideia \u00e9 que ele sirva como refer\u00eancia tanto para revisitar conceitos importantes quanto para inspirar futuros trabalhos na \u00e1rea de Intelig\u00eancia Artificial aplicada a Redes Neurais.</p>"},{"location":"data/exercicio1/exercicio1/","title":"Exerc\u00edcio 1 \u2014 Data","text":""},{"location":"data/exercicio1/exercicio1/#objetivo","title":"Objetivo","text":"<p>O objetivo deste exerc\u00edcio foi gerar um conjunto de dados sint\u00e9ticos em 2 dimens\u00f5es para analisar sua separabilidade entre classes. Esse processo \u00e9 importante porque ajuda a entender como uma rede neural simples ou mais profunda teria que se adaptar para classificar os dados corretamente.</p>"},{"location":"data/exercicio1/exercicio1/#etapa-1-geracao-dos-dados-sinteticos","title":"Etapa 1 \u2014 Gera\u00e7\u00e3o dos Dados Sint\u00e9ticos","text":"<p>O objetivo desta etapa foi criar um conjunto de dados bidimensionais (vari\u00e1veis <code>x1</code> e <code>x2</code>) divididos em 4 classes distintas, cada uma com 100 pontos, totalizando 400 amostras.</p> <p>Para garantir reprodutibilidade, defini a semente do gerador de n\u00fameros aleat\u00f3rios (<code>np.random.seed(42)</code>), o que faz com que os mesmos pontos sejam gerados a cada execu\u00e7\u00e3o do c\u00f3digo.</p> <p>Cada classe foi gerada usando a fun\u00e7\u00e3o <code>np.random.normal</code>, que amostra valores de uma distribui\u00e7\u00e3o normal (gaussiana) a partir de um valor m\u00e9dio (<code>loc</code>) e um desvio padr\u00e3o (<code>scale</code>). Assim:</p> <ul> <li>Classe 0: centrada em (2, 3), mais espalhada no eixo y (<code>scale=2.5</code>).  </li> <li>Classe 1: centrada em (5, 6), com varia\u00e7\u00e3o moderada em ambos os eixos.  </li> <li>Classe 2: centrada em (8, 1), compacta em torno da m\u00e9dia.  </li> <li>Classe 3: centrada em (15, 4), bem concentrada em <code>x1</code> mas com maior varia\u00e7\u00e3o em <code>x2</code>.</li> </ul> <p>Por fim, usei <code>np.column_stack</code> para juntar as duas vari\u00e1veis (<code>x1</code>, <code>x2</code>) em um array bidimensional, representando os pontos de cada classe.</p> <pre><code>np.random.seed(42)  \nn = 100\n\n# Classe 0\nc0_x1 = np.random.normal(loc=2, scale=0.8, size=n)\nc0_x2 = np.random.normal(loc=3, scale=2.5, size=n)\nc0 = np.column_stack((c0_x1, c0_x2))\n\n# Classe 1\nc1_x1 = np.random.normal(loc=5, scale=1.2, size=n)\nc1_x2 = np.random.normal(loc=6, scale=1.9, size=n)\nc1 = np.column_stack((c1_x1, c1_x2))\n\n# Classe 2\nc2_x1 = np.random.normal(loc=8, scale=0.9, size=n)\nc2_x2 = np.random.normal(loc=1, scale=0.9, size=n)\nc2 = np.column_stack((c2_x1, c2_x2))\n\n# Classe 3\nc3_x1 = np.random.normal(loc=15, scale=0.5, size=n)\nc3_x2 = np.random.normal(loc=4, scale=2.0, size=n)\nc3 = np.column_stack((c3_x1, c3_x2))\n</code></pre>"},{"location":"data/exercicio1/exercicio1/#etapa-2-visualizacao-dos-dados","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o dos Dados","text":"<p>Com as quatro classes j\u00e1 geradas, o pr\u00f3ximo passo foi visualizar a distribui\u00e7\u00e3o dos pontos em um gr\u00e1fico de dispers\u00e3o (scatter plot).</p> <p>Esse gr\u00e1fico \u00e9 essencial porque permite observar a separabilidade entre classes e j\u00e1 adianta poss\u00edveis regi\u00f5es de sobreposi\u00e7\u00e3o que precisaremos analisar na pr\u00f3xima etapa.</p> <pre><code>plt.figure(figsize=(7,5))\nplt.scatter(c0[:,0], c0[:,1], label=\"Class 0\", s=18, alpha=0.8)\nplt.scatter(c1[:,0], c1[:,1], label=\"Class 1\", s=18, alpha=0.8)\nplt.scatter(c2[:,0], c2[:,1], label=\"Class 2\", s=18, alpha=0.8)\nplt.scatter(c3[:,0], c3[:,1], label=\"Class 3\", s=18, alpha=0.8)\n\nplt.title(\"Data\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p> <p>Observando o gr\u00e1fico de dispers\u00e3o obtido:</p> <ul> <li> <p>Classe 0 (azul): est\u00e1 localizada \u00e0 esquerda, centrada em torno de <code>x1 \u2248 2</code>.   Apresenta grande varia\u00e7\u00e3o em <code>x2</code>, formando uma nuvem vertical que vai de valores negativos at\u00e9 acima de 7.  </p> </li> <li> <p>Classe 1 (laranja): posicionada mais ao centro, em <code>x1 \u2248 5</code>.   Distribui-se em n\u00edveis mais altos de <code>x2</code> (acima de 5), com dispers\u00e3o consider\u00e1vel, o que gera sobreposi\u00e7\u00e3o com a Classe 0 em algumas regi\u00f5es.  </p> </li> <li> <p>Classe 2 (verde): centrada em <code>x1 \u2248 8</code> e valores baixos de <code>x2</code> (por volta de 1).   \u00c9 a classe mais compacta, com baixa dispers\u00e3o, o que facilita sua identifica\u00e7\u00e3o. No entanto, apresenta certa proximidade com a Classe 1 na regi\u00e3o de fronteira.  </p> </li> <li> <p>Classe 3 (vermelha): bem afastada das demais, em <code>x1 \u2248 15</code>.   Mesmo com varia\u00e7\u00e3o em <code>x2</code>, mant\u00e9m-se completamente separada das outras classes, o que torna sua classifica\u00e7\u00e3o a mais simples do conjunto.</p> </li> </ul>"},{"location":"data/exercicio1/exercicio1/#conclusoes-sobre-separabilidade","title":"Conclus\u00f5es sobre separabilidade","text":"<ul> <li>Existe uma ordem clara das classes ao longo do eixo <code>x1</code>: C0 \u2192 C1 \u2192 C2 \u2192 C3.  </li> <li>Por\u00e9m, as classes 0 e 1 apresentam sobreposi\u00e7\u00e3o em parte do espa\u00e7o (principalmente entre <code>x1=2</code> e <code>x1=5</code>), o que dificulta a separa\u00e7\u00e3o por um limite totalmente linear.  </li> <li>A Classe 2 \u00e9 bem definida e isolada verticalmente, embora esteja pr\u00f3xima da Classe 1 na horizontal.  </li> <li>A Classe 3 est\u00e1 completamente isolada, sendo a mais f\u00e1cil de separar.  </li> </ul>"},{"location":"data/exercicio1/exercicio1/#etapa-4-definicao-de-fronteiras-lineares-simples","title":"Etapa 4 \u2014 Defini\u00e7\u00e3o de Fronteiras Lineares Simples","text":"<p>Para explorar a separabilidade dos dados, tracei fronteiras lineares verticais ao longo do eixo <code>x1</code>, representando limites de decis\u00e3o iniciais entre as classes.</p> <pre><code>xlines = [\n    (2 + 5) / 2,   \n    (5 + 8) / 2,   \n    (8 + 15) / 2   \n]\n\nplt.figure(figsize=(7,5))\nplt.scatter(c0[:,0], c0[:,1], label=\"Class 0\", s=18, alpha=0.8)\nplt.scatter(c1[:,0], c1[:,1], label=\"Class 1\", s=18, alpha=0.8)\nplt.scatter(c2[:,0], c2[:,1], label=\"Class 2\", s=18, alpha=0.8)\nplt.scatter(c3[:,0], c3[:,1], label=\"Class 3\", s=18, alpha=0.8)\n\nfor x in xlines:\n    plt.axvline(x, linestyle=\"--\", linewidth=2)\n\nplt.title(\"Scatter + fronteiras lineares (simples)\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Divis\u00e3o das classes</p>"},{"location":"data/exercicio1/exercicio1/#analise","title":"An\u00e1lise","text":"<ul> <li>Esse m\u00e9todo de fronteira \u00e9 simples e intuitivo, funcionando bem quando as classes est\u00e3o distribu\u00eddas principalmente em torno de valores diferentes de <code>x1</code>.  </li> <li>No entanto, ele n\u00e3o considera a dispers\u00e3o em <code>x2</code>, o que gera problemas de sobreposi\u00e7\u00e3o:  </li> <li>Classes 0 e 1 continuam com regi\u00f5es de confus\u00e3o, pois se sobrep\u00f5em verticalmente.  </li> <li>Classes 1 e 2 tamb\u00e9m podem apresentar mistura em regi\u00f5es pr\u00f3ximas \u00e0 linha divis\u00f3ria.  </li> <li>A Classe 3, por estar bem afastada, \u00e9 separada de forma perfeita com esse limite linear.</li> </ul>"},{"location":"data/exercicio1/exercicio1/#implicacao-para-redes-neurais","title":"Implica\u00e7\u00e3o para redes neurais","text":"<ul> <li>Um modelo linear simples poderia usar limites parecidos com estes para classificar os dados.  </li> <li>Por\u00e9m, como h\u00e1 sobreposi\u00e7\u00e3o entre classes, uma rede neural com m\u00faltiplas camadas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares teria mais flexibilidade para ajustar fronteiras curvas ou inclinadas, capturando melhor as regi\u00f5es de confus\u00e3o.</li> </ul>"},{"location":"data/exercicio2/exercicio2/","title":"Exercicio 2 - Data","text":""},{"location":"data/exercicio2/exercicio2/#objetivo","title":"Objetivo","text":"<p>O objetivo deste exerc\u00edcio foi criar dois conjuntos de dados em 5 dimens\u00f5es (Classes A e B), cada um com 500 amostras, a partir de distribui\u00e7\u00f5es normais multivariadas. Em seguida, reduzir a dimensionalidade com PCA para 2D e analisar a separabilidade linear.  </p> <p>Minha hip\u00f3tese inicial era de que, como as classes possuem covari\u00e2ncias diferentes, a fronteira \u00f3tima n\u00e3o seria linear, representando um desafio para modelos simples como Perceptron ou Regress\u00e3o Log\u00edstica, e justificando o uso de modelos mais complexos (ex: redes neurais).</p>"},{"location":"data/exercicio2/exercicio2/#etapa-1-definicao-dos-parametros-e-semente-do-gerador","title":"Etapa 1 \u2014 Defini\u00e7\u00e3o dos par\u00e2metros e semente do gerador","text":"<p>Aqui eu preparei o gerador de n\u00fameros aleat\u00f3rios e defini os par\u00e2metros das distribui\u00e7\u00f5es normais multivariadas para as duas classes (A e B).</p> <ul> <li> <p><code>rng = np.random.default_rng(42)</code>   Criei um gerador pseudoaleat\u00f3rio com semente 42 para garantir reprodutibilidade. Isso significa que, ao rodar o c\u00f3digo v\u00e1rias vezes, os mesmos dados ser\u00e3o gerados.</p> </li> <li> <p><code>mu_A</code> e <code>mu_B</code>   S\u00e3o os vetores de m\u00e9dias das classes, cada um com 5 dimens\u00f5es.  </p> </li> <li>Classe A \u00e9 centrada na origem.  </li> <li> <p>Classe B \u00e9 deslocada em rela\u00e7\u00e3o \u00e0 A (todas as m\u00e9dias iguais a 1.5).</p> </li> <li> <p><code>Sigma_A</code> e <code>Sigma_B</code>   S\u00e3o as matrizes de covari\u00e2ncia (5\u00d75) que controlam o espalhamento e as correla\u00e7\u00f5es entre as vari\u00e1veis de cada classe.  </p> </li> <li>Em A, h\u00e1 correla\u00e7\u00f5es positivas entre algumas vari\u00e1veis.  </li> <li>Em B, h\u00e1 correla\u00e7\u00f5es negativas e vari\u00e2ncias maiores.  </li> </ul> <p>Esses par\u00e2metros ser\u00e3o usados na fun\u00e7\u00e3o <code>rng.multivariate_normal</code> para gerar os pontos de cada classe na etapa seguinte.</p> <pre><code>rng = np.random.default_rng(42)\n\nmu_A = np.array([0., 0., 0., 0., 0.])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n])\n\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n])\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-2-amostragem-dos-dados-e-criacao-dos-rotulos","title":"Etapa 2 \u2014 Amostragem dos dados e cria\u00e7\u00e3o dos r\u00f3tulos","text":"<p>Nesta parte eu realmente gerei os pontos das duas classes e preparei os conjuntos de dados e r\u00f3tulos.</p> <ul> <li> <p><code>nA = nB = 500</code>   Defini que cada classe ter\u00e1 500 amostras, totalizando 1000 pontos.</p> </li> <li> <p><code>rng.multivariate_normal(mean=..., cov=..., size=...)</code>   Fun\u00e7\u00e3o que gera amostras de uma distribui\u00e7\u00e3o normal multivariada.  </p> </li> <li><code>mean</code> \u2192 vetor de m\u00e9dias da classe.  </li> <li><code>cov</code> \u2192 matriz de covari\u00e2ncia que define vari\u00e2ncias e correla\u00e7\u00f5es.  </li> <li> <p><code>size</code> \u2192 n\u00famero de amostras a serem geradas.   Assim, <code>A</code> cont\u00e9m os pontos da Classe A (500\u00d75) e <code>B</code> os da Classe B (500\u00d75).</p> </li> <li> <p><code>np.vstack([A, B])</code>   Empilhei os dois conjuntos verticalmente, formando a matriz <code>X</code> com shape (1000, 5).</p> </li> <li> <p><code>np.hstack([np.zeros(nA, int), np.ones(nB, int)])</code>   Criei o vetor de r\u00f3tulos <code>y</code>:  </p> </li> <li><code>0</code> representa a Classe A.  </li> <li><code>1</code> representa a Classe B.   Isso me permite identificar de qual classe cada ponto pertence.</li> </ul> <p>Com isso, foi finalizada a gera\u00e7\u00e3o do dataset 5D completo, pronto para redu\u00e7\u00e3o de dimensionalidade e an\u00e1lise.</p> <pre><code>nA = nB = 500\nA = rng.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nB = rng.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\n\nX = np.vstack([A, B])           \ny = np.hstack([np.zeros(nA, int), np.ones(nB, int)])\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-3-reducao-de-dimensionalidade-com-pca-5d-2d","title":"Etapa 3 \u2014 Redu\u00e7\u00e3o de dimensionalidade com PCA (5D \u2192 2D)","text":"<p>Nesta etapa eu utilizei o PCA (Principal Component Analysis) para reduzir os dados de 5 dimens\u00f5es para apenas 2, permitindo visualiza\u00e7\u00e3o em gr\u00e1fico de dispers\u00e3o.</p> <ul> <li><code>pca = PCA(n_components=2, random_state=42)</code>   Criei um objeto PCA que mant\u00e9m apenas 2 componentes principais.  </li> <li> <p>O PCA encontra dire\u00e7\u00f5es de maior vari\u00e2ncia nos dados, ignorando r\u00f3tulos de classe.</p> </li> <li> <p><code>X2 = pca.fit_transform(X)</code>   Aqui eu ajustei o PCA aos dados 5D (<code>fit</code>) e projetei os pontos nesses dois eixos principais (<code>transform</code>).   O resultado \u00e9 uma matriz <code>X2</code> com shape (1000, 2), representando cada amostra em duas dimens\u00f5es (PC1 e PC2).</p> </li> <li> <p><code>df[\"pc1\"] = X2[:, 0]</code> e <code>df[\"pc2\"] = X2[:, 1]</code>   Adicionei as duas novas colunas ao DataFrame para facilitar visualiza\u00e7\u00e3o e plotagem.</p> </li> <li> <p><code>pca.explained_variance_ratio_</code>   Essa fun\u00e7\u00e3o retorna quanto da vari\u00e2ncia total dos dados originais foi preservada por cada componente.  </p> </li> <li>O <code>print</code> mostra a vari\u00e2ncia explicada por PC1 e PC2 separadamente.  </li> <li>Tamb\u00e9m calculei a soma, para verificar quanta informa\u00e7\u00e3o 5D conseguimos reter em 2D.</li> </ul> <pre><code>pca = PCA(n_components=2, random_state=42)\nX2 = pca.fit_transform(X)\ndf[\"pc1\"] = X2[:, 0]\ndf[\"pc2\"] = X2[:, 1]\n\nprint(\"Vari\u00e2ncia explicada (PC1, PC2):\", np.round(pca.explained_variance_ratio_, 3))\nprint(\"Soma da vari\u00e2ncia explicada:\", np.round(pca.explained_variance_ratio_.sum(), 3))\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-4-visualizacao-grafica-das-classes-no-espaco-2d","title":"Etapa 4 \u2014 Visualiza\u00e7\u00e3o gr\u00e1fica das classes no espa\u00e7o 2D","text":"<p>Nesta etapa eu fiz a visualiza\u00e7\u00e3o final dos dados ap\u00f3s a redu\u00e7\u00e3o de dimensionalidade com PCA. Usei o Matplotlib para criar um gr\u00e1fico de dispers\u00e3o que mostra as duas classes projetadas no plano formado pelos dois componentes principais (PC1 e PC2).</p> <p>Esse gr\u00e1fico permite comparar visualmente como as duas classes se distribuem em 2D e perceber se existe ou n\u00e3o separa\u00e7\u00e3o linear entre elas.</p> <pre><code>plt.figure(figsize=(7, 5))\nplt.scatter(df.loc[df[\"class\"] == 0, \"pc1\"], df.loc[df[\"class\"] == 0, \"pc2\"],\n            s=14, alpha=0.8, label=\"Classe A\")\nplt.scatter(df.loc[df[\"class\"] == 1, \"pc1\"], df.loc[df[\"class\"] == 1, \"pc2\"],\n            s=14, alpha=0.8, label=\"Classe B\")\nplt.title(\"PCA (5D \u2192 2D): Classe A vs Classe B\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p> <p>Separabilidade linear (an\u00e1lise final):</p> <p>Na proje\u00e7\u00e3o em 2D, \u00e9 poss\u00edvel perceber que as duas classes n\u00e3o ficam totalmente separadas: h\u00e1 regi\u00f5es onde os pontos de A e B se misturam. Mesmo considerando os dados no espa\u00e7o original de 5 dimens\u00f5es, a forma como as duas classes est\u00e3o distribu\u00eddas faz com que uma linha reta n\u00e3o seja suficiente para dividi-las corretamente.</p> <p>Isso significa que modelos simples que trabalham apenas com separa\u00e7\u00f5es retas, como o Perceptron, n\u00e3o conseguem representar bem esse tipo de situa\u00e7\u00e3o. Por outro lado, modelos mais avan\u00e7ados, como redes neurais, conseguem criar fronteiras curvas e mais flex\u00edveis, o que permite distinguir melhor as duas classes mesmo quando elas n\u00e3o est\u00e3o separadas de forma simples.</p>"},{"location":"data/exercicio3/exercicio3/","title":"Exerc\u00edcio 3 \u2014 DATA","text":""},{"location":"data/exercicio3/exercicio3/#objetivo","title":"Objetivo","text":"<p>O objetivo \u00e9 preparar o conjunto de dados Spaceship Titanic (train.csv do Kaggle) para uso em uma rede neural que utiliza a fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh.  </p> <p>A seguir, realizei o passo a passo de carregamento, explora\u00e7\u00e3o, tratamento de nulos, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas e padroniza\u00e7\u00e3o de num\u00e9ricas, al\u00e9m de visualizar o impacto do pr\u00e9-processamento.</p>"},{"location":"data/exercicio3/exercicio3/#etapa-1-carregamento-e-descricao-do-dataset","title":"Etapa 1 \u2014 Carregamento e descri\u00e7\u00e3o do dataset","text":"<p>O arquivo <code>train.csv</code> cont\u00e9m cerca de 8700 passageiros. Cada linha representa um passageiro, com informa\u00e7\u00f5es pessoais, dados de viagem e gastos. A coluna <code>Transported</code> \u00e9 o alvo (True/False), indicando se o passageiro foi transportado para outra dimens\u00e3o.</p> <p>Principais tipos de vari\u00e1veis:</p> <ul> <li>Identifica\u00e7\u00e3o: <code>PassengerId</code>, <code>Name</code> .</li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>.</li> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.</li> <li>Alvo: <code>Transported</code>.</li> </ul> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data/train.csv\")\ndisplay(df.head())\nprint(df.info())\nprint(df.isnull().sum())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#valores-ausentes","title":"Valores ausentes","text":"<p>Ap\u00f3s carregar o dataset <code>train.csv</code>, percebi que ele possui 8693 linhas e 14 colunas. Algumas vari\u00e1veis apresentam valores ausentes, enquanto outras est\u00e3o completas:</p> <ul> <li>Sem valores ausentes:</li> <li><code>PassengerId</code> \u2192 identificador \u00fanico de cada passageiro.  </li> <li> <p><code>Transported</code> \u2192 vari\u00e1vel alvo (true ou false para saber se o passageiro foi transportado).</p> </li> <li> <p>Com valores ausentes:</p> </li> <li><code>HomePlanet</code> (201) \u2192 planeta de origem.  </li> <li><code>CryoSleep</code> (217) \u2192 se o passageiro estava em criossuspens\u00e3o.  </li> <li><code>Cabin</code> (199) \u2192 cabine onde estava hospedado.  </li> <li><code>Destination</code> (182) \u2192 planeta de destino.  </li> <li><code>Age</code> (179) \u2192 idade do passageiro.  </li> <li><code>VIP</code> (203) \u2192 se contratou servi\u00e7o VIP.  </li> <li><code>RoomService</code> (181), <code>FoodCourt</code> (183), <code>ShoppingMall</code> (208), <code>Spa</code> (183), <code>VRDeck</code> (188) \u2192 gastos.  </li> <li><code>Name</code> (200) \u2192 nome do passageiro.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>O conjunto apresenta valores faltantes em quase todas as colunas de entrada.  </li> <li>O percentual de valores ausentes em cada coluna \u00e9 relativamente pequeno (entre 2% e 3% do total de linhas), ou seja, \u00e9 vi\u00e1vel aplicar t\u00e9cnicas de preenchimento  em vez de descartar linhas inteiras.  </li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-2-definicao-dos-tipos-de-variaveis","title":"Etapa 2 \u2014 Defini\u00e7\u00e3o dos tipos de vari\u00e1veis","text":"<ul> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.  </li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>.  </li> <li>Alvo: <code>Transported</code>.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-3-tratamento-de-valores-ausentes","title":"Etapa 3 \u2014 Tratamento de valores ausentes","text":"<p>Pelo <code>df.info()</code> e <code>df.isnull().sum()</code>:</p> <ul> <li>H\u00e1 nulos em quase todas as colunas de entrada (entre ~179 e ~217 linhas por coluna, \u22482%\u20133% do total de 8693).</li> </ul> <p>Estrat\u00e9gia adotada:</p> <ol> <li> <p>Categ\u00f3ricas (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>):</p> </li> <li> <p>Preencher com a moda (valor mais frequente).</p> </li> <li> <p>Para <code>Cabin</code>, al\u00e9m de preencher, separar em <code>Deck</code>/<code>Num</code>/<code>Side</code> (formato <code>deck/num/side</code>), pois \u00e9 uma string composta que carrega informa\u00e7\u00e3o \u00fatil.</p> </li> <li> <p>Num\u00e9ricas (<code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>):</p> </li> <li> <p>Preencher com a mediana (robusta a outliers).</p> </li> <li> <p>Regra de neg\u00f3cio adicional: se <code>CryoSleep == True</code>, os gastos deveriam ser zero (passageiro confinado \u00e0 cabine). Assim, nulos nos gastos para quem est\u00e1 em <code>CryoSleep</code> ser\u00e3o imputados com 0; os demais, com mediana.</p> </li> </ol>"},{"location":"data/exercicio3/exercicio3/#etapa-31-preparos-para-imputacao","title":"Etapa 3.1 \u2014 Preparos para imputa\u00e7\u00e3o","text":"<p>Antes de imputar:</p> <ul> <li> <p>Defino listas auxiliares de colunas.</p> </li> <li> <p>Converto <code>CryoSleep</code> e <code>VIP</code> para booleanos/bits depois de preencher (eles vieram como <code>object</code> por causa dos NaNs).</p> </li> <li> <p>Para <code>Cabin</code>, separo em tr\u00eas colunas (<code>Deck</code>, <code>Num</code>, <code>Side</code>), imputo <code>Deck</code>/<code>Side</code> com a moda e <code>Num</code> com a mediana (num\u00e9rica).</p> </li> </ul> <pre><code>num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Cabin\", \"Destination\", \"VIP\"]  # Cabin ser\u00e1 expandida\ntarget = \"Transported\"\n\ncabin_split = df[\"Cabin\"].str.split(\"/\", expand=True)\ndf[\"Deck\"] = cabin_split[0]\ndf[\"Num\"]  = pd.to_numeric(cabin_split[1], errors=\"coerce\")  # num\u00e9rico\ndf[\"Side\"] = cabin_split[2]\n\ncat_cols_expanded = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\nnum_cols_expanded = num_cols + [\"Num\"]\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-32-imputacao-das-categoricas","title":"Etapa 3.2 \u2014 Imputa\u00e7\u00e3o das categ\u00f3ricas","text":"<p>Regra: preencher categ\u00f3ricas com a moda (valor mais frequente). Depois, padronizo <code>CryoSleep</code> e <code>VIP</code> para inteiros 0/1 (necess\u00e1rio para modelos e para aplicar a regra dos gastos = 0 se em criossono).</p> <pre><code>for col in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\", \"CryoSleep\", \"VIP\"]:\n    df[col] = df[col].fillna(df[col].mode()[0])\n\ndef to_bool01(series):\n    return (\n        series\n        .replace({True: 1, False: 0, \"True\": 1, \"False\": 0, \"TRUE\": 1, \"FALSE\": 0})\n        .astype(int)\n    )\n\ndf[\"CryoSleep\"] = to_bool01(df[\"CryoSleep\"])\ndf[\"VIP\"]       = to_bool01(df[\"VIP\"])\n\ndf[\"Transported\"] = df[\"Transported\"].astype(int)\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-33-imputacao-das-numericas","title":"Etapa 3.3 \u2014 Imputa\u00e7\u00e3o das num\u00e9ricas","text":"<p>Regra geral: imputar mediana. Regra de neg\u00f3cio adicional para gastos: se <code>CryoSleep == 1</code>, nulos em gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>) viram 0; os demais nulos seguem para mediana.</p> <p>Para <code>Num</code> (n\u00famero da cabine, derivado de <code>Cabin</code>), uso mediana.</p> <pre><code>spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\nfor col in spend_cols:\n    mask = (df[\"CryoSleep\"] == 1) &amp; (df[col].isna())\n    df.loc[mask, col] = 0.0\n\nfor col in num_cols_expanded:\n    df[col] = df[col].fillna(df[col].median())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-34-limpeza-final-desta-etapa","title":"Etapa 3.4 \u2014 Limpeza final desta etapa","text":"<ul> <li><code>Cabin</code> original \u00e9 removida (substitu\u00edda por <code>Deck</code>, <code>Num</code>, <code>Side</code>).</li> <li><code>Name</code> n\u00e3o ser\u00e1 usado como atributo neste exerc\u00edcio (texto livre), ent\u00e3o removo.</li> <li>Fa\u00e7o uma checagem final para garantir zero valores ausentes nas colunas que seguem para o modelo.</li> </ul> <pre><code>df = df.drop(columns=[\"Cabin\", \"Name\"])\n\nmissing_total = df.isnull().sum().sum()\nprint(f\"valores ausentes: {missing_total}\")\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-35-comentarios-e-justificativas","title":"Etapa 3.5 \u2014 Coment\u00e1rios e justificativas","text":"<ul> <li>Moda nas categ\u00f3ricas: mant\u00e9m a coer\u00eancia dos dados e evita criar categorias artificiais com baixa frequ\u00eancia.</li> <li>Mediana nas num\u00e9ricas: \u00e9 robusta a valores extremos (outliers), comuns nas colunas de gastos.</li> <li>Regra de gastos = 0 em CryoSleep: condiz com a defini\u00e7\u00e3o do problema (passageiros em criossono ficam na cabine e n\u00e3o consomem servi\u00e7os).</li> <li>Separar <code>Cabin</code> em <code>Deck</code>/<code>Num</code>/<code>Side</code>: aproveita a estrutura da informa\u00e7\u00e3o, em vez de trat\u00e1-la como uma string opaca. <code>Deck</code> e <code>Side</code> entram como categ\u00f3ricas; <code>Num</code>, como num\u00e9rica.</li> <li>Remover <code>Name</code>: campo textual livre, de pouca utilidade neste exerc\u00edcio de pr\u00e9-processamento para uma MLP com <code>tanh</code> (sem embeddings/PLN). Pode ser reintroduzido em projetos onde se extraem sobrenomes/grupos.</li> <li>Ap\u00f3s estas decis\u00f5es, o dataset est\u00e1 livre de nulos e pronto para codifica\u00e7\u00e3o categ\u00f3rica e escalonamento (pr\u00f3ximas etapas).</li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-4-codificacao-de-variaveis-categoricas","title":"Etapa 4 \u2014 Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas","text":"<p>Para treinar uma rede neural, todas as entradas precisam estar em formato num\u00e9rico.</p> <ul> <li>Vari\u00e1veis bin\u00e1rias:  </li> <li><code>CryoSleep</code> e <code>VIP</code> j\u00e1 foram convertidas para 0/1.  </li> <li>Vari\u00e1veis multiclasse:  </li> <li><code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code> \u2192 apliquei One-Hot Encoding, criando colunas dummy (0/1).  </li> </ul> <p>Assim, todas as categorias foram transformadas em indicadores num\u00e9ricos sem introduzir ordens artificiais.</p> <pre><code>df = pd.get_dummies(df, columns=[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"], drop_first=True)\n\ndisplay(df.head())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-5-escalonamento-dos-atributos-numericos","title":"Etapa 5 \u2014 Escalonamento dos atributos num\u00e9ricos","text":"<p>A fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh \u00e9 centrada em zero e gera sa\u00eddas em [-1, 1]. Portanto, \u00e9 essencial que as vari\u00e1veis num\u00e9ricas estejam padronizadas para:</p> <ul> <li>m\u00e9dia = 0  </li> <li>desvio padr\u00e3o = 1  </li> </ul> <p>Isso melhora a estabilidade do treinamento e acelera a converg\u00eancia. Para isso, usei <code>StandardScaler</code> da biblioteca <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nnum_cols_final = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Num\"]\n\nscaler = StandardScaler()\ndf[num_cols_final] = scaler.fit_transform(df[num_cols_final])\n\ndisplay(df[num_cols_final].head())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-6-visualizacao-do-impacto-do-pre-processamento","title":"Etapa 6 \u2014 Visualiza\u00e7\u00e3o do impacto do pr\u00e9-processamento","text":"<p>Para evidenciar a transforma\u00e7\u00e3o feita, comparei a distribui\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas antes e depois do escalonamento:</p> <ul> <li>Idade (<code>Age</code>): mostra como foi centralizada em torno de zero e ajustada em escala.  </li> <li>Gasto em <code>FoodCourt</code>: mostra como valores muito altos foram reduzidos para uma escala padr\u00e3o, sem alterar a forma da distribui\u00e7\u00e3o.</li> </ul> <p>Isso comprova que os dados foram preparados corretamente para uso em uma rede neural com <code>tanh</code>.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Dados originais para compara\u00e7\u00e3o\ndf_raw = pd.read_csv(\"data/train.csv\")\n\nfig, axes = plt.subplots(2, 2, figsize=(10,6))\n\n# Antes\naxes[0,0].hist(df_raw[\"Age\"].dropna(), bins=30, color=\"skyblue\")\naxes[0,0].set_title(\"Age (Antes)\")\n\naxes[0,1].hist(df_raw[\"FoodCourt\"].dropna(), bins=30, color=\"salmon\")\naxes[0,1].set_title(\"FoodCourt (Antes)\")\n\n# Depois\naxes[1,0].hist(df[\"Age\"], bins=30, color=\"skyblue\")\naxes[1,0].set_title(\"Age (Depois)\")\n\naxes[1,1].hist(df[\"FoodCourt\"], bins=30, color=\"salmon\")\naxes[1,1].set_title(\"FoodCourt (Depois)\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p> </p> <p>Compara\u00e7\u00e3o pr\u00e9-p\u00f3s processamento</p>"},{"location":"data/exercicio3/exercicio3/#importancia-do-pre-processamento","title":"Import\u00e2ncia do Pr\u00e9-processamento","text":""},{"location":"data/exercicio3/exercicio3/#variavel-age","title":"Vari\u00e1vel Age","text":"<ul> <li>Antes: a idade estava distribu\u00edda entre 0 e 80 anos, com concentra\u00e7\u00e3o principal entre 20 e 40.  </li> <li>Depois: ap\u00f3s a padroniza\u00e7\u00e3o, os valores passaram a variar aproximadamente entre -2 e +3, centralizados em torno de zero.  </li> <li>Impacto: a transforma\u00e7\u00e3o mant\u00e9m a forma da distribui\u00e7\u00e3o, mas coloca a vari\u00e1vel em uma escala adequada para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>, que trabalha melhor com valores pr\u00f3ximos de zero.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#variavel-foodcourt","title":"Vari\u00e1vel FoodCourt","text":"<ul> <li>Antes: os gastos apresentavam valores de 0 at\u00e9 quase 30.000, mas altamente concentrados em torno de zero.  </li> <li>Depois: ap\u00f3s normaliza\u00e7\u00e3o/padroniza\u00e7\u00e3o, os valores foram reduzidos para uma faixa pr\u00f3xima de 0 a 18.  </li> <li>Impacto: isso evita que essa vari\u00e1vel, por estar em uma escala muito maior que as demais, dominasse o processo de treinamento da rede neural.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#comparacao-geral","title":"Compara\u00e7\u00e3o Geral","text":"<ul> <li>O pr\u00e9-processamento preservou o padr\u00e3o das distribui\u00e7\u00f5es, mas ajustou suas escalas para torn\u00e1-las compar\u00e1veis.  </li> <li>Agora, cada vari\u00e1vel contribui de maneira mais equilibrada para o aprendizado, sem que uma tenha peso desproporcional apenas por conta da unidade de medida.  </li> <li>Al\u00e9m disso, como os valores foram centralizados (no caso de Age) e reduzidos (no caso de FoodCourt), a rede neural ter\u00e1 treinamento mais est\u00e1vel e eficiente, evitando satura\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#conclusao","title":"Conclus\u00e3o","text":"<p>O pr\u00e9-processamento foi essencial para:</p> <ul> <li>Reduzir diferen\u00e7as de escala entre atributos,  </li> <li>Adaptar os dados para o intervalo adequado da fun\u00e7\u00e3o de ativa\u00e7\u00e3o,  </li> <li>Garantir que todos os atributos tenham relev\u00e2ncia equilibrada na classifica\u00e7\u00e3o.</li> </ul>"},{"location":"metricas/main/","title":"Metricas","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> <p></p> <p></p> <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"mlp/main/","title":"MLP","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> </p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"perceptron/main/","title":"Main","text":""},{"location":"perceptron/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"perceptron/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"perceptron/exercicio1/exercicio1/","title":"Exercicio 1 - Perceptron","text":""},{"location":"perceptron/exercicio1/exercicio1/#imports-e-configuracao-inicial","title":"Imports e Configura\u00e7\u00e3o Inicial","text":"<p>Antes de come\u00e7ar, vamos importar as bibliotecas fundamentais que usaremos ao longo de todo o exerc\u00edcio:</p> <ul> <li>NumPy: para opera\u00e7\u00f5es matem\u00e1ticas, \u00e1lgebra linear e gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios</li> <li>Matplotlib: para criar todas as visualiza\u00e7\u00f5es e gr\u00e1ficos</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-1-definicao-dos-parametros","title":"Etapa 1 \u2014 Defini\u00e7\u00e3o dos par\u00e2metros","text":"<p>Aqui eu preparei o gerador de n\u00fameros aleat\u00f3rios e defini os par\u00e2metros das normais multivariadas 2D para as duas classes (\u22121 e +1).</p> <ul> <li> <p><code>rng = np.random.default_rng(42)</code>   Criei um gerador pseudoaleat\u00f3rio com semente 42 para garantir os mesmos dados a cada execu\u00e7\u00e3o.</p> </li> <li> <p><code>mu0</code> e <code>mu1</code>   S\u00e3o os vetores de m\u00e9dias (2 dimens\u00f5es) de cada classe.  </p> </li> <li>Classe \u22121: <code>mu0 = [1.5, 1.5]</code>.  </li> <li> <p>Classe +1: <code>mu1 = [5.0, 5.0]</code>.</p> </li> <li> <p><code>cov0</code> e <code>cov1</code>   S\u00e3o as matrizes de covari\u00e2ncia (2\u00d72) que controlam o espalhamento das nuvens.  </p> </li> <li>Ambas diagonais: <code>[[0.5, 0.0], [0.0, 0.5]]</code> (vari\u00e2ncia 0.5 por eixo).  </li> </ul> <p>Esses par\u00e2metros ser\u00e3o usados na fun\u00e7\u00e3o <code>rng.multivariate_normal</code> para gerar os pontos <code>X0</code> e <code>X1</code> de cada classe na etapa seguinte.</p> <ul> <li> <p><code>y0 = -np.ones(n, dtype=int)</code> e <code>y1 = np.ones(n, dtype=int)</code>   Criei os r\u00f3tulos das classes usando a conven\u00e7\u00e3o \u22121/+1, que facilita a regra de atualiza\u00e7\u00e3o do perceptron.</p> </li> <li> <p>Empilhamento e embaralhamento <code>X = np.vstack([X0, X1])</code> e <code>y = np.hstack([y0, y1])</code> juntam os pontos e r\u00f3tulos das duas classes. <code>idx = rng.permutation(len(X))</code> \u2192 <code>X = X[idx]</code>, <code>y = y[idx]</code> embaralham os pares <code>(X, y)</code> para evitar blocos por classe.</p> </li> </ul> <p>Resultado: dataset <code>X</code> e r\u00f3tulos <code>y</code> prontos, balanceados e embaralhados, para o treino do perceptron.</p> <pre><code>rng = np.random.default_rng(42)\n\n# Par\u00e2metros\nn = 1000\n\nmu0 = np.array([1.5, 1.5], dtype=float)\nmu1 = np.array([5.0, 5.0], dtype=float)\n\ncov0 = np.array([[0.5, 0.0],[0.0, 0.5]], dtype=float)\ncov1 = np.array([[0.5, 0.0],[0.0, 0.5]], dtype=float)\n\n# Gera\u00e7\u00e3o das amostras\nX0 = rng.multivariate_normal(mean=mu0, cov=cov0, size=n)\nX1 = rng.multivariate_normal(mean=mu1, cov=cov1, size=n)\n\ny0 = -np.ones(n, dtype=int)\ny1 =  np.ones(n, dtype=int)\n\n# Empilha e embaralha\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])\n\nidx = rng.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-2-visualizacao-inicial-dos-dados","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o inicial dos dados","text":"<p>Aqui eu plotei a distribui\u00e7\u00e3o dos pontos por classe para inspecionar rapidamente o dataset antes do treino.</p> <ul> <li><code>mask_pos</code> e <code>mask_neg</code> separam os \u00edndices das amostras da classe +1 e classe \u22121.</li> <li>Desenhei dois gr\u00e1ficos de dispers\u00e3o (um por classe), usando marcadores diferentes.</li> </ul> <p>Objetivo: verificar forma, espalhamento e poss\u00edvel sobreposi\u00e7\u00e3o entre as classes, validando a separabilidade linear esperada para o perceptron.</p> <pre><code># Scatter das duas classes\nmask_pos = y == 1\nmask_neg = y == -1\n\nplt.figure()\nplt.scatter(X[mask_neg, 0], X[mask_neg, 1], label=\"Classe -1\", marker=\"o\", alpha=0.7)\nplt.scatter(X[mask_pos, 0], X[mask_pos, 1], label=\"Classe +1\", marker=\"s\", alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Distribui\u00e7\u00e3o dos dados por classe\")\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-3-idealizacao-do-perceptron","title":"Etapa 3 - Idealiza\u00e7\u00e3o do Perceptron","text":"<p>Resumo: O Perceptron \u00e9 uma rede neural de uma \u00fanica camada (um \u00fanico neur\u00f4nio). Ele recebe um vetor de entradas, calcula uma soma ponderada usando pesos e um vi\u00e9s (bias), e aplica uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o em degrau para produzir uma sa\u00edda bin\u00e1ria.</p> <p></p>"},{"location":"perceptron/exercicio1/exercicio1/#31-componentes-da-figura","title":"3.1 Componentes da figura","text":"<ul> <li>Entradas <code>x\u2081, x\u2082, \u2026, x_m</code> (Input): caracter\u00edsticas do exemplo a classificar.  </li> <li>Pesos <code>w\u2081, w\u2082, \u2026, w_m</code> (Weight): import\u00e2ncia de cada entrada.  </li> <li>Somat\u00f3rio <code>\u03a3</code> (Network input function): calcula a soma ponderada.  </li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o (Activation function): converte o n\u00famero real em decis\u00e3o bin\u00e1ria.  </li> <li>Sa\u00edda (Output): r\u00f3tulo previsto.</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#32-equacoes","title":"3.2 Equa\u00e7\u00f5es","text":"<ol> <li> <p>Soma ponderada (potencial de ativa\u00e7\u00e3o)    $$    a \\;=\\; \\sum_{i=1}^{m} w_i\\,x_i \\;+\\; b \\;=\\; w \\cdot x \\;+\\; b    $$</p> </li> <li> <p>Ativa\u00e7\u00e3o (degrau) </p> </li> <li>Forma 0/1:      $$      \\text{activation}(a) =      \\begin{cases}        1, &amp; a \\ge 0 \\        0, &amp; a &lt; 0      \\end{cases}      $$</li> <li> <p>Forma \u22121/+1 (a que usaremos):      $$      \\hat{y} =      \\begin{cases}        +1, &amp; a \\ge 0 \\        -1, &amp; a &lt; 0      \\end{cases}      $$</p> </li> <li> <p>Regra de decis\u00e3o (equivalente)    $$    \\hat{y} = \\mathrm{sign}(w \\cdot x + b)    $$</p> </li> </ol>"},{"location":"perceptron/exercicio1/exercicio1/#33-interpretacao-geometrica-por-que-e-linear","title":"3.3 Interpreta\u00e7\u00e3o geom\u00e9trica (por que \u00e9 linear)","text":"<ul> <li>A fronteira de decis\u00e3o \u00e9 o conjunto de pontos que satisfaz   $$   w \\cdot x + b = 0.   $$</li> <li>Em 2D, essa fronteira \u00e9 uma reta(nosso caso); em dimens\u00f5es maiores, um hiperplano.  </li> <li>O vetor w \u00e9 perpendicular \u00e0 fronteira; o bias \\(b\\) desloca a reta/hiperplano.</li> </ul> <p>Pr\u00f3xima etapa: veremos como o perceptron aprende \u2014 calculando o erro entre \\(\\hat{y}\\) e o r\u00f3tulo verdadeiro \\(y\\) e ajustando \\(w\\) e \\(b\\) com a regra de atualiza\u00e7\u00e3o.</p>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-4-implementacao-do-perceptron","title":"Etapa 4 \u2014 Implementa\u00e7\u00e3o do Perceptron","text":""},{"location":"perceptron/exercicio1/exercicio1/#41-funcao-degrau-step","title":"4.1 Fun\u00e7\u00e3o degrau (step)","text":"<p>Ideia: Dado um escalar ou vetor <code>z</code>, retornamos +1 quando <code>z \u2265 0</code> e \u22121 quando <code>z &lt; 0</code>. Isso corresponde \u00e0 fun\u00e7\u00e3o de ativa\u00e7\u00e3o do perceptron.</p> <p>F\u00f3rmula:</p> \\[ \\text{sign}(z) = \\begin{cases} +1, &amp; z \\geq 0 \\\\ -1, &amp; z &lt; 0 \\end{cases} \\] <pre><code>def step_sign(z):\n    \"\"\"  \n    Par\u00e2metros:\n    - z: potencial de ativa\u00e7\u00e3o\n\n    Retorna:\n    - +1 para valores \u2265 0, -1 para valores &lt; 0\n    \"\"\"\n    return np.where(z &gt;= 0, 1, -1)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#42-predicao-forward-pass","title":"4.2 Predi\u00e7\u00e3o (forward pass)","text":"<p>Ideia: Para cada amostra <code>x</code>, calculamos o potencial de ativa\u00e7\u00e3o <code>a = w \u00b7 x + b</code> e aplicamos a fun\u00e7\u00e3o degrau. Se <code>a \u2265 0</code> \u2192 classe +1; caso contr\u00e1rio \u2192 \u22121.</p> <p>F\u00f3rmula</p> \\[ \\hat{y} = \\text{sign}(w \\cdot x + b) \\] <p>Par\u00e2metros:</p> <ul> <li> <p>X: <code>matriz de features (n_amostras, n_dimens\u00f5es)</code></p> </li> <li> <p>w: <code>vetor de pesos (n_dimens\u00f5es,)</code></p> </li> <li> <p>b: <code>bias (escalar)</code></p> </li> </ul> <p>Retorna:</p> <ul> <li>vetor de predi\u00e7\u00f5es {-1, +1} para cada amostra</li> </ul> <pre><code>def predict(X, w, b):\n    return step_sign(X @ w + b)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#43-treinamento-aprendizado-com-o-erro","title":"4.3 Treinamento (aprendizado com o erro)","text":"<p>Ideia central: percorrer os dados v\u00e1rias vezes (\u00e9pocas); para cada amostra <code>x_i</code> com r\u00f3tulo <code>y_i \u2208 {\u22121, +1}</code>, se errou (isto \u00e9, <code>y_i (w \u00b7 x_i + b) \u2264 0</code>), ajuste os par\u00e2metros:</p> <p>Regra de atualiza\u00e7\u00e3o</p> \\[ w \\leftarrow w + \\eta y_i x_i, \\quad\\quad b \\leftarrow b + \\eta y_i \\] <ul> <li>eta \u00e9 a taxa de aprendizado.  </li> <li>Embaralhamos as amostras a cada \u00e9poca (opcional) para evitar ciclos.  </li> <li>Guardamos hist\u00f3rico de acur\u00e1cia e n\u00ba de atualiza\u00e7\u00f5es para avaliar a converg\u00eancia.</li> </ul> Fun\u00e7\u00e3o - train_perceptron <pre><code>def train_perceptron(X, y, eta=0.01, max_epochs=100, shuffle=True):\n    \"\"\"    \n    Para cada amostra (x\u1d62, y\u1d62) classificada incorretamente:\n    w \u2190 w + \u03b7\u00b7y\u1d62\u00b7x\u1d62    (atualiza\u00e7\u00e3o dos pesos)\n    b \u2190 b + \u03b7\u00b7y\u1d62      (atualiza\u00e7\u00e3o do bias)\n\n    Uma amostra \u00e9 considerada incorreta se: y\u1d62\u00b7(w\u00b7x\u1d62 + b) \u2264 0\n\n    Par\u00e2metros:\n    - X: matriz de features (n_amostras, n_dimens\u00f5es)\n    - y: vetor de r\u00f3tulos {-1, +1}\n    - eta: taxa de aprendizado\n    - max_epochs: n\u00famero m\u00e1ximo de \u00e9pocas\n    - shuffle: embaralhar amostras a cada \u00e9poca\n\n    Retorna:\n    - w: pesos finais\n    - b: bias final\n    - history: hist\u00f3rico de treinamento (acur\u00e1cia, atualiza\u00e7\u00f5es, par\u00e2metros)\n    - y_hat: predi\u00e7\u00f5es finais\n    \"\"\"\n    n, d = X.shape\n\n    np.random.seed(42)\n    w = np.random.uniform(-2, 2, d)      # pesos aleat\u00f3rios \n    b = np.random.uniform(-5, 5)         # bias aleat\u00f3rio\n\n    history = []\n\n    # LOOP PRINCIPAL: percorrer \u00e9pocas\n    for epoch in range(1, max_epochs + 1):\n        updates = 0\n\n        # Embaralhar dados para evitar ciclos e melhorar converg\u00eancia\n        if shuffle:\n            idx = np.random.permutation(n)\n            X_epoch, y_epoch = X[idx], y[idx]\n        else:\n            X_epoch, y_epoch = X, y\n\n        # PERCORRER CADA AMOSTRA NA \u00c9POCA\n        for xi, yi in zip(X_epoch, y_epoch):\n            # Calcular potencial de ativa\u00e7\u00e3o: a = w\u00b7x + b\n            z = np.dot(w, xi) + b\n\n            # CRIT\u00c9RIO DE ERRO: yi * z &lt;= 0\n            # Se yi=+1 e z&lt;0: predi\u00e7\u00e3o errada (-1), precisa corrigir\n            # Se yi=-1 e z&gt;0: predi\u00e7\u00e3o errada (+1), precisa corrigir\n            if yi * z &lt;= 0:  \n                # REGRA DE ATUALIZA\u00c7\u00c3O\n                w += eta * yi * xi  # w \u2190 w + \u03b7\u00b7yi\u00b7xi\n                b += eta * yi       # b \u2190 b + \u03b7\u00b7yi\n                updates += 1\n\n        # AVALIAR PERFORMANCE DA \u00c9POCA\n        y_hat = predict(X, w, b)\n        acc = (y_hat == y).mean()  # acur\u00e1cia = % acertos\n\n        history.append({\n            \"epoch\": epoch,\n            \"accuracy\": acc,\n            \"updates\": updates,\n            \"w\": w.copy(),\n            \"b\": b\n        })\n\n        # CRIT\u00c9RIO DE PARADA: converg\u00eancia\n        if updates == 0:\n            print(f\"Converg\u00eancia alcan\u00e7ada na \u00e9poca {epoch}!\")\n            break\n\n    return w, b, history, y_hat\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#44-checagens","title":"4.4 Checagens","text":"<p>Acur\u00e1cia simples:</p> <pre><code>def accuracy(y_true, y_pred):\n    return np.mean(y_true == y_pred)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-5-treinamento-do-perceptron-com-os-dados-do-exercicio","title":"Etapa 5 \u2014 Treinamento do Perceptron com os Dados do Exerc\u00edcio","text":"<p>Objetivo: aplicar o perceptron implementado nos dados gerados na Etapa 1:</p> <ul> <li>Taxa de aprendizado \u03b7 = 0.01</li> <li>M\u00e1ximo de 100 \u00e9pocas</li> <li>Parada antecipada por converg\u00eancia (nenhuma atualiza\u00e7\u00e3o em uma \u00e9poca completa)</li> <li>Rastreamento da acur\u00e1cia ap\u00f3s cada \u00e9poca</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#51-treinamento-e-resultados","title":"5.1 Treinamento e resultados","text":"<pre><code>w_final, b_final, history, y_pred_final = train_perceptron(\n    X, y, \n    eta=0.001,       # taxa pequena para ver evolu\u00e7\u00e3o gradual\n    max_epochs=100,  # m\u00e1ximo de \u00e9pocas\n    shuffle=True     # embaralhar para evitar ciclos\n)\n\n\n# Metricas finais\nfinal_accuracy = accuracy(y, y_pred_final)  # % de acertos\nfinal_epoch = history[-1][\"epoch\"]          # \u00e9poca de converg\u00eancia\ntotal_updates = sum([h[\"updates\"] for h in history])  # total de atualiza\u00e7\u00f5es\n\nprint(\"=== RESULTADOS FINAIS ===\")\nprint(f\"Pesos finais: w = [{w_final[0]:.4f}, {w_final[1]:.4f}]\")\nprint(f\"Vi\u00e9s final: b = {b_final:.4f}\")\nprint(f\"Acur\u00e1cia final: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"Total de atualiza\u00e7\u00f5es durante todo o treinamento: {total_updates}\")\n\n# Erros\nmisclassified_mask = (y != y_pred_final)  # m\u00e1scara dos erros\nn_misclassified = np.sum(misclassified_mask)  # contar erros\nprint(f\"Pontos mal classificados: {n_misclassified} de {len(y)} ({n_misclassified/len(y)*100:.2f}%)\")\n</code></pre> <pre><code>Converg\u00eancia alcan\u00e7ada na \u00e9poca 16!\n\n=== RESULTADOS FINAIS ===\nPesos finais: w = [0.0205, 0.0451]\nVi\u00e9s final: b = -0.2251\nAcur\u00e1cia final: 1.0000 (100.00%)\nTotal de atualiza\u00e7\u00f5es durante todo o treinamento: 4341\nPontos mal classificados: 0 de 2000 (0.00%)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-6-visualizacao-dos-resultados","title":"Etapa 6 \u2014 Visualiza\u00e7\u00e3o dos Resultados","text":"<p>Objetivo: criar visualiza\u00e7\u00f5es claras e informativas que mostrem:</p> <ol> <li>Evolu\u00e7\u00e3o da acur\u00e1cia durante o treinamento</li> <li>Fronteira de decis\u00e3o aprendida pelo perceptron</li> <li>Pontos mal classificados (se houver)</li> </ol>"},{"location":"perceptron/exercicio1/exercicio1/#61-curva-de-acuracia-por-epoca","title":"6.1 Curva de acur\u00e1cia por \u00e9poca","text":"<p>A primeira visualiza\u00e7\u00e3o importante \u00e9 observar como a acur\u00e1cia evolui durante o treinamento. Isso nos mostra:</p> <ol> <li>Velocidade de converg\u00eancia: quantas \u00e9pocas o perceptron precisou para aprender</li> <li>Estabilidade: se a acur\u00e1cia melhora consistentemente ou oscila</li> <li>Padr\u00e3o de atualiza\u00e7\u00f5es: como o n\u00famero de corre\u00e7\u00f5es diminui \u00e0 medida que o modelo aprende</li> </ol> <p></p> <p>Os gr\u00e1ficos indicam converg\u00eancia r\u00e1pida e est\u00e1vel do perceptron:</p> <ul> <li>Velocidade de converg\u00eancia: a acur\u00e1cia sai de ~0,4 nas primeiras \u00e9pocas, d\u00e1 um salto por volta da \u00e9poca 4 e atinge 1,0 entre as \u00e9pocas 5\u20136, mantendo-se em plat\u00f4 at\u00e9 o fim.</li> <li>Estabilidade: ap\u00f3s atingir 100%, a acur\u00e1cia permanece constante, sugerindo que n\u00e3o h\u00e1 oscila\u00e7\u00f5es causadas por taxa de aprendizado alta ou por dados ruidosos.</li> <li>Padr\u00e3o de atualiza\u00e7\u00f5es: o n\u00famero de corre\u00e7\u00f5es come\u00e7a muito alto (~1,1k), cai abruptamente ap\u00f3s a \u00e9poca 4 e tende a zero a partir da 6\u00aa \u2014 sinal de que o hiperplano j\u00e1 separa corretamente todas as amostras.</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#62-visualizacao-da-fronteira-de-decisao-final","title":"6.2 Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o final","text":"<p>Agora vamos criar uma visualiza\u00e7\u00e3o detalhada do resultado final, mostrando:</p> <ul> <li>Pontos corretamente classificados: marcados com s\u00edmbolos normais</li> <li>Pontos mal classificados: destacados com um 'X' para f\u00e1cil identifica\u00e7\u00e3o  </li> <li>Fronteira de decis\u00e3o: a linha aprendida pelo perceptron</li> <li>Equa\u00e7\u00e3o da fronteira: mostrando os valores finais de w\u2081, w\u2082 e b</li> </ul> <p>Esta visualiza\u00e7\u00e3o nos permite avaliar visualmente a qualidade da separa\u00e7\u00e3o:</p> <p></p> Fronteira de Decis\u00e3o <pre><code># Fun\u00e7\u00e3o para plotar fronteira de decis\u00e3o\ndef plot_decision_boundary(X, y, w, b, title=\"Fronteira de Decis\u00e3o\"):\n    plt.figure(figsize=(10, 8))\n\n    # Separar pontos por classe\n    mask_pos = y == 1\n    mask_neg = y == -1\n\n    # Plot dos pontos corretamente classificados\n    correct_mask = (y == predict(X, w, b))\n\n    plt.scatter(X[mask_neg &amp; correct_mask, 0], X[mask_neg &amp; correct_mask, 1], \n            c='blue', marker='o', alpha=0.7, s=30, label=\"Classe -1 (correto)\")\n    plt.scatter(X[mask_pos &amp; correct_mask, 0], X[mask_pos &amp; correct_mask, 1], \n            c='red', marker='s', alpha=0.7, s=30, label=\"Classe +1 (correto)\")\n\n    # Plot dos pontos mal classificados (se existirem)\n    if np.any(~correct_mask):\n        plt.scatter(X[mask_neg &amp; ~correct_mask, 0], X[mask_neg &amp; ~correct_mask, 1], \n                c='blue', marker='x', s=100, linewidth=3, label=\"Classe -1 (ERRO)\")\n        plt.scatter(X[mask_pos &amp; ~correct_mask, 0], X[mask_pos &amp; ~correct_mask, 1], \n                c='red', marker='x', s=100, linewidth=3, label=\"Classe +1 (ERRO)\")\n\n    # Calcular e plotar a linha de decis\u00e3o\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n\n    if abs(w[1]) &gt; 1e-10:\n        x1_line = np.array([x1_min, x1_max])\n        x2_line = -(w[0] * x1_line + b) / w[1]\n        plt.plot(x1_line, x2_line, 'k-', linewidth=2, label=f\"Fronteira: {w[0]:.3f}x\u2081 + {w[1]:.3f}x\u2082 + {b:.3f} = 0\")\n\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.axis('equal')\n\n    return plt.gca()\n\nplot_decision_boundary(X, y, w_final, b_final, \n                    f\"Resultado Final do Perceptron (\u00c9poca {final_epoch})\")\nplt.show()\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#63-evolucao-do-perceptron-por-epoca","title":"6.3 Evolu\u00e7\u00e3o do Perceptron por \u00c9poca","text":"<p>O painel mostra, \u00e9poca a \u00e9poca, como o perceptron aprende a separar duas classes (azul = \u22121, vermelho = +1). Em cada subgr\u00e1fico aparece:</p> <ul> <li>Fronteira de decis\u00e3o (reta preta) definida por \\(w\\cdot x + b = 0\\).</li> <li>Pontos das classes e a acur\u00e1cia obtida naquela \u00e9poca.</li> <li>A cada \u00e9poca, os pesos \\(w\\) e o bias \\(b\\) s\u00e3o ajustados a partir das amostras mal classificadas (regra do perceptron), fazendo a reta girar (mudan\u00e7a em \\(w\\)) e transladar (mudan\u00e7a em \\(b\\)) at\u00e9 atingir a separa\u00e7\u00e3o correta.</li> </ul> <p>O que observar:</p> <ul> <li>Orienta\u00e7\u00e3o da reta: o modelo explora diferentes dire\u00e7\u00f5es (sinal e magnitude dos pesos), buscando a que melhor separa os grupos.</li> <li>Posicionamento da reta: o bias desloca a fronteira para cima/baixo, refinando a divis\u00e3o.</li> <li>Acur\u00e1cia por \u00e9poca: indica o progresso do aprendizado e o qu\u00e3o perto o modelo est\u00e1 do resultado esperado.</li> <li>Estabiliza\u00e7\u00e3o: quando n\u00e3o h\u00e1 mais corre\u00e7\u00f5es necess\u00e1rias, a reta para de se mover e a acur\u00e1cia se mant\u00e9m.</li> </ul> <p></p>"},{"location":"perceptron/exercicio1/exercicio1/#64-conclusoes-a-partir-do-grafico","title":"6.4 Conclus\u00f5es a partir do gr\u00e1fico","text":"<p>O painel mostra um aprendizado progressivo e est\u00e1vel do perceptron:</p> <ul> <li>Arranque e salto de desempenho. A acur\u00e1cia parte baixa (\u2248 0,39 na \u00e9poca 1), evolui lentamente nas \u00e9pocas 2\u20133, e d\u00e1 um salto na \u00e9poca 4 (\u2248 0,73).  </li> <li>Quase perfeito cedo. J\u00e1 na \u00e9poca 5 o modelo atinge \u2248 0,9895, e entre as \u00e9pocas 6\u20138 estabiliza acima de 0,994.</li> <li>Ajustes finos com pequenas oscila\u00e7\u00f5es. Entre as \u00e9pocas 9\u201314 h\u00e1 microvaria\u00e7\u00f5es (ex.: \u00e9poca 11 \u2248 0,9950) t\u00edpicas do ajuste online em pontos pr\u00f3ximos \u00e0 fronteira; em seguida, a acur\u00e1cia volta a 0,999+.</li> <li>Converg\u00eancia. A partir das \u00e9pocas 15\u201316 a acur\u00e1cia chega a 1,0000, indicando nenhum erro no conjunto de treino.</li> <li>Geometria da fronteira. A reta inicia com inclina\u00e7\u00e3o positiva, depois gira e assume inclina\u00e7\u00e3o negativa, evidenciando a corre\u00e7\u00e3o do vetor de pesos e o deslocamento via bias at\u00e9 alinhar a separa\u00e7\u00e3o aos dados.</li> <li>Diagn\u00f3stico dos dados. O alcance de 100% sem instabilidades sugere dados linearmente separ\u00e1veis e taxa de aprendizado adequada (sem oscila\u00e7\u00f5es amplas).</li> </ul>"},{"location":"perceptron/exercicio2/exercicio2/","title":"Exerc\u00edcio 2 - Perceptron","text":"<p>O objetivo desse exercicio \u00e9 mostrar como esse modelo de perceptron age para um dataset muito mais complicado. Al\u00e9m disso, vamos refletir sobre poss\u00edveis saidas para esse problema.</p>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-1-geracao-do-conjunto-com-overlap-dados-de-treino","title":"Etapa 1 \u2014 Gera\u00e7\u00e3o do conjunto com overlap (dados de treino)","text":"<p>Objetivo do bloco: criar duas classes 2D com m\u00e9dias pr\u00f3ximas e vari\u00e2ncia alta para induzir overlap (n\u00e3o separabilidade linear perfeita), j\u00e1 com r\u00f3tulos em {-1, +1} e dados embaralhados no final.</p> <p>O que o c\u00f3digo faz:</p> <ul> <li><code>rng2 = np.random.default_rng(42)</code>: fixa a semente para reprodutibilidade.</li> <li><code>n2 = 1000</code>: define 1.000 amostras por classe (total = 2.000).</li> <li><code>mu0_ex2</code>, <code>mu1_ex2</code>: m\u00e9dias das classes [-1] \u2192 [3, 3] e [+1] \u2192 [4, 4].</li> <li><code>sigma2 = 1.5</code> e <code>cov_ex2 = [[1.5, 0], [0, 1.5]]</code>: covari\u00e2ncia isotr\u00f3pica com vari\u00e2ncia 1.5 em cada eixo.</li> <li><code>X0_ex2</code>, <code>X1_ex2</code>: amostragem multivariada normal de cada classe.</li> <li><code>y0_ex2 = -1</code>, <code>y1_ex2 = +1</code>: r\u00f3tulos.</li> <li><code>X_ex2 = vstack(...)</code>, <code>y_ex2 = hstack(...)</code>: concatena as duas classes em um \u00fanico conjunto.</li> <li><code>idx2 = rng2.permutation(...)</code> e reindexa\u00e7\u00e3o: embaralha as amostras para n\u00e3o ficarem em blocos por classe.</li> </ul> <p>Sa\u00eddas esperadas:</p> <ul> <li><code>X_ex2</code> com shape (2000, 2) e <code>y_ex2</code> com shape (2000,).</li> <li>Distribui\u00e7\u00e3o com overlap vis\u00edvel \u2014 ideal para avaliar as limita\u00e7\u00f5es do Perceptron em dados n\u00e3o totalmente separ\u00e1veis.</li> </ul> <pre><code>rng2 = np.random.default_rng(42)\n\nn2 = 1000 \n\nmu0_ex2 = np.array([3.0, 3.0])  \nmu1_ex2 = np.array([4.0, 4.0])\n\nsigma2 = 1.5\ncov_ex2 = np.array([[sigma2, 0.0], [0.0, sigma2]])\n\nX0_ex2 = rng2.multivariate_normal(mu0_ex2, cov_ex2, n2)\nX1_ex2 = rng2.multivariate_normal(mu1_ex2, cov_ex2, n2)\n\ny0_ex2 = -np.ones(n2, dtype=int)\ny1_ex2 = np.ones(n2, dtype=int)\n\nX_ex2 = np.vstack([X0_ex2, X1_ex2])\ny_ex2 = np.hstack([y0_ex2, y1_ex2])\n\nidx2 = rng2.permutation(len(X_ex2))\nX_ex2 = X_ex2[idx2]\ny_ex2 = y_ex2[idx2]\n</code></pre>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-2-visualizacao-inicial-das-classes","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o inicial das classes","text":"<p>Vamos plotar um gr\u00e1fico de dispers\u00e3o (x\u2081 vs x\u2082) separando as amostras por classe (\u22121 em azul, +1 em vermelho). Essa visualiza\u00e7\u00e3o serve para:</p> <ul> <li>Confirmar o overlap entre as distribui\u00e7\u00f5es (as nuvens se interpenetram).</li> <li>Ver forma e dispers\u00e3o dos clusters (vari\u00e2ncia maior, sem correla\u00e7\u00e3o entre eixos).</li> <li>Antecipar o comportamento do Perceptron: como a separa\u00e7\u00e3o n\u00e3o \u00e9 perfeita, a fronteira linear dever\u00e1 resultar em erros residuais mesmo ap\u00f3s o treino.</li> </ul> <p>Esta etapa \u00e9 o \u201cantes do treino\u201d: \u00e9 nossa linha de base visual para comparar depois com a fronteira aprendida.</p> <pre><code>plt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 2)\nmask_pos_ex2 = y_ex2 == 1\nmask_neg_ex2 = y_ex2 == -1\nplt.scatter(X_ex2[mask_neg_ex2, 0], X_ex2[mask_neg_ex2, 1], c='blue', alpha=0.6, s=20, label=\"Classe -1\")\nplt.scatter(X_ex2[mask_pos_ex2, 0], X_ex2[mask_pos_ex2, 1], c='red', alpha=0.6, s=20, label=\"Classe +1\")\nplt.title(\"Dispers\u00e3o das classes -1 e +1\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>An\u00e1lise do scatter</p> <ul> <li>As nuvens azul (\u22121) e vermelha (+1) est\u00e3o fortemente sobrepostas no miolo do plano \u2014 coerente com m\u00e9dias pr\u00f3ximas (\u2248[3,3] e [4,4]) e vari\u00e2ncia alta (1.5).  </li> <li>H\u00e1 uma tend\u00eancia da classe +1 ocupar valores um pouco maiores de \\(x_1\\) e \\(x_2\\), mas sem um \u201cgap\u201d limpo.  </li> <li>Consequ\u00eancia pr\u00e1tica: um limite linear dever\u00e1 separar \u201cem m\u00e9dia\u201d as classes, mas erros residuais s\u00e3o inevit\u00e1veis \u2014 esperamos acur\u00e1cia &lt; 100% e atualiza\u00e7\u00f5es persistentes durante o treino do Perceptron.</li> </ul>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-3-treinamento-do-perceptron","title":"Etapa 3 \u2014 Treinamento do Perceptron","text":"<p>Aqui treinamos o modelo reutilizando exatamente a fun\u00e7\u00e3o <code>train_perceptron</code> do Exerc\u00edcio 1 sobre o conjunto com overlap:</p> <ul> <li><code>w_ex2, b_ex2, history_ex2, y_pred_ex2 = train_perceptron(X_ex2, y_ex2, eta=0.001, max_epochs=100, shuffle=True)</code> </li> <li><code>X_ex2</code>, <code>y_ex2</code>: dados gerados na etapa anterior.  </li> <li><code>eta=0.001</code>: taxa de aprendizado menor para estabilizar a aprendizagem em dados n\u00e3o totalmente separ\u00e1veis.  </li> <li><code>max_epochs=100</code>: limite superior de \u00e9pocas.  </li> <li><code>shuffle=True</code>: embaralha as amostras a cada \u00e9poca, o que ajuda a evitar ciclos em cen\u00e1rios com overlap.  </li> <li>Sa\u00eddas:  <ul> <li><code>w_ex2</code>, <code>b_ex2</code>: pesos e vi\u00e9s aprendidos;  </li> <li><code>history_ex2</code>: lista com m\u00e9tricas por \u00e9poca (<code>accuracy</code>, <code>updates</code>, <code>w</code>, <code>b</code>);  </li> <li><code>y_pred_ex2</code>: predi\u00e7\u00f5es finais no treino ({-1, +1}).</li> </ul> </li> </ul> <p>Em seguida calculamos as m\u00e9tricas:</p> <ul> <li><code>final_acc_ex2 = accuracy(y_ex2, y_pred_ex2)</code>: acur\u00e1cia final no conjunto de treino (mesma fun\u00e7\u00e3o de utilit\u00e1rio usada no Exerc\u00edcio 1).  </li> <li><code>final_epoch_ex2 = history_ex2[-1][\"epoch\"]</code>: \u00faltima \u00e9poca executada (serve para inferir se bateu o limite de 100).  </li> <li><code>total_updates_ex2 = sum(h[\"updates\"] for h in history_ex2)</code>: total de atualiza\u00e7\u00f5es (quantas corre\u00e7\u00f5es ocorreram ao longo de todo o treinamento).</li> </ul> <p>Lembrando: como h\u00e1 overlap, \u00e9 esperado que a acur\u00e1cia n\u00e3o atinja 100% e que possamos n\u00e3o zerar atualiza\u00e7\u00f5es antes de chegar a <code>max_epochs</code>.</p> <pre><code>w_ex2, b_ex2, history_ex2, y_pred_ex2 = train_perceptron(\n    X_ex2, y_ex2, \n    eta=0.001,      \n    max_epochs=100, \n    shuffle=True\n)\n\nfinal_acc_ex2 = accuracy(y_ex2, y_pred_ex2)\nfinal_epoch_ex2 = history_ex2[-1][\"epoch\"]\ntotal_updates_ex2 = sum([h[\"updates\"] for h in history_ex2])\n\nprint(\"RESULTADOS:\")\nprint(f\"   Convergiu? {'N\u00c3O' if final_epoch_ex2 &gt;= 100 else 'SIM'}\")\nprint(f\"   \u00c9pocas usadas: {final_epoch_ex2}/100\")\nprint(f\"   Acur\u00e1cia final: {final_acc_ex2:.3f} ({final_acc_ex2*100:.1f}%)\")\nprint(f\"   Total de atualiza\u00e7\u00f5es: {total_updates_ex2}\")\n</code></pre> <pre><code>RESULTADOS:\n   Convergiu? N\u00c3O\n   \u00c9pocas usadas: 100/100\n   Acur\u00e1cia final: 0.725 (72.5%)\n   Total de atualiza\u00e7\u00f5es: 75725\n</code></pre>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-4-fronteira-de-decisao-sobre-os-dados-e-destaque-dos-erros","title":"Etapa 4 \u2014 Fronteira de decis\u00e3o sobre os dados e destaque dos erros","text":"<p>O que este bloco faz:</p> <ul> <li> <p>Reaproveita os par\u00e2metros aprendidos no treino (<code>w_ex2</code>, <code>b_ex2</code>) para sobrepor a fronteira de decis\u00e3o \\(w_1x_1 + w_2x_2 + b = 0\\) ao scatter das classes.</p> </li> <li> <p>Erros de classifica\u00e7\u00e3o s\u00e3o real\u00e7ados com marcadores \u2018x\u2019 amarelos: <code>errors_ex2 = (y_ex2 != y_pred_ex2)</code>.   Em dados com overlap, \u00e9 esperado ver esses pontos concentrados pr\u00f3ximos \u00e0 fronteira, evidenciando a limita\u00e7\u00e3o do classificador linear.</p> </li> </ul> <p>O que observar no resultado:</p> <ul> <li>Inclina\u00e7\u00e3o e posi\u00e7\u00e3o da reta refletem os pesos \\(w\\) e o bias \\(b\\) aprendidos: rota\u00e7\u00e3o \u21e2 \\(w\\), deslocamento \u21e2 \\(b\\).</li> <li>Em fun\u00e7\u00e3o do overlap, haver\u00e1 erros residuais (as marcas amarelas), mesmo ap\u00f3s treinamento, e a acur\u00e1cia n\u00e3o deve chegar a 100%.</li> <li>A fronteira se posiciona como um compromisso entre as duas distribui\u00e7\u00f5es, minimizando erros em m\u00e9dia.</li> </ul> <pre><code>plt.figure(figsize=(14, 5))\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_ex2[mask_neg_ex2, 0], X_ex2[mask_neg_ex2, 1], c='blue', alpha=0.6, s=15, label=\"Classe -1\")\nplt.scatter(X_ex2[mask_pos_ex2, 0], X_ex2[mask_pos_ex2, 1], c='red', alpha=0.6, s=15, label=\"Classe +1\")\n\nx1_range_ex2 = np.array([X_ex2[:, 0].min()-0.5, X_ex2[:, 0].max()+0.5])\nif abs(w_ex2[1]) &gt; 1e-10:\n    x2_line_ex2 = -(w_ex2[0] * x1_range_ex2 + b_ex2) / w_ex2[1]\n    plt.plot(x1_range_ex2, x2_line_ex2, 'k-', linewidth=3, label=\"Fronteira\")\n\n# Destacar erros\nerrors_ex2 = (y_ex2 != y_pred_ex2)\nif np.any(errors_ex2):\n    plt.scatter(X_ex2[errors_ex2, 0], X_ex2[errors_ex2, 1], \n               c='yellow', marker='x', s=50, linewidth=2, label=\"Erros\")\n\nplt.title(f\"Fronteira de decis\u00e3o: {final_acc_ex2:.3f} acur\u00e1cia\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-5-conclusoes-sobre-os-resultados","title":"Etapa 5 \u2014 Conclus\u00f5es sobre os resultados","text":"<p>A fronteira aprendida \u00e9 linear (reta com inclina\u00e7\u00e3o negativa) e a acur\u00e1cia final ficou em \u2248 0,725. Os erros (marcados em amarelo) concentram-se na regi\u00e3o central onde as duas distribui\u00e7\u00f5es se sobrep\u00f5em, exatamente onde um limite linear n\u00e3o consegue discriminar perfeitamente.</p> <p>Por que chegamos a ~0,72 de acur\u00e1cia? - Os dados foram gerados por duas gaussianas com m\u00e9dias pr\u00f3ximas \\([3,3]\\) e \\([4,4]\\) e mesma covari\u00e2ncia isotr\u00f3pica \\(\\Sigma = 1.5\\,I\\). Logo, o problema n\u00e3o \u00e9 linearmente separ\u00e1vel. - O Perceptron ter chegado a 0,725 \u00e9 coerente e pr\u00f3ximo do \u00f3timo para um modelo linear.</p> <p>E o MLP? Um Multi-Layer Perceptron (com ativa\u00e7\u00f5es n\u00e3o lineares) pode aprender fronteiras curvas e superar modelos lineares se a separa\u00e7\u00e3o \u00f3tima for n\u00e3o linear. Neste dataset espec\u00edfico (duas gaussianas com covari\u00e2ncias iguais), o \u00f3timo \u00e9 linear; portanto, um MLP bem regularizado tende a empatar com um linear em m\u00e9dia (no teste). Ele s\u00f3 superaria de forma consistente se a estrutura real dos dados exigir n\u00e3o linearidade.</p> <p>Em suma: era previsto que uma reta n\u00e3o separasse perfeitamente essas duas classes. Para ganhos reais, use lineares bem calibrados ou modelos n\u00e3o lineares quando houver evid\u00eancia de fronteira n\u00e3o linear.</p>"},{"location":"projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}