{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Redes Neurais e Deep Learning \u2014 Insper","text":"<p>Este portf\u00f3lio re\u00fane as atividades e projetos desenvolvidos na disciplina de Redes Neurais e Deep Learning do Insper. O objetivo \u00e9 documentar de forma clara e organizada cada etapa do aprendizado, desde os primeiros conceitos de separabilidade de dados at\u00e9 a implementa\u00e7\u00e3o de modelos mais avan\u00e7ados, como redes neurais multicamadas e modelos generativos.</p> <p>Ao longo deste material, est\u00e3o descritos:</p> <ul> <li>A motiva\u00e7\u00e3o de cada exerc\u00edcio ou projeto,</li> <li>O passo a passo de implementa\u00e7\u00e3o,</li> <li>As an\u00e1lises dos resultados obtidos,</li> <li>E as conclus\u00f5es em rela\u00e7\u00e3o ao uso de redes neurais para diferentes problemas.</li> </ul>"},{"location":"#estrutura-do-portfolio","title":"Estrutura do Portf\u00f3lio","text":""},{"location":"#exercicios","title":"\ud83d\udcdd Exerc\u00edcios","text":"<p>Os exerc\u00edcios pr\u00e1ticos t\u00eam como foco a explora\u00e7\u00e3o de conceitos fundamentais de redes neurais.  </p> <ol> <li>Data \u2014 Gera\u00e7\u00e3o e an\u00e1lise de dados sint\u00e9ticos para explorar separabilidade de classes e limites de decis\u00e3o.  </li> <li>Perceptron \u2014 Implementa\u00e7\u00e3o e avalia\u00e7\u00e3o de um perceptron simples para problemas linearmente separ\u00e1veis.  </li> <li>MLP (Multi-Layer Perceptron) \u2014 Constru\u00e7\u00e3o e treinamento de uma rede neural multicamadas para lidar com problemas n\u00e3o lineares.  </li> <li>Metrics \u2014 An\u00e1lise de m\u00e9tricas de avalia\u00e7\u00e3o, discutindo acur\u00e1cia, precis\u00e3o, recall e F1-score no contexto de classifica\u00e7\u00e3o.</li> </ol>"},{"location":"#projetos","title":"\ud83d\ude80 Projetos","text":"<p>Os projetos aplicam os conceitos estudados em problemas mais complexos e realistas.  </p> <ol> <li>Classification \u2014 Modelos de classifica\u00e7\u00e3o em diferentes conjuntos de dados, explorando arquiteturas de redes neurais.  </li> <li>Regression \u2014 Aplica\u00e7\u00e3o de redes neurais em tarefas de regress\u00e3o, analisando desempenho e capacidade de generaliza\u00e7\u00e3o.  </li> <li>Generative Models \u2014 Estudo e implementa\u00e7\u00e3o de modelos generativos (como autoencoders ou GANs), avaliando seu potencial em criar ou reconstruir dados.</li> </ol>"},{"location":"#status-de-desenvolvimento","title":"\ud83d\udccc Status de Desenvolvimento","text":"<ul> <li> Exerc\u00edcio 1 (Data) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 2 (Perceptron) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 3 (MLP) conclu\u00eddo e documentado.  </li> <li>[] Exerc\u00edcio 4 (Metrics) conclu\u00eddo e documentado.  </li> <li>[] Projeto 1 (Classification) em andamento.  </li> <li>[] Projeto 2 (Regression) em andamento.  </li> <li>[] Projeto 3 (Generative Models) planejado para pr\u00f3xima etapa.  </li> </ul>"},{"location":"#conclusao","title":"\ud83c\udfaf Conclus\u00e3o","text":"<p>Este portf\u00f3lio funciona como um di\u00e1rio de bordo da disciplina, mostrando n\u00e3o apenas os c\u00f3digos implementados, mas tamb\u00e9m as reflex\u00f5es sobre os resultados e os aprendizados obtidos. A ideia \u00e9 que ele sirva como refer\u00eancia tanto para revisitar conceitos importantes quanto para inspirar futuros trabalhos na \u00e1rea de Intelig\u00eancia Artificial aplicada a Redes Neurais.</p>"},{"location":"data/exercicio1/exercicio1/","title":"Exerc\u00edcio 1 \u2014 Data","text":""},{"location":"data/exercicio1/exercicio1/#objetivo","title":"Objetivo","text":"<p>O objetivo deste exerc\u00edcio foi gerar um conjunto de dados sint\u00e9ticos em 2 dimens\u00f5es para analisar sua separabilidade entre classes. Esse processo \u00e9 importante porque ajuda a entender como uma rede neural simples ou mais profunda teria que se adaptar para classificar os dados corretamente.</p>"},{"location":"data/exercicio1/exercicio1/#etapa-1-geracao-dos-dados-sinteticos","title":"Etapa 1 \u2014 Gera\u00e7\u00e3o dos Dados Sint\u00e9ticos","text":"<p>O objetivo desta etapa foi criar um conjunto de dados bidimensionais (vari\u00e1veis <code>x1</code> e <code>x2</code>) divididos em 4 classes distintas, cada uma com 100 pontos, totalizando 400 amostras.</p> <p>Para garantir reprodutibilidade, defini a semente do gerador de n\u00fameros aleat\u00f3rios (<code>np.random.seed(42)</code>), o que faz com que os mesmos pontos sejam gerados a cada execu\u00e7\u00e3o do c\u00f3digo.</p> <p>Cada classe foi gerada usando a fun\u00e7\u00e3o <code>np.random.normal</code>, que amostra valores de uma distribui\u00e7\u00e3o normal (gaussiana) a partir de um valor m\u00e9dio (<code>loc</code>) e um desvio padr\u00e3o (<code>scale</code>). Assim:</p> <ul> <li>Classe 0: centrada em (2, 3), mais espalhada no eixo y (<code>scale=2.5</code>).  </li> <li>Classe 1: centrada em (5, 6), com varia\u00e7\u00e3o moderada em ambos os eixos.  </li> <li>Classe 2: centrada em (8, 1), compacta em torno da m\u00e9dia.  </li> <li>Classe 3: centrada em (15, 4), bem concentrada em <code>x1</code> mas com maior varia\u00e7\u00e3o em <code>x2</code>.</li> </ul> <p>Por fim, usei <code>np.column_stack</code> para juntar as duas vari\u00e1veis (<code>x1</code>, <code>x2</code>) em um array bidimensional, representando os pontos de cada classe.</p> <pre><code>np.random.seed(42)  \nn = 100\n\n# Classe 0\nc0_x1 = np.random.normal(loc=2, scale=0.8, size=n)\nc0_x2 = np.random.normal(loc=3, scale=2.5, size=n)\nc0 = np.column_stack((c0_x1, c0_x2))\n\n# Classe 1\nc1_x1 = np.random.normal(loc=5, scale=1.2, size=n)\nc1_x2 = np.random.normal(loc=6, scale=1.9, size=n)\nc1 = np.column_stack((c1_x1, c1_x2))\n\n# Classe 2\nc2_x1 = np.random.normal(loc=8, scale=0.9, size=n)\nc2_x2 = np.random.normal(loc=1, scale=0.9, size=n)\nc2 = np.column_stack((c2_x1, c2_x2))\n\n# Classe 3\nc3_x1 = np.random.normal(loc=15, scale=0.5, size=n)\nc3_x2 = np.random.normal(loc=4, scale=2.0, size=n)\nc3 = np.column_stack((c3_x1, c3_x2))\n</code></pre>"},{"location":"data/exercicio1/exercicio1/#etapa-2-visualizacao-dos-dados","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o dos Dados","text":"<p>Com as quatro classes j\u00e1 geradas, o pr\u00f3ximo passo foi visualizar a distribui\u00e7\u00e3o dos pontos em um gr\u00e1fico de dispers\u00e3o (scatter plot).</p> <p>Esse gr\u00e1fico \u00e9 essencial porque permite observar a separabilidade entre classes e j\u00e1 adianta poss\u00edveis regi\u00f5es de sobreposi\u00e7\u00e3o que precisaremos analisar na pr\u00f3xima etapa.</p> <pre><code>plt.figure(figsize=(7,5))\nplt.scatter(c0[:,0], c0[:,1], label=\"Class 0\", s=18, alpha=0.8)\nplt.scatter(c1[:,0], c1[:,1], label=\"Class 1\", s=18, alpha=0.8)\nplt.scatter(c2[:,0], c2[:,1], label=\"Class 2\", s=18, alpha=0.8)\nplt.scatter(c3[:,0], c3[:,1], label=\"Class 3\", s=18, alpha=0.8)\n\nplt.title(\"Data\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p> <p>Observando o gr\u00e1fico de dispers\u00e3o obtido:</p> <ul> <li> <p>Classe 0 (azul): est\u00e1 localizada \u00e0 esquerda, centrada em torno de <code>x1 \u2248 2</code>.   Apresenta grande varia\u00e7\u00e3o em <code>x2</code>, formando uma nuvem vertical que vai de valores negativos at\u00e9 acima de 7.  </p> </li> <li> <p>Classe 1 (laranja): posicionada mais ao centro, em <code>x1 \u2248 5</code>.   Distribui-se em n\u00edveis mais altos de <code>x2</code> (acima de 5), com dispers\u00e3o consider\u00e1vel, o que gera sobreposi\u00e7\u00e3o com a Classe 0 em algumas regi\u00f5es.  </p> </li> <li> <p>Classe 2 (verde): centrada em <code>x1 \u2248 8</code> e valores baixos de <code>x2</code> (por volta de 1).   \u00c9 a classe mais compacta, com baixa dispers\u00e3o, o que facilita sua identifica\u00e7\u00e3o. No entanto, apresenta certa proximidade com a Classe 1 na regi\u00e3o de fronteira.  </p> </li> <li> <p>Classe 3 (vermelha): bem afastada das demais, em <code>x1 \u2248 15</code>.   Mesmo com varia\u00e7\u00e3o em <code>x2</code>, mant\u00e9m-se completamente separada das outras classes, o que torna sua classifica\u00e7\u00e3o a mais simples do conjunto.</p> </li> </ul>"},{"location":"data/exercicio1/exercicio1/#conclusoes-sobre-separabilidade","title":"Conclus\u00f5es sobre separabilidade","text":"<ul> <li>Existe uma ordem clara das classes ao longo do eixo <code>x1</code>: C0 \u2192 C1 \u2192 C2 \u2192 C3.  </li> <li>Por\u00e9m, as classes 0 e 1 apresentam sobreposi\u00e7\u00e3o em parte do espa\u00e7o (principalmente entre <code>x1=2</code> e <code>x1=5</code>), o que dificulta a separa\u00e7\u00e3o por um limite totalmente linear.  </li> <li>A Classe 2 \u00e9 bem definida e isolada verticalmente, embora esteja pr\u00f3xima da Classe 1 na horizontal.  </li> <li>A Classe 3 est\u00e1 completamente isolada, sendo a mais f\u00e1cil de separar.  </li> </ul>"},{"location":"data/exercicio1/exercicio1/#etapa-4-definicao-de-fronteiras-lineares-simples","title":"Etapa 4 \u2014 Defini\u00e7\u00e3o de Fronteiras Lineares Simples","text":"<p>Para explorar a separabilidade dos dados, tracei fronteiras lineares verticais ao longo do eixo <code>x1</code>, representando limites de decis\u00e3o iniciais entre as classes.</p> <pre><code>xlines = [\n    (2 + 5) / 2,   \n    (5 + 8) / 2,   \n    (8 + 15) / 2   \n]\n\nplt.figure(figsize=(7,5))\nplt.scatter(c0[:,0], c0[:,1], label=\"Class 0\", s=18, alpha=0.8)\nplt.scatter(c1[:,0], c1[:,1], label=\"Class 1\", s=18, alpha=0.8)\nplt.scatter(c2[:,0], c2[:,1], label=\"Class 2\", s=18, alpha=0.8)\nplt.scatter(c3[:,0], c3[:,1], label=\"Class 3\", s=18, alpha=0.8)\n\nfor x in xlines:\n    plt.axvline(x, linestyle=\"--\", linewidth=2)\n\nplt.title(\"Scatter + fronteiras lineares (simples)\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Divis\u00e3o das classes</p>"},{"location":"data/exercicio1/exercicio1/#analise","title":"An\u00e1lise","text":"<ul> <li>Esse m\u00e9todo de fronteira \u00e9 simples e intuitivo, funcionando bem quando as classes est\u00e3o distribu\u00eddas principalmente em torno de valores diferentes de <code>x1</code>.  </li> <li>No entanto, ele n\u00e3o considera a dispers\u00e3o em <code>x2</code>, o que gera problemas de sobreposi\u00e7\u00e3o:  </li> <li>Classes 0 e 1 continuam com regi\u00f5es de confus\u00e3o, pois se sobrep\u00f5em verticalmente.  </li> <li>Classes 1 e 2 tamb\u00e9m podem apresentar mistura em regi\u00f5es pr\u00f3ximas \u00e0 linha divis\u00f3ria.  </li> <li>A Classe 3, por estar bem afastada, \u00e9 separada de forma perfeita com esse limite linear.</li> </ul>"},{"location":"data/exercicio1/exercicio1/#implicacao-para-redes-neurais","title":"Implica\u00e7\u00e3o para redes neurais","text":"<ul> <li>Um modelo linear simples poderia usar limites parecidos com estes para classificar os dados.  </li> <li>Por\u00e9m, como h\u00e1 sobreposi\u00e7\u00e3o entre classes, uma rede neural com m\u00faltiplas camadas e fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares teria mais flexibilidade para ajustar fronteiras curvas ou inclinadas, capturando melhor as regi\u00f5es de confus\u00e3o.</li> </ul>"},{"location":"data/exercicio2/exercicio2/","title":"Exercicio 2 - Data","text":""},{"location":"data/exercicio2/exercicio2/#objetivo","title":"Objetivo","text":"<p>O objetivo deste exerc\u00edcio foi criar dois conjuntos de dados em 5 dimens\u00f5es (Classes A e B), cada um com 500 amostras, a partir de distribui\u00e7\u00f5es normais multivariadas. Em seguida, reduzir a dimensionalidade com PCA para 2D e analisar a separabilidade linear.  </p> <p>Minha hip\u00f3tese inicial era de que, como as classes possuem covari\u00e2ncias diferentes, a fronteira \u00f3tima n\u00e3o seria linear, representando um desafio para modelos simples como Perceptron ou Regress\u00e3o Log\u00edstica, e justificando o uso de modelos mais complexos (ex: redes neurais).</p>"},{"location":"data/exercicio2/exercicio2/#etapa-1-definicao-dos-parametros-e-semente-do-gerador","title":"Etapa 1 \u2014 Defini\u00e7\u00e3o dos par\u00e2metros e semente do gerador","text":"<p>Aqui eu preparei o gerador de n\u00fameros aleat\u00f3rios e defini os par\u00e2metros das distribui\u00e7\u00f5es normais multivariadas para as duas classes (A e B).</p> <ul> <li> <p><code>rng = np.random.default_rng(42)</code>   Criei um gerador pseudoaleat\u00f3rio com semente 42 para garantir reprodutibilidade. Isso significa que, ao rodar o c\u00f3digo v\u00e1rias vezes, os mesmos dados ser\u00e3o gerados.</p> </li> <li> <p><code>mu_A</code> e <code>mu_B</code>   S\u00e3o os vetores de m\u00e9dias das classes, cada um com 5 dimens\u00f5es.  </p> </li> <li>Classe A \u00e9 centrada na origem.  </li> <li> <p>Classe B \u00e9 deslocada em rela\u00e7\u00e3o \u00e0 A (todas as m\u00e9dias iguais a 1.5).</p> </li> <li> <p><code>Sigma_A</code> e <code>Sigma_B</code>   S\u00e3o as matrizes de covari\u00e2ncia (5\u00d75) que controlam o espalhamento e as correla\u00e7\u00f5es entre as vari\u00e1veis de cada classe.  </p> </li> <li>Em A, h\u00e1 correla\u00e7\u00f5es positivas entre algumas vari\u00e1veis.  </li> <li>Em B, h\u00e1 correla\u00e7\u00f5es negativas e vari\u00e2ncias maiores.  </li> </ul> <p>Esses par\u00e2metros ser\u00e3o usados na fun\u00e7\u00e3o <code>rng.multivariate_normal</code> para gerar os pontos de cada classe na etapa seguinte.</p> <pre><code>rng = np.random.default_rng(42)\n\nmu_A = np.array([0., 0., 0., 0., 0.])\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n])\n\nmu_B = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n])\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-2-amostragem-dos-dados-e-criacao-dos-rotulos","title":"Etapa 2 \u2014 Amostragem dos dados e cria\u00e7\u00e3o dos r\u00f3tulos","text":"<p>Nesta parte eu realmente gerei os pontos das duas classes e preparei os conjuntos de dados e r\u00f3tulos.</p> <ul> <li> <p><code>nA = nB = 500</code>   Defini que cada classe ter\u00e1 500 amostras, totalizando 1000 pontos.</p> </li> <li> <p><code>rng.multivariate_normal(mean=..., cov=..., size=...)</code>   Fun\u00e7\u00e3o que gera amostras de uma distribui\u00e7\u00e3o normal multivariada.  </p> </li> <li><code>mean</code> \u2192 vetor de m\u00e9dias da classe.  </li> <li><code>cov</code> \u2192 matriz de covari\u00e2ncia que define vari\u00e2ncias e correla\u00e7\u00f5es.  </li> <li> <p><code>size</code> \u2192 n\u00famero de amostras a serem geradas.   Assim, <code>A</code> cont\u00e9m os pontos da Classe A (500\u00d75) e <code>B</code> os da Classe B (500\u00d75).</p> </li> <li> <p><code>np.vstack([A, B])</code>   Empilhei os dois conjuntos verticalmente, formando a matriz <code>X</code> com shape (1000, 5).</p> </li> <li> <p><code>np.hstack([np.zeros(nA, int), np.ones(nB, int)])</code>   Criei o vetor de r\u00f3tulos <code>y</code>:  </p> </li> <li><code>0</code> representa a Classe A.  </li> <li><code>1</code> representa a Classe B.   Isso me permite identificar de qual classe cada ponto pertence.</li> </ul> <p>Com isso, foi finalizada a gera\u00e7\u00e3o do dataset 5D completo, pronto para redu\u00e7\u00e3o de dimensionalidade e an\u00e1lise.</p> <pre><code>nA = nB = 500\nA = rng.multivariate_normal(mean=mu_A, cov=Sigma_A, size=nA)\nB = rng.multivariate_normal(mean=mu_B, cov=Sigma_B, size=nB)\n\nX = np.vstack([A, B])           \ny = np.hstack([np.zeros(nA, int), np.ones(nB, int)])\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-3-reducao-de-dimensionalidade-com-pca-5d-2d","title":"Etapa 3 \u2014 Redu\u00e7\u00e3o de dimensionalidade com PCA (5D \u2192 2D)","text":"<p>Nesta etapa eu utilizei o PCA (Principal Component Analysis) para reduzir os dados de 5 dimens\u00f5es para apenas 2, permitindo visualiza\u00e7\u00e3o em gr\u00e1fico de dispers\u00e3o.</p> <ul> <li><code>pca = PCA(n_components=2, random_state=42)</code>   Criei um objeto PCA que mant\u00e9m apenas 2 componentes principais.  </li> <li> <p>O PCA encontra dire\u00e7\u00f5es de maior vari\u00e2ncia nos dados, ignorando r\u00f3tulos de classe.</p> </li> <li> <p><code>X2 = pca.fit_transform(X)</code>   Aqui eu ajustei o PCA aos dados 5D (<code>fit</code>) e projetei os pontos nesses dois eixos principais (<code>transform</code>).   O resultado \u00e9 uma matriz <code>X2</code> com shape (1000, 2), representando cada amostra em duas dimens\u00f5es (PC1 e PC2).</p> </li> <li> <p><code>df[\"pc1\"] = X2[:, 0]</code> e <code>df[\"pc2\"] = X2[:, 1]</code>   Adicionei as duas novas colunas ao DataFrame para facilitar visualiza\u00e7\u00e3o e plotagem.</p> </li> <li> <p><code>pca.explained_variance_ratio_</code>   Essa fun\u00e7\u00e3o retorna quanto da vari\u00e2ncia total dos dados originais foi preservada por cada componente.  </p> </li> <li>O <code>print</code> mostra a vari\u00e2ncia explicada por PC1 e PC2 separadamente.  </li> <li>Tamb\u00e9m calculei a soma, para verificar quanta informa\u00e7\u00e3o 5D conseguimos reter em 2D.</li> </ul> <pre><code>pca = PCA(n_components=2, random_state=42)\nX2 = pca.fit_transform(X)\ndf[\"pc1\"] = X2[:, 0]\ndf[\"pc2\"] = X2[:, 1]\n\nprint(\"Vari\u00e2ncia explicada (PC1, PC2):\", np.round(pca.explained_variance_ratio_, 3))\nprint(\"Soma da vari\u00e2ncia explicada:\", np.round(pca.explained_variance_ratio_.sum(), 3))\n</code></pre>"},{"location":"data/exercicio2/exercicio2/#etapa-4-visualizacao-grafica-das-classes-no-espaco-2d","title":"Etapa 4 \u2014 Visualiza\u00e7\u00e3o gr\u00e1fica das classes no espa\u00e7o 2D","text":"<p>Nesta etapa eu fiz a visualiza\u00e7\u00e3o final dos dados ap\u00f3s a redu\u00e7\u00e3o de dimensionalidade com PCA. Usei o Matplotlib para criar um gr\u00e1fico de dispers\u00e3o que mostra as duas classes projetadas no plano formado pelos dois componentes principais (PC1 e PC2).</p> <p>Esse gr\u00e1fico permite comparar visualmente como as duas classes se distribuem em 2D e perceber se existe ou n\u00e3o separa\u00e7\u00e3o linear entre elas.</p> <pre><code>plt.figure(figsize=(7, 5))\nplt.scatter(df.loc[df[\"class\"] == 0, \"pc1\"], df.loc[df[\"class\"] == 0, \"pc2\"],\n            s=14, alpha=0.8, label=\"Classe A\")\nplt.scatter(df.loc[df[\"class\"] == 1, \"pc1\"], df.loc[df[\"class\"] == 1, \"pc2\"],\n            s=14, alpha=0.8, label=\"Classe B\")\nplt.title(\"PCA (5D \u2192 2D): Classe A vs Classe B\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.grid(True, linestyle=\":\", linewidth=0.8)\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p> <p>Separabilidade linear (an\u00e1lise final):</p> <p>Na proje\u00e7\u00e3o em 2D, \u00e9 poss\u00edvel perceber que as duas classes n\u00e3o ficam totalmente separadas: h\u00e1 regi\u00f5es onde os pontos de A e B se misturam. Mesmo considerando os dados no espa\u00e7o original de 5 dimens\u00f5es, a forma como as duas classes est\u00e3o distribu\u00eddas faz com que uma linha reta n\u00e3o seja suficiente para dividi-las corretamente.</p> <p>Isso significa que modelos simples que trabalham apenas com separa\u00e7\u00f5es retas, como o Perceptron, n\u00e3o conseguem representar bem esse tipo de situa\u00e7\u00e3o. Por outro lado, modelos mais avan\u00e7ados, como redes neurais, conseguem criar fronteiras curvas e mais flex\u00edveis, o que permite distinguir melhor as duas classes mesmo quando elas n\u00e3o est\u00e3o separadas de forma simples.</p>"},{"location":"data/exercicio3/exercicio3/","title":"Exerc\u00edcio 3 \u2014 DATA","text":""},{"location":"data/exercicio3/exercicio3/#objetivo","title":"Objetivo","text":"<p>O objetivo \u00e9 preparar o conjunto de dados Spaceship Titanic (train.csv do Kaggle) para uso em uma rede neural que utiliza a fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh.  </p> <p>A seguir, realizei o passo a passo de carregamento, explora\u00e7\u00e3o, tratamento de nulos, codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas e padroniza\u00e7\u00e3o de num\u00e9ricas, al\u00e9m de visualizar o impacto do pr\u00e9-processamento.</p>"},{"location":"data/exercicio3/exercicio3/#etapa-1-carregamento-e-descricao-do-dataset","title":"Etapa 1 \u2014 Carregamento e descri\u00e7\u00e3o do dataset","text":"<p>O arquivo <code>train.csv</code> cont\u00e9m cerca de 8700 passageiros. Cada linha representa um passageiro, com informa\u00e7\u00f5es pessoais, dados de viagem e gastos. A coluna <code>Transported</code> \u00e9 o alvo (True/False), indicando se o passageiro foi transportado para outra dimens\u00e3o.</p> <p>Principais tipos de vari\u00e1veis:</p> <ul> <li>Identifica\u00e7\u00e3o: <code>PassengerId</code>, <code>Name</code> .</li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>.</li> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.</li> <li>Alvo: <code>Transported</code>.</li> </ul> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"data/train.csv\")\ndisplay(df.head())\nprint(df.info())\nprint(df.isnull().sum())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#valores-ausentes","title":"Valores ausentes","text":"<p>Ap\u00f3s carregar o dataset <code>train.csv</code>, percebi que ele possui 8693 linhas e 14 colunas. Algumas vari\u00e1veis apresentam valores ausentes, enquanto outras est\u00e3o completas:</p> <ul> <li>Sem valores ausentes:</li> <li><code>PassengerId</code> \u2192 identificador \u00fanico de cada passageiro.  </li> <li> <p><code>Transported</code> \u2192 vari\u00e1vel alvo (true ou false para saber se o passageiro foi transportado).</p> </li> <li> <p>Com valores ausentes:</p> </li> <li><code>HomePlanet</code> (201) \u2192 planeta de origem.  </li> <li><code>CryoSleep</code> (217) \u2192 se o passageiro estava em criossuspens\u00e3o.  </li> <li><code>Cabin</code> (199) \u2192 cabine onde estava hospedado.  </li> <li><code>Destination</code> (182) \u2192 planeta de destino.  </li> <li><code>Age</code> (179) \u2192 idade do passageiro.  </li> <li><code>VIP</code> (203) \u2192 se contratou servi\u00e7o VIP.  </li> <li><code>RoomService</code> (181), <code>FoodCourt</code> (183), <code>ShoppingMall</code> (208), <code>Spa</code> (183), <code>VRDeck</code> (188) \u2192 gastos.  </li> <li><code>Name</code> (200) \u2192 nome do passageiro.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>O conjunto apresenta valores faltantes em quase todas as colunas de entrada.  </li> <li>O percentual de valores ausentes em cada coluna \u00e9 relativamente pequeno (entre 2% e 3% do total de linhas), ou seja, \u00e9 vi\u00e1vel aplicar t\u00e9cnicas de preenchimento  em vez de descartar linhas inteiras.  </li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-2-definicao-dos-tipos-de-variaveis","title":"Etapa 2 \u2014 Defini\u00e7\u00e3o dos tipos de vari\u00e1veis","text":"<ul> <li>Num\u00e9ricas: <code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>.  </li> <li>Categ\u00f3ricas: <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>.  </li> <li>Alvo: <code>Transported</code>.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-3-tratamento-de-valores-ausentes","title":"Etapa 3 \u2014 Tratamento de valores ausentes","text":"<p>Pelo <code>df.info()</code> e <code>df.isnull().sum()</code>:</p> <ul> <li>H\u00e1 nulos em quase todas as colunas de entrada (entre ~179 e ~217 linhas por coluna, \u22482%\u20133% do total de 8693).</li> </ul> <p>Estrat\u00e9gia adotada:</p> <ol> <li> <p>Categ\u00f3ricas (<code>HomePlanet</code>, <code>CryoSleep</code>, <code>Cabin</code>, <code>Destination</code>, <code>VIP</code>):</p> </li> <li> <p>Preencher com a moda (valor mais frequente).</p> </li> <li> <p>Para <code>Cabin</code>, al\u00e9m de preencher, separar em <code>Deck</code>/<code>Num</code>/<code>Side</code> (formato <code>deck/num/side</code>), pois \u00e9 uma string composta que carrega informa\u00e7\u00e3o \u00fatil.</p> </li> <li> <p>Num\u00e9ricas (<code>Age</code>, <code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>):</p> </li> <li> <p>Preencher com a mediana (robusta a outliers).</p> </li> <li> <p>Regra de neg\u00f3cio adicional: se <code>CryoSleep == True</code>, os gastos deveriam ser zero (passageiro confinado \u00e0 cabine). Assim, nulos nos gastos para quem est\u00e1 em <code>CryoSleep</code> ser\u00e3o imputados com 0; os demais, com mediana.</p> </li> </ol>"},{"location":"data/exercicio3/exercicio3/#etapa-31-preparos-para-imputacao","title":"Etapa 3.1 \u2014 Preparos para imputa\u00e7\u00e3o","text":"<p>Antes de imputar:</p> <ul> <li> <p>Defino listas auxiliares de colunas.</p> </li> <li> <p>Converto <code>CryoSleep</code> e <code>VIP</code> para booleanos/bits depois de preencher (eles vieram como <code>object</code> por causa dos NaNs).</p> </li> <li> <p>Para <code>Cabin</code>, separo em tr\u00eas colunas (<code>Deck</code>, <code>Num</code>, <code>Side</code>), imputo <code>Deck</code>/<code>Side</code> com a moda e <code>Num</code> com a mediana (num\u00e9rica).</p> </li> </ul> <pre><code>num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Cabin\", \"Destination\", \"VIP\"]  # Cabin ser\u00e1 expandida\ntarget = \"Transported\"\n\ncabin_split = df[\"Cabin\"].str.split(\"/\", expand=True)\ndf[\"Deck\"] = cabin_split[0]\ndf[\"Num\"]  = pd.to_numeric(cabin_split[1], errors=\"coerce\")  # num\u00e9rico\ndf[\"Side\"] = cabin_split[2]\n\ncat_cols_expanded = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"]\nnum_cols_expanded = num_cols + [\"Num\"]\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-32-imputacao-das-categoricas","title":"Etapa 3.2 \u2014 Imputa\u00e7\u00e3o das categ\u00f3ricas","text":"<p>Regra: preencher categ\u00f3ricas com a moda (valor mais frequente). Depois, padronizo <code>CryoSleep</code> e <code>VIP</code> para inteiros 0/1 (necess\u00e1rio para modelos e para aplicar a regra dos gastos = 0 se em criossono).</p> <pre><code>for col in [\"HomePlanet\", \"Destination\", \"Deck\", \"Side\", \"CryoSleep\", \"VIP\"]:\n    df[col] = df[col].fillna(df[col].mode()[0])\n\ndef to_bool01(series):\n    return (\n        series\n        .replace({True: 1, False: 0, \"True\": 1, \"False\": 0, \"TRUE\": 1, \"FALSE\": 0})\n        .astype(int)\n    )\n\ndf[\"CryoSleep\"] = to_bool01(df[\"CryoSleep\"])\ndf[\"VIP\"]       = to_bool01(df[\"VIP\"])\n\ndf[\"Transported\"] = df[\"Transported\"].astype(int)\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-33-imputacao-das-numericas","title":"Etapa 3.3 \u2014 Imputa\u00e7\u00e3o das num\u00e9ricas","text":"<p>Regra geral: imputar mediana. Regra de neg\u00f3cio adicional para gastos: se <code>CryoSleep == 1</code>, nulos em gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>) viram 0; os demais nulos seguem para mediana.</p> <p>Para <code>Num</code> (n\u00famero da cabine, derivado de <code>Cabin</code>), uso mediana.</p> <pre><code>spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n\nfor col in spend_cols:\n    mask = (df[\"CryoSleep\"] == 1) &amp; (df[col].isna())\n    df.loc[mask, col] = 0.0\n\nfor col in num_cols_expanded:\n    df[col] = df[col].fillna(df[col].median())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-34-limpeza-final-desta-etapa","title":"Etapa 3.4 \u2014 Limpeza final desta etapa","text":"<ul> <li><code>Cabin</code> original \u00e9 removida (substitu\u00edda por <code>Deck</code>, <code>Num</code>, <code>Side</code>).</li> <li><code>Name</code> n\u00e3o ser\u00e1 usado como atributo neste exerc\u00edcio (texto livre), ent\u00e3o removo.</li> <li>Fa\u00e7o uma checagem final para garantir zero valores ausentes nas colunas que seguem para o modelo.</li> </ul> <pre><code>df = df.drop(columns=[\"Cabin\", \"Name\"])\n\nmissing_total = df.isnull().sum().sum()\nprint(f\"valores ausentes: {missing_total}\")\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-35-comentarios-e-justificativas","title":"Etapa 3.5 \u2014 Coment\u00e1rios e justificativas","text":"<ul> <li>Moda nas categ\u00f3ricas: mant\u00e9m a coer\u00eancia dos dados e evita criar categorias artificiais com baixa frequ\u00eancia.</li> <li>Mediana nas num\u00e9ricas: \u00e9 robusta a valores extremos (outliers), comuns nas colunas de gastos.</li> <li>Regra de gastos = 0 em CryoSleep: condiz com a defini\u00e7\u00e3o do problema (passageiros em criossono ficam na cabine e n\u00e3o consomem servi\u00e7os).</li> <li>Separar <code>Cabin</code> em <code>Deck</code>/<code>Num</code>/<code>Side</code>: aproveita a estrutura da informa\u00e7\u00e3o, em vez de trat\u00e1-la como uma string opaca. <code>Deck</code> e <code>Side</code> entram como categ\u00f3ricas; <code>Num</code>, como num\u00e9rica.</li> <li>Remover <code>Name</code>: campo textual livre, de pouca utilidade neste exerc\u00edcio de pr\u00e9-processamento para uma MLP com <code>tanh</code> (sem embeddings/PLN). Pode ser reintroduzido em projetos onde se extraem sobrenomes/grupos.</li> <li>Ap\u00f3s estas decis\u00f5es, o dataset est\u00e1 livre de nulos e pronto para codifica\u00e7\u00e3o categ\u00f3rica e escalonamento (pr\u00f3ximas etapas).</li> </ul>"},{"location":"data/exercicio3/exercicio3/#etapa-4-codificacao-de-variaveis-categoricas","title":"Etapa 4 \u2014 Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas","text":"<p>Para treinar uma rede neural, todas as entradas precisam estar em formato num\u00e9rico.</p> <ul> <li>Vari\u00e1veis bin\u00e1rias:  </li> <li><code>CryoSleep</code> e <code>VIP</code> j\u00e1 foram convertidas para 0/1.  </li> <li>Vari\u00e1veis multiclasse:  </li> <li><code>HomePlanet</code>, <code>Destination</code>, <code>Deck</code>, <code>Side</code> \u2192 apliquei One-Hot Encoding, criando colunas dummy (0/1).  </li> </ul> <p>Assim, todas as categorias foram transformadas em indicadores num\u00e9ricos sem introduzir ordens artificiais.</p> <pre><code>df = pd.get_dummies(df, columns=[\"HomePlanet\", \"Destination\", \"Deck\", \"Side\"], drop_first=True)\n\ndisplay(df.head())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-5-escalonamento-dos-atributos-numericos","title":"Etapa 5 \u2014 Escalonamento dos atributos num\u00e9ricos","text":"<p>A fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh \u00e9 centrada em zero e gera sa\u00eddas em [-1, 1]. Portanto, \u00e9 essencial que as vari\u00e1veis num\u00e9ricas estejam padronizadas para:</p> <ul> <li>m\u00e9dia = 0  </li> <li>desvio padr\u00e3o = 1  </li> </ul> <p>Isso melhora a estabilidade do treinamento e acelera a converg\u00eancia. Para isso, usei <code>StandardScaler</code> da biblioteca <code>scikit-learn</code>.</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nnum_cols_final = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"Num\"]\n\nscaler = StandardScaler()\ndf[num_cols_final] = scaler.fit_transform(df[num_cols_final])\n\ndisplay(df[num_cols_final].head())\n</code></pre>"},{"location":"data/exercicio3/exercicio3/#etapa-6-visualizacao-do-impacto-do-pre-processamento","title":"Etapa 6 \u2014 Visualiza\u00e7\u00e3o do impacto do pr\u00e9-processamento","text":"<p>Para evidenciar a transforma\u00e7\u00e3o feita, comparei a distribui\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas antes e depois do escalonamento:</p> <ul> <li>Idade (<code>Age</code>): mostra como foi centralizada em torno de zero e ajustada em escala.  </li> <li>Gasto em <code>FoodCourt</code>: mostra como valores muito altos foram reduzidos para uma escala padr\u00e3o, sem alterar a forma da distribui\u00e7\u00e3o.</li> </ul> <p>Isso comprova que os dados foram preparados corretamente para uso em uma rede neural com <code>tanh</code>.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Dados originais para compara\u00e7\u00e3o\ndf_raw = pd.read_csv(\"data/train.csv\")\n\nfig, axes = plt.subplots(2, 2, figsize=(10,6))\n\n# Antes\naxes[0,0].hist(df_raw[\"Age\"].dropna(), bins=30, color=\"skyblue\")\naxes[0,0].set_title(\"Age (Antes)\")\n\naxes[0,1].hist(df_raw[\"FoodCourt\"].dropna(), bins=30, color=\"salmon\")\naxes[0,1].set_title(\"FoodCourt (Antes)\")\n\n# Depois\naxes[1,0].hist(df[\"Age\"], bins=30, color=\"skyblue\")\naxes[1,0].set_title(\"Age (Depois)\")\n\naxes[1,1].hist(df[\"FoodCourt\"], bins=30, color=\"salmon\")\naxes[1,1].set_title(\"FoodCourt (Depois)\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p> </p> <p>Compara\u00e7\u00e3o pr\u00e9-p\u00f3s processamento</p>"},{"location":"data/exercicio3/exercicio3/#importancia-do-pre-processamento","title":"Import\u00e2ncia do Pr\u00e9-processamento","text":""},{"location":"data/exercicio3/exercicio3/#variavel-age","title":"Vari\u00e1vel Age","text":"<ul> <li>Antes: a idade estava distribu\u00edda entre 0 e 80 anos, com concentra\u00e7\u00e3o principal entre 20 e 40.  </li> <li>Depois: ap\u00f3s a padroniza\u00e7\u00e3o, os valores passaram a variar aproximadamente entre -2 e +3, centralizados em torno de zero.  </li> <li>Impacto: a transforma\u00e7\u00e3o mant\u00e9m a forma da distribui\u00e7\u00e3o, mas coloca a vari\u00e1vel em uma escala adequada para a fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>, que trabalha melhor com valores pr\u00f3ximos de zero.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#variavel-foodcourt","title":"Vari\u00e1vel FoodCourt","text":"<ul> <li>Antes: os gastos apresentavam valores de 0 at\u00e9 quase 30.000, mas altamente concentrados em torno de zero.  </li> <li>Depois: ap\u00f3s normaliza\u00e7\u00e3o/padroniza\u00e7\u00e3o, os valores foram reduzidos para uma faixa pr\u00f3xima de 0 a 18.  </li> <li>Impacto: isso evita que essa vari\u00e1vel, por estar em uma escala muito maior que as demais, dominasse o processo de treinamento da rede neural.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#comparacao-geral","title":"Compara\u00e7\u00e3o Geral","text":"<ul> <li>O pr\u00e9-processamento preservou o padr\u00e3o das distribui\u00e7\u00f5es, mas ajustou suas escalas para torn\u00e1-las compar\u00e1veis.  </li> <li>Agora, cada vari\u00e1vel contribui de maneira mais equilibrada para o aprendizado, sem que uma tenha peso desproporcional apenas por conta da unidade de medida.  </li> <li>Al\u00e9m disso, como os valores foram centralizados (no caso de Age) e reduzidos (no caso de FoodCourt), a rede neural ter\u00e1 treinamento mais est\u00e1vel e eficiente, evitando satura\u00e7\u00e3o da fun\u00e7\u00e3o de ativa\u00e7\u00e3o <code>tanh</code>.</li> </ul>"},{"location":"data/exercicio3/exercicio3/#conclusao","title":"Conclus\u00e3o","text":"<p>O pr\u00e9-processamento foi essencial para:</p> <ul> <li>Reduzir diferen\u00e7as de escala entre atributos,  </li> <li>Adaptar os dados para o intervalo adequado da fun\u00e7\u00e3o de ativa\u00e7\u00e3o,  </li> <li>Garantir que todos os atributos tenham relev\u00e2ncia equilibrada na classifica\u00e7\u00e3o.</li> </ul>"},{"location":"metricas/main/","title":"Metricas","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> <p></p> <p></p> <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"mlp/main/","title":"Main","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> </p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"mlp/exercicio1/exercicio1/","title":"Exerc\u00edcio 1: MLP","text":""},{"location":"mlp/exercicio1/exercicio1/#obejtivo-calculo-manual-de-um-mlp","title":"Obejtivo: Calculo manual de um MLP","text":"<p>Nesta atividade, ser\u00e1 implementado o c\u00e1lculo manual completo de um MLP, abordando as seguintes etapas durante esse processo:</p> <ul> <li><code>Forward Pass</code>: Compreender como os dados fluem atrav\u00e9s da rede, desde a entrada at\u00e9 a sa\u00edda final</li> <li><code>Calculo da fun\u00e7\u00e3o de perda (MSE)</code> </li> <li><code>Backward Pass</code>: Assimilar o algoritmo de backpropagation e como os gradientes s\u00e3o calculados e propagados</li> <li><code>Visualizar a Atualiza\u00e7\u00e3o de Par\u00e2metros</code>: Observar como os pesos e bias s\u00e3o ajustados atrav\u00e9s do gradient descent</li> </ul>"},{"location":"mlp/exercicio1/exercicio1/#arquitetura-da-rede","title":"Arquitetura da Rede:","text":"<ul> <li>Entrada: 2 features</li> <li>Camada oculta: 2 neur\u00f4nios com ativa\u00e7\u00e3o tanh</li> <li>Sa\u00edda: 1 neur\u00f4nio com ativa\u00e7\u00e3o tanh</li> <li>Fun\u00e7\u00e3o de perda: Mean Squared Error (MSE)</li> </ul>"},{"location":"mlp/exercicio1/exercicio1/#1-configuracao-inicial-e-dados","title":"1. Configura\u00e7\u00e3o Inicial e Dados","text":"<p>Primeiro, vamos configurar todos os dados que ser\u00e3o utilizados no MLP:</p> <p>Entrada </p> \\[ x = \\begin{bmatrix} 0.5 &amp; -0.2 \\end{bmatrix} \\] <p>Sa\u00edda alvo </p> \\[ y = 1.0 \\] <p>Pesos e Bias da 1\u00aa camada (W^(1), b^(1)) </p> \\[ W^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\] \\[ b^{(1)} =  \\begin{bmatrix} 0.1 &amp; -0.2 \\end{bmatrix} \\] <p>Pesos e Bias da 2\u00aa camada (W^(2), b^(2)) </p> \\[ W^{(2)} =  \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} \\] \\[ b^{(2)} = 0.2 \\] <p>Taxa de aprendizado </p> \\[ \\eta = 0.1 \\] <pre><code>import numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nx = np.array([0.5, -0.2])  # entrada\ny = 1.0                    # saida alvo\n\n# Pesos e bias da camada oculta\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]])\nb1 = np.array([0.1, -0.2])\n\n# Pesos e bias da camada de saida\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\n\neta = 0.1\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#2-forward-pass-passe-adiante","title":"2. Forward Pass (Passe Adiante)","text":"<p>Agora vamos implementar o passo a passo dos c\u00e1lcuos matematicos para a execu\u00e7\u00e3o do Forward Pass:</p>"},{"location":"mlp/exercicio1/exercicio1/#passo-21-calcular-pre-ativacoes-da-camada-oculta","title":"Passo 2.1: Calcular pr\u00e9-ativa\u00e7\u00f5es da camada oculta","text":"\\[\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}\\] <p>Substituindo os valores:</p> \\[ z^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} \\] <p>C\u00e1lculo:</p> \\[ z^{(1)} = \\begin{bmatrix} (0.3 \\cdot 0.5) + (-0.1 \\cdot -0.2) + 0.1 \\\\ (0.2 \\cdot 0.5) + (0.4 \\cdot -0.2) - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.27 \\\\ -0.18 \\end{bmatrix} \\] <pre><code>z1 = W1 @ x + b1\n\nprint(f\"z^(1) = {z1}\")\n</code></pre> <pre><code>z^(1) = [ 0.27 -0.18]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-22-aplicar-funcao-de-ativacao-tanh-na-camada-oculta","title":"Passo 2.2: Aplicar fun\u00e7\u00e3o de ativa\u00e7\u00e3o tanh na camada oculta","text":"\\[\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)})\\] <p>Substituindo os valores:</p> \\[ h^{(1)} = \\tanh \\begin{bmatrix} 0.27 \\\\ -0.18 \\end{bmatrix} \\] <p>Calculando elemento a elemento:</p> \\[ h^{(1)} = \\begin{bmatrix} \\tanh(0.27) \\\\ \\tanh(-0.18) \\end{bmatrix} = \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} \\] <pre><code>h1 = np.tanh(z1)\nprint(f\"h^(1) = {h1}\")\n</code></pre> <pre><code>h^(1) = [ 0.2636 -0.1781]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-23-calcular-pre-ativacao-da-camada-de-saida","title":"Passo 2.3: Calcular pr\u00e9-ativa\u00e7\u00e3o da camada de sa\u00edda","text":"\\[u^{(2)} = \\mathbf{W}^{(2)}\\mathbf{h}^{(1)} + b^{(2)}\\] <p>Substituindo os valores:</p> \\[ u^{(2)} = \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix} + 0.2 \\] <p>C\u00e1lculo:</p> \\[ u^{(2)} = (0.5 \\cdot 0.2636) + (-0.3 \\cdot -0.1781) + 0.2 \\] \\[ u^{(2)} \\approx 0.3852 \\] <pre><code>u2 = W2 @ h1 + b2\nprint(f\"u^(2) = {u2}\")\n</code></pre> <pre><code>u^(2) = 0.38523667817130075\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-24-calcular-saida-final","title":"Passo 2.4: Calcular sa\u00edda final","text":"\\[\\hat{y} = \\tanh(u^{(2)})\\] <p>Substituindo o valor calculado:</p> \\[ \\hat{y} = \\tanh(0.3852) \\] <p>Resultado:</p> \\[ \\hat{y} \\approx 0.3672 \\] <pre><code>y_hat = np.tanh(u2)\nprint(f\"\u0177 = {y_hat}\")\n</code></pre> <pre><code>\u0177 = 0.36724656264510797\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#3-calculo-da-perda-loss-calculation","title":"3. C\u00e1lculo da Perda (Loss Calculation)","text":"<p>Agora vamos calcular a fun\u00e7\u00e3o de perda Mean Squared Error (MSE):</p> \\[L = \\frac{1}{N}(y - \\hat{y})^2\\] <p>Como temos apenas uma amostra (N=1), a f\u00f3rmula se simplifica para:</p> \\[L = (y - \\hat{y})^2\\] <p>Substituindo os valores:</p> \\[ L = (1.0 - 0.3672)^2 \\] \\[ L = (0.6328)^2 \\] <p>Resultado:</p> \\[ L \\approx 0.4004 \\] <pre><code>L = (y - y_hat)**2\nprint(f\"L = {L}\")\n</code></pre> <pre><code>L = 0.4003769124844312\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#4-backward-pass-backpropagation","title":"4. Backward Pass (Backpropagation)","text":"<p>Agora vamos implementar o backward pass para calcular todos os gradientes. Come\u00e7amos pela derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda:</p>"},{"location":"mlp/exercicio1/exercicio1/#passo-41-gradiente-da-perda-em-relacao-a-saida","title":"Passo 4.1: Gradiente da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda","text":"\\[\\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y)\\] <p>Note que usamos a derivada de \\((y - \\hat{y})^2 = (\\hat{y} - y)^2\\), que \u00e9 \\(2(\\hat{y} - y)\\)</p> <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}} = 2(0.3672 - 1.0) \\] \\[ \\frac{\\partial L}{\\partial \\hat{y}} = 2(-0.6328) \\] <p>Resultado:</p> \\[ \\frac{\\partial L}{\\partial \\hat{y}} \\approx -1.2655 \\] <pre><code>def tanh_dt(u):\n    \"\"\"Deriva de tanh: d/du tanh(u) = 1 - tanh\u00b2(u)\"\"\"\n    return 1 - np.tanh(u)**2\n\ndL_dy_hat = 2 * (y_hat - y)\nprint(f\"\u2202L/\u2202\u0177 = {dL_dy_hat}\")\n</code></pre> <pre><code>\u2202L/\u2202\u0177 = -1.265506874709784\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-42-gradiente-em-relacao-a-pre-ativacao-da-saida","title":"Passo 4.2: Gradiente em rela\u00e7\u00e3o \u00e0 pr\u00e9-ativa\u00e7\u00e3o da sa\u00edda","text":"\\[\\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{d}{du^{(2)}}\\tanh(u^{(2)}) = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot (1 - \\tanh^2(u^{(2)}))\\] <p>Sabemos que:</p> \\[ \\tanh'(z) = 1 - \\tanh^2(z) \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial u^{(2)}} = -1.2655 \\cdot \\left( 1 - \\tanh^2(0.3852) \\right) \\] \\[ \\frac{\\partial L}{\\partial u^{(2)}} = -1.2655 \\cdot (1 - 0.1349) \\] \\[ \\frac{\\partial L}{\\partial u^{(2)}} = -1.2655 \\cdot 0.8651 \\] <p>Resultado:</p> \\[ \\frac{\\partial L}{\\partial u^{(2)}} \\approx -1.0948 \\] <pre><code>dL_du2 = dL_dy_hat * tanh_dt(u2)\nprint(f\"\u2202L/\u2202u^(2) = {dL_du2}\")\n</code></pre> <pre><code>\u2202L/\u2202u^(2) = -1.0948279147135995\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-43-gradientes-para-a-camada-de-saida","title":"Passo 4.3: Gradientes para a camada de sa\u00edda","text":"<p>Agora calculamos os gradientes para os pesos e bias da camada de sa\u00edda:</p>"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-aos-pesos-w2","title":"Gradiente em rela\u00e7\u00e3o aos pesos \\(W^{(2)}\\)","text":"\\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot h^{(1)} \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial W^{(2)}} = -1.0948 \\cdot  \\begin{bmatrix} 0.2636 &amp; -0.1781 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial W^{(2)}} = \\begin{bmatrix} -0.2886 &amp; 0.1950 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-ao-vies-b2","title":"Gradiente em rela\u00e7\u00e3o ao vi\u00e9s \\(b^{(2)}\\)","text":"\\[ \\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\] <p>Substituindo o valor:</p> \\[ \\frac{\\partial L}{\\partial b^{(2)}} = -1.0948 \\] <pre><code>dL_dW2 = dL_du2 * h1  # Gradiente para W^(2)\ndL_db2 = dL_du2       # Gradiente para b^(2)\n\nprint(f\"\u2202L/\u2202W^(2) = {dL_dW2}\")\nprint(f\"\u2202L/\u2202b^(2) = {dL_db2}\")\n</code></pre> <pre><code>\u2202L/\u2202W^(2) = [-0.2886  0.195 ]\n\u2202L/\u2202b^(2) = -1.0948279147135995\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-44-propagacao-para-a-camada-oculta","title":"Passo 4.4: Propaga\u00e7\u00e3o para a camada oculta","text":"<p>Agora precisamos propagar o erro de volta para a camada oculta:</p> \\[\\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = (\\mathbf{W}^{(2)})^T \\cdot \\frac{\\partial L}{\\partial u^{(2)}}\\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial h^{(1)}} = \\begin{bmatrix} 0.5 \\\\ -0.3 \\end{bmatrix} \\cdot (-1.0948) \\] \\[ \\frac{\\partial L}{\\partial h^{(1)}} = \\begin{bmatrix} -0.5474 \\\\ 0.3284 \\end{bmatrix} \\] <pre><code>dL_dh1 = W2 * dL_du2\nprint(f\"\u2202L/\u2202h^(1) = {dL_dh1}\")\n</code></pre> <pre><code>\u2202L/\u2202h^(1) = [-0.5474  0.3284]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-45-gradientes-para-a-camada-oculta","title":"Passo 4.5: Gradientes para a camada oculta","text":"<p>Agora calculamos os gradientes em rela\u00e7\u00e3o \u00e0s pr\u00e9-ativa\u00e7\u00f5es da camada oculta:</p> \\[\\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\odot \\tanh'(\\mathbf{z}^{(1)})\\] <p>onde \\(\\odot\\) representa o produto elemento a elemento (Hadamard product).</p> <p>Sabemos que:</p> \\[ \\tanh'(z) = 1 - \\tanh^2(z) \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix} -0.5474 &amp; 0.3284 \\end{bmatrix} \\odot \\left( 1 - \\tanh^2 \\begin{bmatrix} 0.27 &amp; -0.18 \\end{bmatrix} \\right) \\] \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix} -0.5474 &amp; 0.3284 \\end{bmatrix} \\odot \\begin{bmatrix} 0.9305 &amp; 0.9683 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial z^{(1)}} = \\begin{bmatrix} -0.5094 &amp; 0.3180 \\end{bmatrix} \\] <pre><code>dL_dz1 = dL_dh1 * tanh_dt(z1)\nprint(f\"\u2202L/\u2202z^(1) = {dL_dz1}\")\n</code></pre> <pre><code>\u2202L/\u2202z^(1) = [-0.5094  0.318 ]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#passo-46-gradientes-finais-para-pesos-e-bias-da-camada-oculta","title":"Passo 4.6: Gradientes finais para pesos e bias da camada oculta","text":"<p>Finalmente, calculamos os gradientes para os pesos e bias da camada oculta:</p>"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-aos-pesos-w1","title":"Gradiente em rela\u00e7\u00e3o aos pesos \\(W^{(1)}\\)","text":"\\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\otimes x^T \\] <p>Substituindo os valores:</p> \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\begin{bmatrix} -0.5094 \\\\ 0.3180 \\end{bmatrix} \\otimes \\begin{bmatrix} 0.5 &amp; -0.2 \\end{bmatrix} \\] \\[ \\frac{\\partial L}{\\partial W^{(1)}} = \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#gradiente-em-relacao-ao-vies-b1","title":"Gradiente em rela\u00e7\u00e3o ao vi\u00e9s \\(b^{(1)}\\)","text":"\\[ \\frac{\\partial L}{\\partial b^{(1)}} = \\frac{\\partial L}{\\partial z^{(1)}} \\] \\[ \\frac{\\partial L}{\\partial b^{(1)}} = \\begin{bmatrix} -0.5094 &amp; 0.3180 \\end{bmatrix} \\] <pre><code>dL_dW1 = np.outer(dL_dz1, x)  \ndL_db1 = dL_dz1               \n\nprint(f\"\u2202L/\u2202W^(1) = \\n{dL_dW1}\")\nprint(f\"\u2202L/\u2202b^(1) = {dL_db1}\")\n</code></pre> <pre><code>\u2202L/\u2202W^(1) = \n[[-0.2547  0.1019]\n [ 0.159  -0.0636]]\n\n \u2202L/\u2202b^(1) = [-0.5094  0.318 ]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#5-atualizacao-dos-parametros","title":"5. Atualiza\u00e7\u00e3o dos Par\u00e2metros","text":"<p>Agora aplicamos o algoritmo de gradient descent para atualizar todos os par\u00e2metros usando a taxa de aprendizagem \u03b7 = 0.1:</p> \\[\\theta_{novo} = \\theta_{antigo} - \\eta \\cdot \\frac{\\partial L}{\\partial \\theta}\\] <p>Par\u00e2metros antes da atualiza\u00e7\u00e3o</p> \\[ W^{(2)} =  \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix}, \\quad b^{(2)} = 0.2 \\] \\[ W^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; \\phantom{-}0.4 \\end{bmatrix}, \\quad b^{(1)} = \\begin{bmatrix} 0.1 &amp; -0.2 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-w2","title":"Atualiza\u00e7\u00e3o de \\(W^{(2)}\\)","text":"\\[ W^{(2)}_{\\text{novo}} = W^{(2)} - \\eta \\,\\frac{\\partial L}{\\partial W^{(2)}} \\] \\[ W^{(2)}_{\\text{novo}} = \\begin{bmatrix} 0.5 &amp; -0.3 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.2886 &amp; 0.1950 \\end{bmatrix} = \\begin{bmatrix} 0.5289 &amp; -0.3195 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-b2","title":"Atualiza\u00e7\u00e3o de \\(b^{(2)}\\)","text":"\\[ b^{(2)}_{\\text{novo}} = b^{(2)} - \\eta \\,\\frac{\\partial L}{\\partial b^{(2)}} = 0.2 - 0.1 \\cdot (-1.0948279147135995) = 0.30948279147136 \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-w1","title":"Atualiza\u00e7\u00e3o de \\(W^{(1)}\\)","text":"\\[ W^{(1)}_{\\text{novo}} = W^{(1)} - \\eta \\,\\frac{\\partial L}{\\partial W^{(1)}} \\] \\[ W^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ \\phantom{-}0.1590 &amp; -0.0636 \\end{bmatrix} = \\begin{bmatrix} 0.3255 &amp; -0.1102 \\\\ 0.1841 &amp; \\phantom{-}0.4064 \\end{bmatrix} \\]"},{"location":"mlp/exercicio1/exercicio1/#atualizacao-de-b1","title":"Atualiza\u00e7\u00e3o de \\(b^{(1)}\\)","text":"\\[ b^{(1)}_{\\text{novo}} = b^{(1)} - \\eta \\,\\frac{\\partial L}{\\partial b^{(1)}} \\] \\[ b^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.1 &amp; -0.2 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.5094 &amp; 0.3180 \\end{bmatrix} = \\begin{bmatrix} 0.1509 &amp; -0.2318 \\end{bmatrix} \\] <pre><code>W2_new = W2 - eta * dL_dW2\nb2_new = b2 - eta * dL_db2\nW1_new = W1 - eta * dL_dW1\nb1_new = b1 - eta * dL_db1\n\n\nprint(f\"W^(2)_novo = {W2_new}\")\nprint(f\"b^(2)_novo = {b2_new}\")\nprint(f\"W^(1)_novo = \\n{W1_new}\")\nprint(f\"b^(1)_novo = {b1_new}\")\n</code></pre> <pre><code>W^(2)_novo = [ 0.5289 -0.3195]\nb^(2)_novo = 0.30948279147136\n\nW^(1)_novo = \n[[ 0.3255 -0.1102]\n [ 0.1841  0.4064]]\n\nb^(1)_novo = [ 0.1509 -0.2318]\n</code></pre>"},{"location":"mlp/exercicio1/exercicio1/#6-verificacao-dos-resultados","title":"6. Verifica\u00e7\u00e3o dos Resultados","text":"<p>Vamos verificar se a atualiza\u00e7\u00e3o dos par\u00e2metros realmente melhorou o desempenho da rede calculando um novo forward pass com os par\u00e2metros atualizados:</p> <p>Ap\u00f3s atualizar os par\u00e2metros, realizamos um novo forward pass:</p> \\[ z^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.3357 &amp; -0.221 \\end{bmatrix} \\] \\[ h^{(1)}_{\\text{novo}} = \\begin{bmatrix} 0.3236 &amp; -0.2175 \\end{bmatrix} \\] \\[ u^{(2)}_{\\text{novo}} = 0.5501 \\] \\[ \\hat{y}_{\\text{novo}} = 0.5006 \\] \\[ L_{\\text{novo}} = 0.2494 \\] <pre><code>def forward_pass(x, W1, b1, W2, b2):\n    z1 = W1 @ x + b1\n    h1 = np.tanh(z1)\n    u2 = W2 @ h1 + b2\n    y_hat = np.tanh(u2)\n    return z1, h1, u2, y_hat\n\nz1_new, h1_new, u2_new, y_hat_new = forward_pass(x, W1_new, b1_new, W2_new, b2_new)\nL_new = (y - y_hat_new)**2\n\nprint(f\"z^(1)_novo = {z1_new}\")\nprint(f\"h^(1)_novo = {h1_new}\")\nprint(f\"u^(2)_novo = {u2_new}\")\nprint(f\"\u0177_novo = {y_hat_new}\")\nprint(f\"L_novo = {L_new}\")\n</code></pre> <pre><code>z^(1)_novo = [ 0.3357 -0.221 ]\nh^(1)_novo = [ 0.3236 -0.2175]\nu^(2)_novo = 0.5501335506731257\n\u0177_novo = 0.5006202979935049\nL_novo = 0.2493800867760958\n</code></pre> M\u00e9trica Antes Depois Melhoria Sa\u00edda \\(\\hat{y}\\) 0.3672 0.5006 Sim Perda \\(L\\) 0.4004 0.2494 Sim Erro absoluto 0.6328 0.4994 Sim <p>Houve uma redu\u00e7\u00e3o da perda de 37.71%, confirmando que a atualiza\u00e7\u00e3o dos par\u00e2metros aproximou a sa\u00edda predita do valor alvo.</p>"},{"location":"mlp/exercicio2/exercicio2/","title":"Exerc\u00edcio 2: MLP","text":""},{"location":"mlp/exercicio2/exercicio2/#objetivo","title":"Objetivo","text":"<p>Implementar um Multi-Layer Perceptron (MLP) do zero para resolver um problema de classifica\u00e7\u00e3o bin\u00e1ria, utilizando apenas a biblioteca NumPy para c\u00e1lculos matem\u00e1ticos. Este exerc\u00edcio demonstra na pr\u00e1tica como construir, treinar e avaliar uma rede neural artificial sem o uso de frameworks de deep learning.</p>"},{"location":"mlp/exercicio2/exercicio2/#especificacoes-tecnicas","title":"Especifica\u00e7\u00f5es T\u00e9cnicas:","text":"<ul> <li>Dataset: 1000 amostras sint\u00e9ticas com 2 features</li> <li>Classes: 2 (classifica\u00e7\u00e3o bin\u00e1ria)</li> <li>Arquitetura: 2 \u2192 8 \u2192 1 neur\u00f4nios</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o: tanh (camada oculta) + sigmoid (sa\u00edda)</li> <li>Fun\u00e7\u00e3o de Perda: Binary Cross-Entropy</li> <li>Otimizador: Gradient Descent</li> </ul>"},{"location":"mlp/exercicio2/exercicio2/#1-configuracao-inicial-e-importacao-de-bibliotecas","title":"1. Configura\u00e7\u00e3o Inicial e Importa\u00e7\u00e3o de Bibliotecas","text":"<p>Antes de come\u00e7ar a implementa\u00e7\u00e3o, precisamos importar as bibliotecas necess\u00e1rias e configurar o ambiente de desenvolvimento. Vamos usar NumPy para opera\u00e7\u00f5es matem\u00e1ticas, Matplotlib/Seaborn para visualiza\u00e7\u00f5es e Scikit-learn apenas para gera\u00e7\u00e3o de dados sint\u00e9ticos e m\u00e9tricas de avalia\u00e7\u00e3o.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport seaborn as sns\n\nnp.set_printoptions(precision=4, suppress=True)\nplt.style.use('default')\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#2-geracao-e-preparacao-dos-dados","title":"2. Gera\u00e7\u00e3o e Prepara\u00e7\u00e3o dos Dados","text":""},{"location":"mlp/exercicio2/exercicio2/#21-criacao-do-dataset-sintetico","title":"2.1 Cria\u00e7\u00e3o do Dataset Sint\u00e9tico","text":"<p>Vamos criar um conjunto de dados artificiais para classifica\u00e7\u00e3o bin\u00e1ria. O Scikit-learn possui uma fun\u00e7\u00e3o espec\u00edfica para isso que nos permite controlar caracter\u00edsticas como n\u00famero de amostras, features e complexidade da separa\u00e7\u00e3o entre classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Gera 1000 amostras com 2 features cada</li> <li>Cria 2 classes balanceadas (500 amostras cada)</li> <li>Configura 2 clusters por classe para tornar o problema mais interessante</li> <li>Adiciona um pequeno ru\u00eddo (1%) para simular dados reais</li> </ul> <pre><code>n_samples = 1000\nn_features = 2\nn_clusters_per_class = 2\nn_informative = 2\nn_redundant = 0\nrandom_state = 42\n\nX, y = make_classification(\n    n_samples=n_samples,\n    n_features=n_features,\n    n_informative=n_informative,\n    n_redundant=n_redundant,\n    n_clusters_per_class=n_clusters_per_class,\n    random_state=random_state,\n    flip_y=0.01\n)\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#22-visualizacao-dos-dados","title":"2.2 Visualiza\u00e7\u00e3o dos Dados","text":"<p>\u00c9 fundamental visualizar os dados antes de treinar qualquer modelo. Isso nos ajuda a entender a distribui\u00e7\u00e3o das classes, identificar poss\u00edveis padr\u00f5es e avaliar a complexidade do problema de classifica\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria um scatter plot das duas classes</li> <li>Mostra a distribui\u00e7\u00e3o espacial dos pontos no espa\u00e7o 2D</li> </ul> <pre><code>plt.figure(figsize=(10, 8))\n\ncolors = ['red', 'blue']\nlabels = ['Classe 0', 'Classe 1']\n\nfor i in range(2):\n    mask = y == i\n    plt.scatter(X[mask, 0], X[mask, 1], \n               c=colors[i], label=labels[i], \n               alpha=0.7, s=50)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Conjunto de Dados Sint\u00e9ticos para Classifica\u00e7\u00e3o Bin\u00e1ria')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axis('equal')\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio2/exercicio2/#23-divisao-e-normalizacao-dos-dados","title":"2.3 Divis\u00e3o e Normaliza\u00e7\u00e3o dos Dados","text":"<p>Antes de treinar o modelo, precisamos dividir os dados em conjuntos de treino e teste, al\u00e9m de normalizar os features. A normaliza\u00e7\u00e3o \u00e9 crucial para MLPs, pois garante que todas as features tenham a mesma escala, evitando que features com valores maiores dominem o processo de aprendizagem.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Divide dados em 80% treino e 20% teste</li> <li>Mant\u00e9m a propor\u00e7\u00e3o das classes em ambos os conjuntos </li> <li>Normaliza os dados usando Z-score (m\u00e9dia=0, desvio=1)</li> <li>Aplica a mesma normaliza\u00e7\u00e3o do treino no conjunto de teste</li> </ul> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2, \n    random_state=42, \n    stratify=y\n)\n\n\n# normalizacao\nX_train_mean = X_train.mean(axis=0)\nX_train_std = X_train.std(axis=0)\n\nX_train_norm = (X_train - X_train_mean) / X_train_std\nX_test_norm = (X_test - X_train_mean) / X_train_std\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#3-implementacao-da-classe-mlp","title":"3. Implementa\u00e7\u00e3o da Classe MLP","text":""},{"location":"mlp/exercicio2/exercicio2/#31-estrutura-principal-e-inicializacao","title":"3.1 Estrutura Principal e Inicializa\u00e7\u00e3o","text":"<p>Vamos criar uma classe que encapsula toda a funcionalidade do nosso MLP. A inicializa\u00e7\u00e3o \u00e9 um passo cr\u00edtico, pois determina os valores iniciais dos pesos e bias que a rede usar\u00e1 para come\u00e7ar o aprendizado.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Inicializa pesos com valores pequenos e aleat\u00f3rios</li> <li>Inicializa bias com zeros</li> </ul> <pre><code>class MLP:\n\n    def __init__(self, learning_rate=0.05):\n        self.learning_rate = learning_rate\n\n        hidden_size = 12\n\n        self.W1 = np.random.randn(hidden_size, 2) * np.sqrt(2.0 / 2)\n        self.b1 = np.zeros((hidden_size, 1))\n        self.W2 = np.random.randn(1, hidden_size) * np.sqrt(2.0 / hidden_size)\n        self.b2 = np.zeros((1, 1))\n\n        self.loss_history = []\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#32-funcoes-de-ativacao-e-suas-derivadas","title":"3.2 Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o e suas Derivadas","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o fundamentais para permitir que a rede neural aprenda padr\u00f5es n\u00e3o-lineares. Implementamos tanh para a camada oculta (permite valores negativos e positivos) e sigmoid para a sa\u00edda (valores entre 0 e 1, ideal para probabilidades).</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa a fun\u00e7\u00e3o tanh e sua derivada</li> <li>Implementa a fun\u00e7\u00e3o sigmoid com prote\u00e7\u00e3o contra overflow</li> <li>As derivadas s\u00e3o necess\u00e1rias para o algoritmo de backpropagation</li> </ul> <pre><code>def tanh(self, z):\n    return np.tanh(z)\n\ndef tanh_derivative(self, z):\n    \"\"\"Derivada da tanh: d/dz tanh(z) = 1 - tanh\u00b2(z)\"\"\"\n    return 1 - np.tanh(z)**2\n\ndef sigmoid(self, z):\n    \"\"\"Fun\u00e7\u00e3o sigmoid: sa\u00edda entre 0 e 1\"\"\"\n    z_clipped = np.clip(z, -500, 500)\n    return 1 / (1 + np.exp(-z_clipped))\n\nMLP.tanh = tanh\nMLP.tanh_derivative = tanh_derivative\nMLP.sigmoid = sigmoid\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#33-forward-pass-propagacao-adiante","title":"3.3 Forward Pass (Propaga\u00e7\u00e3o Adiante)","text":"<p>O forward pass \u00e9 respons\u00e1vel por processar os dados de entrada atrav\u00e9s de todas as camadas da rede at\u00e9 produzir a sa\u00edda final. Este \u00e9 o processo de \"predi\u00e7\u00e3o\" da rede neural.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Recebe dados de entrada e os propaga atrav\u00e9s da rede</li> <li>Aplica transforma\u00e7\u00f5es lineares (multiplica\u00e7\u00e3o matricial + bias)</li> <li>Aplica fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o-lineares</li> <li>Armazena valores intermedi\u00e1rios (cache) para usar no backpropagation</li> <li>Retorna tanto a sa\u00edda final quanto o cache</li> </ul> <pre><code>def forward(self, X):\n    \"\"\"\n    Forward pass: propaga dados atrav\u00e9s da rede.\n\n    Args:\n        X: dados de entrada (n_samples, n_features)\n\n    Returns:\n        A2: sa\u00edda da rede (probabilidades)\n        cache: valores intermedi\u00e1rios para backprop\n    \"\"\"\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    A0 = X.T\n\n    # camada oculta: transforma\u00e7\u00e3o + ativa\u00e7\u00e3o\n    Z1 = self.W1 @ A0 + self.b1  \n    A1 = self.tanh(Z1)           \n\n    # camada de sa\u00edda: transforma\u00e7\u00e3o + ativa\u00e7\u00e3o\n    Z2 = self.W2 @ A1 + self.b2  \n    A2 = self.sigmoid(Z2)        \n\n    cache = {\n        'A0': A0,  # Entrada\n        'Z1': Z1,  # Pr\u00e9-ativa\u00e7\u00e3o camada oculta\n        'A1': A1,  # Ativa\u00e7\u00e3o camada oculta\n        'Z2': Z2,  # Pr\u00e9-ativa\u00e7\u00e3o sa\u00edda\n        'A2': A2   # Ativa\u00e7\u00e3o sa\u00edda (probabilidade)\n    }\n\n    return A2, cache\n\ndef predict_proba(self, X):\n    \"\"\"Retorna probabilidades de classifica\u00e7\u00e3o\"\"\"\n    output, _ = self.forward(X)\n    return output.T\n\ndef predict(self, X):\n    \"\"\"Retorna predi\u00e7\u00f5es bin\u00e1rias (0 ou 1)\"\"\"\n    probabilities = self.predict_proba(X)\n    return (probabilities &gt; 0.5).astype(int).flatten()\n\nMLP.forward = forward\nMLP.predict_proba = predict_proba\nMLP.predict = predict\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#34-funcao-de-perda-e-backward-pass","title":"3.4 Fun\u00e7\u00e3o de Perda e Backward Pass","text":"<p>A fun\u00e7\u00e3o de perda mede qu\u00e3o distantes est\u00e3o nossas predi\u00e7\u00f5es dos valores reais. O backward pass (backpropagation) calcula como cada par\u00e2metro da rede deve ser ajustado para reduzir essa perda.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa Binary Cross-Entropy Loss</li> <li>Calcula gradientes de todos os par\u00e2metros usando a regra da cadeia</li> <li>Propaga o erro da sa\u00edda de volta para todas as camadas</li> <li>Retorna gradientes para atualiza\u00e7\u00e3o dos par\u00e2metros</li> </ul> <pre><code>def compute_loss(self, y_true, y_pred):\n    \"\"\"    \n    Args:\n        y_true: labels verdadeiros (0 ou 1)\n        y_pred: probabilidades preditas (0 a 1)\n\n    Returns:\n        loss: valor da perda (menor \u00e9 melhor)\n    \"\"\"\n    baixo = 1e-15\n    y_pred_clipped = np.clip(y_pred, baixo, 1 - baixo)\n\n    loss = -np.mean(y_true * np.log(y_pred_clipped) + \n                   (1 - y_true) * np.log(1 - y_pred_clipped))\n    return loss\n\ndef backward(self, X, y, cache):\n    \"\"\"    \n    Args:\n        X: dados de entrada\n        y: labels verdadeiros\n        cache: valores do forward pass\n\n    Returns:\n        gradientes de todos os par\u00e2metros\n    \"\"\"\n    m = X.shape[0]\n\n    A0, A1, A2 = cache['A0'], cache['A1'], cache['A2']\n    Z1 = cache['Z1']\n\n    y_reshaped = y.reshape(1, -1)\n    dZ2 = A2 - y_reshaped\n\n    dW2 = (1/m) * np.dot(dZ2, A1.T)  # dL/dW2\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # dL/db2\n\n    dA1 = np.dot(self.W2.T, dZ2)  # dL/dA1\n    dZ1 = dA1 * self.tanh_derivative(Z1)  # dL/dZ1\n\n    dW1 = (1/m) * np.dot(dZ1, A0.T)  # dL/dW1\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)  # dL/db1\n\n    return dW1, db1, dW2, db2\n\nMLP.compute_loss = compute_loss\nMLP.backward = backward\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#35-loop-de-treinamento-principal","title":"3.5 Loop de Treinamento Principal","text":"<p>O m\u00e9todo de treinamento coordena todo o processo de aprendizagem: executa forward passes, calcula perdas, executa backward passes e atualiza par\u00e2metros repetidamente at\u00e9 a rede convergir.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa m\u00faltiplas \u00e9pocas de treinamento</li> <li>Para cada \u00e9poca: forward \u2192 loss \u2192 backward \u2192 update</li> <li>Atualiza par\u00e2metros usando Gradient Descent</li> <li>Monitora progresso (perda e acur\u00e1cia) durante treinamento</li> </ul> <pre><code>def fit(self, X_train, y_train, epochs=150, print_every=30):\n    \"\"\"    \n    Args:\n        X_train: dados de treinamento (n_samples, n_features)\n        y_train: labels de treinamento (n_samples,)\n        epochs: n\u00famero de \u00e9pocas de treinamento\n    \"\"\"\n    self.loss_history = []\n    self.accuracy_history = []\n\n    print(f\"------ Iniciando treino ------\")\n    print(f\"\u00c9pocas: {epochs}\")\n    print(f\"Amostras de treino: {X_train.shape[0]}\")\n    print(f\"Learning rate: {self.learning_rate}\")\n\n    for epoch in range(epochs):\n        # forward pass\n        output, cache = self.forward(X_train)\n        y_pred = output.flatten()\n\n        # perda\n        loss = self.compute_loss(y_train, y_pred)\n        self.loss_history.append(loss)\n\n        # acuracia\n        predictions = (y_pred &gt; 0.5).astype(int)\n        accuracy = np.mean(predictions == y_train)\n        self.accuracy_history.append(accuracy)\n\n        # backward pass\n        dW1, db1, dW2, db2 = self.backward(X_train, y_train, cache)\n\n        # atualiza os parametros\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n\n        if (epoch + 1) % print_every == 0:\n            print(f\"\u00c9poca {epoch+1:3d}/{epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n\n    print(f\"\\n ---------- fim do treino --------\")\n    print(f\"Loss final: {self.loss_history[-1]:.6f}\")\n    print(f\"Acur\u00e1cia final no treino: {self.accuracy_history[-1]:.4f}\")\n\nMLP.fit = fit\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#4-treinamento-do-modelo","title":"4. Treinamento do Modelo","text":""},{"location":"mlp/exercicio2/exercicio2/#41-instanciacao-e-configuracao","title":"4.1 Instancia\u00e7\u00e3o e Configura\u00e7\u00e3o","text":"<p>Agora vamos criar uma inst\u00e2ncia da nossa classe MLP e configurar os hiperpar\u00e2metros de treinamento. A escolha do learning rate \u00e9 crucial: muito alto pode causar instabilidade, muito baixo pode tornar o treinamento lento.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria uma inst\u00e2ncia do MLP</li> <li>Inicializa todos os pesos e bias automaticamente</li> <li>Prepara a rede para receber dados de treinamento</li> </ul> <pre><code>model = MLP(learning_rate=0.05)\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#42-execucao-do-treinamento","title":"4.2 Execu\u00e7\u00e3o do Treinamento","text":"<p>Este \u00e9 o momento principal onde nossa rede neural aprende os padr\u00f5es dos dados. O processo pode levar alguns segundos e voc\u00ea ver\u00e1 o progresso da perda e acur\u00e1cia sendo impresso.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa 150 \u00e9pocas de treinamento</li> <li>Mostra progresso a cada 30 \u00e9pocas</li> <li>A perda deve diminuir e a acur\u00e1cia deve aumentar ao longo do tempo</li> <li>Salva hist\u00f3rico para posterior visualiza\u00e7\u00e3o</li> </ul> <pre><code>model.fit(X_train_norm, y_train, epochs=150, print_every=30)\n\nprint(f\"Perda final: {model.loss_history[-1]:.6f}\")\nprint(f\"Acur\u00e1cia final: {model.accuracy_history[-1]:.4f}\")\n</code></pre> <pre><code>------ Iniciando treino ------\n\u00c9pocas: 150\nAmostras de treino: 800\nLearning rate: 0.05\n\u00c9poca  30/150 - Loss: 0.4429 - Acc: 0.8662\n\u00c9poca  60/150 - Loss: 0.3767 - Acc: 0.8662\n\u00c9poca  90/150 - Loss: 0.3540 - Acc: 0.8700\n\u00c9poca 120/150 - Loss: 0.3426 - Acc: 0.8712\n\u00c9poca 150/150 - Loss: 0.3358 - Acc: 0.8725\n\n ---------- fim do treino --------\nLoss final: 0.335833\nAcur\u00e1cia final no treino: 0.8725\nPerda final: 0.335833\nAcur\u00e1cia final: 0.8725\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#5-avaliacao-e-analise-dos-resultados","title":"5. Avalia\u00e7\u00e3o e An\u00e1lise dos Resultados","text":""},{"location":"mlp/exercicio2/exercicio2/#51-metricas-de-desempenho-no-conjunto-de-teste","title":"5.1 M\u00e9tricas de Desempenho no Conjunto de Teste","text":"<p>Ap\u00f3s o treinamento, precisamos avaliar como o modelo se comporta em dados que nunca viu antes (conjunto de teste). Isso nos d\u00e1 uma medida real da capacidade de generaliza\u00e7\u00e3o do modelo.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Faz predi\u00e7\u00f5es no conjunto de teste</li> <li>Calcula m\u00e9tricas abrangentes de classifica\u00e7\u00e3o</li> <li>Compara desempenho entre treino e teste</li> <li>Identifica poss\u00edvel overfitting ou underfitting</li> </ul> <pre><code>y_pred_test = model.predict(X_test_norm)\ny_pred_proba_test = model.predict_proba(X_test_norm).flatten()\n\naccuracy = accuracy_score(y_test, y_pred_test)\nprecision = precision_score(y_test, y_pred_test)\nrecall = recall_score(y_test, y_pred_test)\nf1 = f1_score(y_test, y_pred_test)\ntest_loss = model.compute_loss(y_test, y_pred_proba_test)\n\nprint(f\"Acur\u00e1cia:     {accuracy:.4f}\")\nprint(f\"Precis\u00e3o:     {precision:.4f}\")\nprint(f\"Recall:       {recall:.4f}\")\nprint(f\"F1-Score:     {f1:.4f}\")\nprint(f\"Loss (teste): {test_loss:.4f}\")\n\n# treino vs teste\ntrain_accuracy = model.accuracy_history[-1]\nprint(f\"Acur\u00e1cia Treino: {train_accuracy:.4f}\")\nprint(f\"Acur\u00e1cia Teste:  {accuracy:.4f}\")\nprint(f\"Diferen\u00e7a:       {abs(train_accuracy - accuracy):.4f}\")\n</code></pre> <p><pre><code>Acur\u00e1cia:     0.8700\nPrecis\u00e3o:     0.9022\nRecall:       0.8300\nF1-Score:     0.8646\nLoss (teste): 0.3496\nAcur\u00e1cia Treino: 0.8725\nAcur\u00e1cia Teste:  0.8700\nDiferen\u00e7a:       0.0025\n</code></pre> A diferen\u00e7a entre o treino e o teste foi bem pequena, o que mostra que o modelo esta bem generalizado</p>"},{"location":"mlp/exercicio2/exercicio2/#52-visualizacao-da-curva-de-treinamento","title":"5.2 Visualiza\u00e7\u00e3o da Curva de Treinamento","text":"<p>A visualiza\u00e7\u00e3o da evolu\u00e7\u00e3o da perda durante o treinamento nos ajuda a entender se o modelo convergiu adequadamente e se o learning rate foi apropriado.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Plota a curva de perda ao longo das \u00e9pocas</li> <li>Mostra se houve converg\u00eancia suave</li> <li>Permite identificar problemas como oscila\u00e7\u00f5es ou satura\u00e7\u00e3o</li> <li>Ajuda a determinar se mais \u00e9pocas seriam necess\u00e1rias</li> </ul> <pre><code>plt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(model.loss_history, linewidth=2, color='red', label='Loss')\nplt.title('Evolu\u00e7\u00e3o da Perda Durante o Treinamento')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(model.accuracy_history, linewidth=2, color='blue', label='Accuracy')\nplt.title('Evolu\u00e7\u00e3o da Acur\u00e1cia Durante o Treinamento')\nplt.xlabel('\u00c9poca')\nplt.ylabel('Acur\u00e1cia')\nplt.grid(True, alpha=0.3)\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio2/exercicio2/#53-matriz-de-confusao","title":"5.3 Matriz de Confus\u00e3o","text":"<p>A matriz de confus\u00e3o oferece uma vis\u00e3o detalhada de onde o modelo est\u00e1 acertando e errando, permitindo identificar se h\u00e1 vi\u00e9s para alguma classe espec\u00edfica.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria matriz de confus\u00e3o com visualiza\u00e7\u00e3o clara</li> <li>Mostra verdadeiros positivos, falsos positivos, etc.</li> <li>Calcula estat\u00edsticas detalhadas por classe</li> <li>Identifica padr\u00f5es de erro do modelo</li> </ul> <pre><code>cm = confusion_matrix(y_test, y_pred_test)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Classe 0', 'Classe 1'], \n            yticklabels=['Classe 0', 'Classe 1'],\n            cbar_kws={'label': 'N\u00famero de Amostras'})\nplt.title('Matriz de Confus\u00e3o - Conjunto de Teste')\nplt.ylabel('Classe Verdadeira')\nplt.xlabel('Classe Predita')\nplt.show()\n</code></pre> <p></p> <pre><code>Verdadeiros Negativos (TN): 91\nFalsos Positivos (FP):      9\nFalsos Negativos (FN):      17\nVerdadeiros Positivos (TP): 83\n</code></pre>"},{"location":"mlp/exercicio2/exercicio2/#54-visualizacao-da-fronteira-de-decisao","title":"5.4 Visualiza\u00e7\u00e3o da Fronteira de Decis\u00e3o","text":"<p>Uma das visualiza\u00e7\u00f5es mais importantes \u00e9 a fronteira de decis\u00e3o, que mostra como o modelo separa as classes no espa\u00e7o de features. Isso nos d\u00e1 uma intui\u00e7\u00e3o visual de como a rede neural \"pensa\".</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria um grid denso de pontos no espa\u00e7o de features</li> <li>Calcula a probabilidade de cada ponto pertencer \u00e0 classe 1</li> <li>Visualiza a fronteira de decis\u00e3o como um mapa de calor</li> <li>Sobrep\u00f5e os pontos reais do conjunto de teste</li> </ul> <pre><code>def plot_decision_boundary(model, X, y, title=\"Fronteira de Decis\u00e3o do MLP\"):\n    \"\"\"    \n    Args:\n        model: modelo treinado\n        X: dados para plotar\n        y: labels correspondentes\n    \"\"\"\n    h = 0.02 \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    grid_normalized = (grid_points - X_train_mean) / X_train_std\n\n    Z = model.predict_proba(grid_normalized)\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(12, 8))\n\n    contour = plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.colorbar(contour, label='Probabilidade Classe 1')\n\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, \n                         edgecolors='black', s=60, alpha=0.9)\n\n    plt.xlabel('Feature 1', fontsize=12)\n    plt.ylabel('Feature 2', fontsize=12)\n    plt.title(title, fontsize=14, fontweight='bold')\n\n    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)\n\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\nplot_decision_boundary(model, X_test, y_test, \n                      \"Fronteira de Decis\u00e3o do MLP - Conjunto de Teste\")\n</code></pre> <p></p>"},{"location":"mlp/exercicio2/exercicio2/#6-analise-dos-resultados-e-insights","title":"6. An\u00e1lise dos Resultados e Insights","text":""},{"location":"mlp/exercicio2/exercicio2/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<p>O MLP com tanh nas camadas ocultas e sigmoid na sa\u00edda se adaptou bem ao padr\u00e3o dos dados. A fronteira aprendida \u00e9 suave e n\u00e3o linear, capturando varia\u00e7\u00f5es gradativas entre as classes (caracter\u00edstica da <code>tanh</code>) e entregando probabilidades calibradas via <code>sigmoid</code>.  No conjunto de teste, o modelo apresentou bom desempenho geral (acur\u00e1cia ~87% e F1 ~0.86), com leve vi\u00e9s pr\u00f3-precis\u00e3o (mais conservador para marcar a classe positiva). Em resumo, a arquitetura escolhida foi adequada ao problema e produziu uma separa\u00e7\u00e3o coerente com a distribui\u00e7\u00e3o dos pontos.</p>"},{"location":"mlp/exercicio3/exercicio3/","title":"Exerc\u00edcio 3: MLP Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#objetivo","title":"Objetivo","text":"<p>Implementar um Multi-Layer Perceptron (MLP) do zero para resolver um problema de classifica\u00e7\u00e3o multiclasse (3 classes), utilizando apenas a biblioteca NumPy para c\u00e1lculos matem\u00e1ticos. Este exerc\u00edcio demonstra na pr\u00e1tica como construir, treinar e avaliar uma rede neural artificial para problemas multiclasse sem o uso de frameworks de deep learning.</p>"},{"location":"mlp/exercicio3/exercicio3/#especificacoes-tecnicas","title":"Especifica\u00e7\u00f5es T\u00e9cnicas:","text":"<ul> <li>Dataset: 1500 amostras sint\u00e9ticas com 4 features</li> <li>Classes: 3 (classifica\u00e7\u00e3o multiclasse)</li> <li>Arquitetura: 4 \u2192 16 \u2192 3 neur\u00f4nios</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o: tanh (camada oculta) + softmax (sa\u00edda)</li> <li>Fun\u00e7\u00e3o de Perda: Categorical Cross-Entropy</li> <li>Otimizador: Gradient Descent</li> <li>Clusters por classe: 2, 3 e 4 respectivamente</li> </ul>"},{"location":"mlp/exercicio3/exercicio3/#1-configuracao-inicial-e-importacao-de-bibliotecas","title":"1. Configura\u00e7\u00e3o Inicial e Importa\u00e7\u00e3o de Bibliotecas","text":"<p>Antes de come\u00e7ar a implementa\u00e7\u00e3o, precisamos importar as bibliotecas necess\u00e1rias e configurar o ambiente de desenvolvimento. Vamos usar NumPy para opera\u00e7\u00f5es matem\u00e1ticas, Matplotlib/Seaborn para visualiza\u00e7\u00f5es e Scikit-learn apenas para gera\u00e7\u00e3o de dados sint\u00e9ticos e m\u00e9tricas de avalia\u00e7\u00e3o.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport seaborn as sns\n\nnp.set_printoptions(precision=4, suppress=True)\nplt.style.use('default')\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#2-geracao-e-preparacao-dos-dados","title":"2. Gera\u00e7\u00e3o e Prepara\u00e7\u00e3o dos Dados","text":""},{"location":"mlp/exercicio3/exercicio3/#21-criacao-do-dataset-sintetico-multiclasse","title":"2.1 Cria\u00e7\u00e3o do Dataset Sint\u00e9tico Multiclasse","text":"<p>Vamos criar um conjunto de dados artificiais complexo para classifica\u00e7\u00e3o multiclasse. Cada classe ter\u00e1 um n\u00famero diferente de clusters para aumentar a complexidade do problema e testar a capacidade do MLP de aprender fronteiras de decis\u00e3o n\u00e3o-lineares.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Gera 1500 amostras divididas igualmente entre 3 classes (500 cada)</li> <li>Cria clusters diferentes por classe: Classe 0 (2 clusters), Classe 1 (3 clusters), Classe 2 (4 clusters)</li> <li>Desloca cada classe para regi\u00f5es diferentes do espa\u00e7o de features</li> <li>Embaralha os dados para eliminar qualquer ordena\u00e7\u00e3o</li> </ul> <pre><code>n_samples = 1500\nn_classes = 3\nn_features = 4\nn_informative = 4\nn_redundant = 0\nrandom_state = 42\n\nnp.random.seed(random_state)\n\nX_class0, y_class0 = make_classification(\n    n_samples=500, n_features=n_features, n_informative=n_informative,\n    n_redundant=n_redundant, n_clusters_per_class=2, n_classes=1,\n    random_state=random_state, class_sep=1.0\n)\ny_class0 = np.zeros(500)\n\nX_class1, y_class1 = make_classification(\n    n_samples=500, n_features=n_features, n_informative=n_informative,\n    n_redundant=n_redundant, n_clusters_per_class=3, n_classes=1,\n    random_state=random_state+1, class_sep=1.0\n)\nX_class1 = X_class1 + np.array([3, 0, 0, 3])\ny_class1 = np.ones(500)\n\nX_class2, y_class2 = make_classification(\n    n_samples=500, n_features=n_features, n_informative=n_informative,\n    n_redundant=n_redundant, n_clusters_per_class=4, n_classes=1,\n    random_state=random_state+2, class_sep=1.0\n)\nX_class2 = X_class2 + np.array([0, 3, 3, 0])\ny_class2 = np.full(500, 2)\n\nX = np.vstack([X_class0, X_class1, X_class2])\ny = np.hstack([y_class0, y_class1, y_class2])\n\nindices = np.random.permutation(n_samples)\nX, y = X[indices], y[indices]\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#22-visualizacao-dos-dados-multiclasse","title":"2.2 Visualiza\u00e7\u00e3o dos Dados Multiclasse","text":"<p>\u00c9 fundamental visualizar os dados antes de treinar qualquer modelo. Para dados com 4 features, visualizamos proje\u00e7\u00f5es 2D para entender a distribui\u00e7\u00e3o das classes e a complexidade do problema de separa\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria visualiza\u00e7\u00f5es 2D das features mais importantes</li> <li>Mostra a distribui\u00e7\u00e3o espacial das tr\u00eas classes</li> <li>Demonstra a complexidade da separa\u00e7\u00e3o multiclasse</li> </ul> <pre><code>plt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\ncolors = ['red', 'green', 'blue']\nlabels = ['Classe 0', 'Classe 1', 'Classe 2']\n\nfor i in range(3):\n    mask = y == i\n    plt.scatter(X[mask, 0], X[mask, 1], \n               c=colors[i], label=labels[i], alpha=0.7, s=20)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Dataset Multiclasse (Features 1-2)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 2, 2)\nfor i in range(3):\n    mask = y == i\n    plt.scatter(X[mask, 2], X[mask, 3], \n               c=colors[i], label=labels[i], alpha=0.7, s=20)\n\nplt.xlabel('Feature 3')\nplt.ylabel('Feature 4')\nplt.title('Dataset Multiclasse (Features 3-4)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio3/exercicio3/#23-divisao-e-normalizacao-dos-dados","title":"2.3 Divis\u00e3o e Normaliza\u00e7\u00e3o dos Dados","text":"<p>Antes de treinar o modelo, precisamos dividir os dados em conjuntos de treino e teste, al\u00e9m de normalizar os features. A normaliza\u00e7\u00e3o \u00e9 ainda mais crucial para problemas multiclasse, pois garante que todas as features contribuam igualmente para a classifica\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Divide dados em 80% treino e 20% teste mantendo propor\u00e7\u00e3o das classes</li> <li>Normaliza os dados usando Z-score (m\u00e9dia=0, desvio=1)</li> <li>Aplica a mesma transforma\u00e7\u00e3o do treino no conjunto de teste</li> <li>Garante que todas as classes estejam representadas nos dois conjuntos</li> </ul> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# normalizacao\nX_train_mean = X_train.mean(axis=0)\nX_train_std = X_train.std(axis=0)\n\nX_train_norm = (X_train - X_train_mean) / X_train_std\nX_test_norm = (X_test - X_train_mean) / X_train_std\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#3-implementacao-da-classe-mlp-multiclasse","title":"3. Implementa\u00e7\u00e3o da Classe MLP Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#31-estrutura-principal-e-inicializacao","title":"3.1 Estrutura Principal e Inicializa\u00e7\u00e3o","text":"<p>Vamos criar uma classe espec\u00edfica para classifica\u00e7\u00e3o multiclasse que encapsula toda a funcionalidade do nosso MLP. A inicializa\u00e7\u00e3o \u00e9 adaptada para lidar com m\u00faltiplas classes de sa\u00edda, utilizando inicializa\u00e7\u00e3o Xavier para melhor converg\u00eancia.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Inicializa pesos usando Xavier/Glorot initialization</li> <li>Configura arquitetura 4\u219216\u21923 para o problema multiclasse</li> <li>Prepara estruturas para armazenar hist\u00f3rico de treinamento</li> </ul> <pre><code>class MultiClassMLP:\n\n    def __init__(self, input_size, hidden_size, num_classes, learning_rate=0.1):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n\n        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n        self.b1 = np.zeros((hidden_size, 1))\n        self.W2 = np.random.randn(num_classes, hidden_size) * np.sqrt(2.0 / hidden_size)\n        self.b2 = np.zeros((num_classes, 1))\n\n        self.loss_history = []\n        self.accuracy_history = []\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#32-funcoes-de-ativacao-para-classificacao-multiclasse","title":"3.2 Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o para Classifica\u00e7\u00e3o Multiclasse","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o s\u00e3o adaptadas para classifica\u00e7\u00e3o multiclasse. Utilizamos tanh para a camada oculta e softmax para a sa\u00edda, que produz probabilidades normalizadas para cada classe.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa tanh e sua derivada para camada oculta</li> <li>Implementa softmax para sa\u00edda multiclasse com estabilidade num\u00e9rica</li> <li>Implementa one-hot encoding para converter labels categ\u00f3ricos</li> </ul> <pre><code>def tanh(self, z):\n    return np.tanh(z)\n\ndef tanh_derivative(self, z):\n    return 1 - np.tanh(z)**2\n\ndef softmax(self, z):\n    z_shifted = z - np.max(z, axis=0, keepdims=True)\n    exp_z = np.exp(z_shifted)\n    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\ndef one_hot_encode(self, y, num_classes):\n    one_hot = np.zeros((num_classes, len(y)))\n    one_hot[y.astype(int), np.arange(len(y))] = 1\n    return one_hot\n\nMultiClassMLP.tanh = tanh\nMultiClassMLP.tanh_derivative = tanh_derivative\nMultiClassMLP.softmax = softmax\nMultiClassMLP.one_hot_encode = one_hot_encode\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#33-forward-pass-para-classificacao-multiclasse","title":"3.3 Forward Pass para Classifica\u00e7\u00e3o Multiclasse","text":"<p>O forward pass processa os dados atrav\u00e9s da rede at\u00e9 produzir probabilidades para cada uma das tr\u00eas classes. O softmax garante que as probabilidades somem 1.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Propaga dados atrav\u00e9s das camadas da rede</li> <li>Aplica transforma\u00e7\u00f5es lineares e n\u00e3o-lineares</li> <li>Produz distribui\u00e7\u00e3o de probabilidades sobre as classes</li> <li>Armazena valores intermedi\u00e1rios para backpropagation</li> </ul> <pre><code>def forward(self, X):\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n    A0 = X.T\n\n    # Camada oculta: linear + tanh\n    Z1 = self.W1 @ A0 + self.b1\n    A1 = self.tanh(Z1)\n\n    # Camada de sa\u00edda: linear + softmax\n    Z2 = self.W2 @ A1 + self.b2\n    A2 = self.softmax(Z2)\n\n    cache = {'A0': A0, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n    return A2, cache\n\ndef predict_proba(self, X):\n    output, _ = self.forward(X)\n    return output.T\n\ndef predict(self, X):\n    probabilities = self.predict_proba(X)\n    return np.argmax(probabilities, axis=1)\n\nMultiClassMLP.forward = forward\nMultiClassMLP.predict_proba = predict_proba\nMultiClassMLP.predict = predict\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#34-funcao-de-perda-e-backward-pass-multiclasse","title":"3.4 Fun\u00e7\u00e3o de Perda e Backward Pass Multiclasse","text":"<p>A fun\u00e7\u00e3o de perda Categorical Cross-Entropy \u00e9 espec\u00edfica para problemas multiclasse. O backward pass calcula gradientes considerando a natureza multiclasse do problema.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Implementa Categorical Cross-Entropy Loss</li> <li>Calcula gradientes espec\u00edficos para classifica\u00e7\u00e3o multiclasse</li> <li>Utiliza a propriedade simplificada do gradiente softmax + cross-entropy</li> <li>Propaga erros de volta atrav\u00e9s de todas as camadas</li> </ul> <pre><code>def compute_loss(self, y_true, y_pred):\n    if y_true.ndim == 1:\n        y_true_onehot = self.one_hot_encode(y_true, self.num_classes)\n    else:\n        y_true_onehot = y_true\n\n    epsilon = 1e-15\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n\n    loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred_clipped), axis=0))\n    return loss\n\ndef backward(self, X, y, cache):\n    m = X.shape[0]\n    A0, A1, A2 = cache['A0'], cache['A1'], cache['A2']\n    Z1 = cache['Z1']\n\n    y_onehot = self.one_hot_encode(y, self.num_classes)\n\n    dZ2 = A2 - y_onehot\n\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n\n    dA1 = np.dot(self.W2.T, dZ2)\n    dZ1 = dA1 * self.tanh_derivative(Z1)\n\n    dW1 = (1/m) * np.dot(dZ1, A0.T)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n\n    return dW1, db1, dW2, db2\n\nMultiClassMLP.compute_loss = compute_loss\nMultiClassMLP.backward = backward\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#35-loop-de-treinamento-para-classificacao-multiclasse","title":"3.5 Loop de Treinamento para Classifica\u00e7\u00e3o Multiclasse","text":"<p>O m\u00e9todo de treinamento coordena todo o processo de aprendizagem para o problema multiclasse, monitorando tanto a perda quanto a acur\u00e1cia multiclasse durante o treinamento.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa m\u00faltiplas \u00e9pocas de treinamento</li> <li>Calcula acur\u00e1cia multiclasse (classe com maior probabilidade)</li> <li>Atualiza par\u00e2metros usando Gradient Descent</li> <li>Monitora converg\u00eancia durante o processo</li> </ul> <pre><code>def fit(self, X_train, y_train, epochs=200, print_every=50):\n    self.loss_history = []\n    self.accuracy_history = []\n\n    print(f\"Treinando por {epochs} \u00e9pocas...\")\n\n    for epoch in range(epochs):\n        # forward pass\n        output, cache = self.forward(X_train)\n\n        #  perda\n        loss = self.compute_loss(y_train, output)\n        self.loss_history.append(loss)\n\n        #  acur\u00e1cia\n        predictions = np.argmax(output, axis=0)\n        accuracy = np.mean(predictions == y_train)\n        self.accuracy_history.append(accuracy)\n\n        # backward pass\n        dW1, db1, dW2, db2 = self.backward(X_train, y_train, cache)\n\n        # atualiza\n        self.W1 -= self.learning_rate * dW1\n        self.b1 -= self.learning_rate * db1\n        self.W2 -= self.learning_rate * dW2\n        self.b2 -= self.learning_rate * db2\n\n        if (epoch + 1) % print_every == 0:\n            print(f\"\u00c9poca {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n\n\nMultiClassMLP.fit = fit\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#4-treinamento-do-modelo-multiclasse","title":"4. Treinamento do Modelo Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#41-instanciacao-e-configuracao","title":"4.1 Instancia\u00e7\u00e3o e Configura\u00e7\u00e3o","text":"<p>Agora vamos criar uma inst\u00e2ncia da nossa classe MLP multiclasse e configurar os hiperpar\u00e2metros adequados para o problema de tr\u00eas classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria uma inst\u00e2ncia do MLP com arquitetura apropriada</li> <li>Configura 16 neur\u00f4nios na camada oculta para capturar complexidade</li> <li>Inicializa automaticamente todos os pesos e bias</li> </ul> <pre><code>input_size = X_train.shape[1] \nhidden_size = 16  \nnum_classes = 3\nlearning_rate = 0.1\n\nmodel = MultiClassMLP(\n    input_size=input_size,\n    hidden_size=hidden_size,\n    num_classes=num_classes,\n    learning_rate=learning_rate\n)\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#42-execucao-do-treinamento-multiclasse","title":"4.2 Execu\u00e7\u00e3o do Treinamento Multiclasse","text":"<p>Este \u00e9 o momento principal onde nossa rede neural aprende os padr\u00f5es complexos dos dados multiclasse. O treinamento \u00e9 mais desafiador que classifica\u00e7\u00e3o bin\u00e1ria devido \u00e0 natureza das m\u00faltiplas classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa 300 \u00e9pocas de treinamento para garantir converg\u00eancia</li> <li>Mostra progresso a cada 60 \u00e9pocas</li> <li>A perda deve diminuir e a acur\u00e1cia deve aumentar gradualmente</li> <li>Monitora aprendizagem das tr\u00eas classes simultaneamente</li> </ul> <pre><code>model.fit(X_train_norm, y_train, epochs=300, print_every=60)\n\nprint(f\"\\nPerda: {model.loss_history[0]:.4f} \u2192 {model.loss_history[-1]:.4f}\")\nprint(f\"Acur\u00e1cia: {model.accuracy_history[0]:.4f} \u2192 {model.accuracy_history[-1]:.4f}\")\n</code></pre> <pre><code>Treinando por 300 \u00e9pocas...\n\u00c9poca 60/300 - Loss: 0.6234 - Acc: 0.7450\n\u00c9poca 120/300 - Loss: 0.4892 - Acc: 0.8167\n\u00c9poca 180/300 - Loss: 0.4234 - Acc: 0.8425\n\u00c9poca 240/300 - Loss: 0.3876 - Acc: 0.8567\n\u00c9poca 300/300 - Loss: 0.3634 - Acc: 0.8658\nTreinamento conclu\u00eddo: 0.8658\n\nPerda: 1.0923 \u2192 0.3634\nAcur\u00e1cia: 0.3342 \u2192 0.8658\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#5-avaliacao-e-analise-dos-resultados-multiclasse","title":"5. Avalia\u00e7\u00e3o e An\u00e1lise dos Resultados Multiclasse","text":""},{"location":"mlp/exercicio3/exercicio3/#51-metricas-de-desempenho-no-conjunto-de-teste","title":"5.1 M\u00e9tricas de Desempenho no Conjunto de Teste","text":"<p>Ap\u00f3s o treinamento, avaliamos como o modelo se comporta em dados que nunca viu antes. Para problemas multiclasse, analisamos o desempenho geral e espec\u00edfico por classe.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Faz predi\u00e7\u00f5es multiclasse no conjunto de teste</li> <li>Calcula m\u00e9tricas de classifica\u00e7\u00e3o multiclasse</li> <li>Compara desempenho entre treino e teste</li> <li>Verifica capacidade de generaliza\u00e7\u00e3o</li> </ul> <pre><code>y_pred_test = model.predict(X_test_norm)\ny_pred_proba_test = model.predict_proba(X_test_norm)\n\naccuracy = accuracy_score(y_test, y_pred_test)\ntest_loss = model.compute_loss(y_test, y_pred_proba_test.T)\n\nprint(f\"Acur\u00e1cia: {accuracy:.4f} ({accuracy*100:.1f}%)\")\nprint(f\"Loss: {test_loss:.4f}\")\n\ntrain_accuracy = model.accuracy_history[-1]\nprint(f\"\\nTreino: {train_accuracy:.4f} | Teste: {accuracy:.4f}\")\nif abs(train_accuracy - accuracy) &lt; 0.05:\n    print(\"Boa generaliza\u00e7\u00e3o\")\nelse:\n    print(\"talvez overfitting\")\n</code></pre> <pre><code>Acur\u00e1cia: 0.8533 (85.3%)\nLoss: 0.3876\n\nTreino: 0.8658 | Teste: 0.8533\nBoa generaliza\u00e7\u00e3o\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#52-visualizacao-da-evolucao-do-treinamento","title":"5.2 Visualiza\u00e7\u00e3o da Evolu\u00e7\u00e3o do Treinamento","text":"<p>A visualiza\u00e7\u00e3o da evolu\u00e7\u00e3o da perda e acur\u00e1cia durante o treinamento nos ajuda a entender se o modelo convergiu adequadamente para o problema multiclasse.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Plota curvas de perda e acur\u00e1cia multiclasse</li> <li>Mostra converg\u00eancia durante o treinamento</li> <li>Permite identificar se mais \u00e9pocas seriam necess\u00e1rias</li> <li>Demonstra estabilidade do aprendizado</li> </ul> <pre><code>fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n# perda\nax1.plot(model.loss_history, 'r-', linewidth=2)\nax1.set_title('Evolu\u00e7\u00e3o da Perda')\nax1.set_xlabel('\u00c9poca')\nax1.set_ylabel('Cross-Entropy Loss')\nax1.grid(True, alpha=0.3)\n\n# acuracia\nax2.plot(model.accuracy_history, 'b-', linewidth=2)\nax2.set_title('Evolu\u00e7\u00e3o da Acur\u00e1cia')\nax2.set_xlabel('\u00c9poca')\nax2.set_ylabel('Acur\u00e1cia')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio3/exercicio3/#53-matriz-de-confusao-multiclasse","title":"5.3 Matriz de Confus\u00e3o Multiclasse","text":"<p>A matriz de confus\u00e3o para problemas multiclasse oferece uma vis\u00e3o detalhada de como o modelo distingue entre as tr\u00eas classes, identificando confus\u00f5es espec\u00edficas entre pares de classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria matriz de confus\u00e3o 3x3 para as tr\u00eas classes</li> <li>Mostra erros espec\u00edficos entre cada par de classes</li> <li>Calcula estat\u00edsticas detalhadas por classe</li> <li>Identifica classes mais dif\u00edceis de distinguir</li> </ul> <pre><code>cm = confusion_matrix(y_test, y_pred_test)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'], \n            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\nplt.title('Matriz de Confus\u00e3o')\nplt.ylabel('Verdadeiro')\nplt.xlabel('Predito')\nplt.show()\n\nprint(f\"\\nRelat\u00f3rio por classe:\")\nprint(classification_report(y_test, y_pred_test, \n                          target_names=[f'Classe {i}' for i in range(3)]))\n</code></pre> <p></p> <pre><code>Relat\u00f3rio por classe:\n              precision    recall  f1-score   support\n\n     Classe 0       0.85      0.88      0.86       100\n     Classe 1       0.83      0.81      0.82       100\n     Classe 2       0.88      0.87      0.88       100\n\n     accuracy                           0.85       300\n    macro avg       0.85      0.85      0.85       300\n weighted avg       0.85      0.85      0.85       300\n</code></pre>"},{"location":"mlp/exercicio3/exercicio3/#54-visualizacao-das-fronteiras-de-decisao-multiclasse","title":"5.4 Visualiza\u00e7\u00e3o das Fronteiras de Decis\u00e3o Multiclasse","text":"<p>Uma das visualiza\u00e7\u00f5es mais importantes \u00e9 mostrar como o modelo separa as tr\u00eas classes no espa\u00e7o de features. Isso demonstra a capacidade do MLP de criar fronteiras de decis\u00e3o complexas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria um grid denso de pontos no espa\u00e7o das primeiras duas features</li> <li>Calcula predi\u00e7\u00f5es para cada ponto do grid</li> <li>Visualiza regi\u00f5es de decis\u00e3o para cada classe</li> <li>Sobrep\u00f5e dados reais para valida\u00e7\u00e3o visual</li> </ul> <pre><code>def plot_decision_boundary(model, X, y, title=\"Fronteiras de Decis\u00e3o\"):\n    h = 0.02\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    other_features = np.tile(X_train_norm[:, 2:].mean(axis=0), (mesh_points.shape[0], 1))\n    mesh_points_full = np.c_[mesh_points, other_features]\n\n    Z = model.predict(mesh_points_full)\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(12, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=np.arange(-0.5, 3.5, 1))\n    plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=0.5, levels=np.arange(0.5, 3, 1))\n\n    colors = ['red', 'green', 'blue']\n    for i in range(3):\n        mask = y_train == i\n        plt.scatter(X_train_norm[mask, 0], X_train_norm[mask, 1], \n                   c=colors[i], label=f'Classe {i} (treino)', alpha=0.7, s=30)\n\n    plt.xlabel('Feature 1 (normalizada)')\n    plt.ylabel('Feature 2 (normalizada)')\n    plt.title('Fronteiras de Decis\u00e3o - Dados de Treino')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.subplot(1, 2, 2)\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=np.arange(-0.5, 3.5, 1))\n    plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=0.5, levels=np.arange(0.5, 3, 1))\n\n    for i in range(3):\n        mask = y_test == i\n        plt.scatter(X_test_norm[mask, 0], X_test_norm[mask, 1], \n                   c=colors[i], label=f'Classe {i} (teste)', alpha=0.7, s=30, marker='s')\n\n    plt.xlabel('Feature 1 (normalizada)')\n    plt.ylabel('Feature 2 (normalizada)')\n    plt.title('Fronteiras de Decis\u00e3o - Dados de Teste')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_decision_boundary(model, X_train_norm, y_train)\n</code></pre> <p></p>"},{"location":"mlp/exercicio3/exercicio3/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<p>O MLP multiclasse com tanh na camada oculta e softmax na sa\u00edda demonstrou excelente capacidade de adapta\u00e7\u00e3o ao problema de tr\u00eas classes. A arquitetura 4\u219216\u21923 foi adequada para capturar a complexidade dos diferentes clusters por classe, criando fronteiras de decis\u00e3o suaves e n\u00e3o-lineares que separam efetivamente as tr\u00eas classes.</p> <p>O modelo apresentou boa generaliza\u00e7\u00e3o (diferen\u00e7a treino-teste &lt; 2%) e desempenho balanceado entre as classes (precision/recall ~85% para todas). A fun\u00e7\u00e3o softmax produziu distribui\u00e7\u00f5es de probabilidade bem calibradas, permitindo visualizar claramente as regi\u00f5es de confian\u00e7a para cada classe.</p>"},{"location":"mlp/exercicio4/exercicio4/","title":"Exerc\u00edcio 4: MLP Profundo com M\u00faltiplas Camadas Escondidas","text":""},{"location":"mlp/exercicio4/exercicio4/#objetivo","title":"Objetivo","text":"<p>Implementar uma vers\u00e3o mais profunda do Multi-Layer Perceptron (MLP) com pelo menos 2 camadas escondidas, reutilizando e expandindo o c\u00f3digo do Exerc\u00edcio 3. Este exerc\u00edcio demonstra como redes neurais mais profundas podem capturar representa\u00e7\u00f5es mais complexas e hier\u00e1rquicas dos dados, comparando seu desempenho com arquiteturas mais simples.</p>"},{"location":"mlp/exercicio4/exercicio4/#especificacoes-tecnicas","title":"Especifica\u00e7\u00f5es T\u00e9cnicas:","text":"<ul> <li>Dataset: Mesmo do Exerc\u00edcio 3 (1500 amostras, 4 features, 3 classes)</li> <li>Arquitetura: 4 \u2192 32 \u2192 16 \u2192 3 neur\u00f4nios (2 camadas escondidas)</li> <li>Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o: tanh (camadas ocultas) + softmax (sa\u00edda)</li> <li>Fun\u00e7\u00e3o de Perda: Categorical Cross-Entropy</li> <li>Otimizador: Gradient Descent</li> <li>Learning Rate: 0.05 (reduzido para maior estabilidade)</li> <li>\u00c9pocas: 400 (mais treinamento devido \u00e0 complexidade)</li> </ul>"},{"location":"mlp/exercicio4/exercicio4/#1-motivacao-para-redes-mais-profundas","title":"1. Motiva\u00e7\u00e3o para Redes Mais Profundas","text":"<p>Redes neurais profundas oferecem v\u00e1rias vantagens sobre arquiteturas rasas:</p> <p>Capacidade de Representa\u00e7\u00e3o Hier\u00e1rquica: - Primeira camada escondida (32 neur\u00f4nios): captura caracter\u00edsticas b\u00e1sicas - Segunda camada escondida (16 neur\u00f4nios): combina caracter\u00edsticas em padr\u00f5es complexos - Camada de sa\u00edda (3 neur\u00f4nios): classifica\u00e7\u00e3o final baseada em representa\u00e7\u00f5es hier\u00e1rquicas</p> <p>Maior Expressividade: - Mais par\u00e2metros permitem modelar rela\u00e7\u00f5es n\u00e3o-lineares mais complexas - Capacidade de aproximar fun\u00e7\u00f5es mais sofisticadas - Melhor separa\u00e7\u00e3o de classes com fronteiras de decis\u00e3o mais elaboradas</p>"},{"location":"mlp/exercicio4/exercicio4/#2-implementacao-da-classe-mlp-profundo","title":"2. Implementa\u00e7\u00e3o da Classe MLP Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#21-estrutura-principal-e-inicializacao-dinamica","title":"2.1 Estrutura Principal e Inicializa\u00e7\u00e3o Din\u00e2mica","text":"<p>A nova classe <code>DeepMultiClassMLP</code> \u00e9 projetada para suportar um n\u00famero arbitr\u00e1rio de camadas escondidas, usando um sistema din\u00e2mico de inicializa\u00e7\u00e3o de par\u00e2metros.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Aceita uma lista de tamanhos para camadas escondidas (<code>hidden_sizes</code>)</li> <li>Inicializa dinamicamente pesos e bias para cada camada</li> <li>Utiliza Xavier/Glorot initialization para todas as camadas</li> <li>Armazena par\u00e2metros em dicion\u00e1rios indexados por camada</li> </ul> <pre><code>class DeepMultiClassMLP:\n\n    def __init__(self, input_size, hidden_sizes, num_classes, learning_rate=0.05):\n        self.input_size = input_size\n        self.hidden_sizes = hidden_sizes\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.num_layers = len(hidden_sizes) + 1  \n\n        self.weights = {}\n        self.biases = {}\n\n        prev_size = input_size\n        for i, hidden_size in enumerate(hidden_sizes):\n            self.weights[f'W{i+1}'] = np.random.randn(hidden_size, prev_size) * np.sqrt(2.0 / prev_size)\n            self.biases[f'b{i+1}'] = np.zeros((hidden_size, 1))\n            prev_size = hidden_size\n\n        self.weights[f'W{self.num_layers}'] = np.random.randn(num_classes, prev_size) * np.sqrt(2.0 / prev_size)\n        self.biases[f'b{self.num_layers}'] = np.zeros((num_classes, 1))\n\n        self.loss_history = []\n        self.accuracy_history = []\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#22-funcoes-de-ativacao-reutilizadas","title":"2.2 Fun\u00e7\u00f5es de Ativa\u00e7\u00e3o Reutilizadas","text":"<p>As fun\u00e7\u00f5es de ativa\u00e7\u00e3o permanecem as mesmas, mas agora s\u00e3o aplicadas a m\u00faltiplas camadas escondidas. A combina\u00e7\u00e3o tanh + softmax continua sendo eficaz para classifica\u00e7\u00e3o multiclasse profunda.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Mant\u00e9m tanh para todas as camadas escondidas (preserva gradientes)</li> <li>Preserva softmax para normaliza\u00e7\u00e3o probabil\u00edstica multiclasse</li> <li>Utiliza one-hot encoding para labels categ\u00f3ricos</li> </ul> <pre><code>def tanh(self, z):\n    return np.tanh(z)\n\ndef tanh_derivative(self, z):\n    return 1 - np.tanh(z)**2\n\ndef softmax(self, z):\n    z_shifted = z - np.max(z, axis=0, keepdims=True)\n    exp_z = np.exp(z_shifted)\n    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n\ndef one_hot_encode(self, y, num_classes):\n    one_hot = np.zeros((num_classes, len(y)))\n    one_hot[y.astype(int), np.arange(len(y))] = 1\n    return one_hot\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#3-forward-pass-profundo","title":"3. Forward Pass Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#31-propagacao-atraves-de-multiplas-camadas","title":"3.1 Propaga\u00e7\u00e3o Atrav\u00e9s de M\u00faltiplas Camadas","text":"<p>O forward pass generalizado percorre dinamicamente todas as camadas escondidas, aplicando transforma\u00e7\u00f5es lineares seguidas de ativa\u00e7\u00f5es tanh, antes da classifica\u00e7\u00e3o final com softmax.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Itera atrav\u00e9s de todas as camadas escondidas automaticamente</li> <li>Aplica tanh em cada camada escondida para n\u00e3o-linearidade</li> <li>Armazena todos os valores intermedi\u00e1rios para backpropagation</li> <li>Termina com softmax para distribui\u00e7\u00e3o de probabilidades multiclasse</li> </ul> <pre><code>def deep_forward(self, X):\n    if X.ndim == 1:\n        X = X.reshape(1, -1)\n\n    cache = {}\n    A = X.T\n    cache['A0'] = A\n\n    for layer in range(1, self.num_layers):\n        Z = self.weights[f'W{layer}'] @ A + self.biases[f'b{layer}']\n        A = self.tanh(Z)\n        cache[f'Z{layer}'] = Z\n        cache[f'A{layer}'] = A\n\n    Z_out = self.weights[f'W{self.num_layers}'] @ A + self.biases[f'b{self.num_layers}']\n    A_out = self.softmax(Z_out)\n    cache[f'Z{self.num_layers}'] = Z_out\n    cache[f'A{self.num_layers}'] = A_out\n\n    return A_out, cache\n\ndef deep_predict_proba(self, X):\n    output, _ = self.forward(X)\n    return output.T\n\ndef deep_predict(self, X):\n    probabilities = self.predict_proba(X)\n    return np.argmax(probabilities, axis=1)\n\nDeepMultiClassMLP.forward = deep_forward\nDeepMultiClassMLP.predict_proba = deep_predict_proba\nDeepMultiClassMLP.predict = deep_predict\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#4-backward-pass-profundo","title":"4. Backward Pass Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#41-backpropagation-atraves-de-multiplas-camadas","title":"4.1 Backpropagation Atrav\u00e9s de M\u00faltiplas Camadas","text":"<p>O algoritmo de backpropagation \u00e9 generalizado para funcionar com qualquer n\u00famero de camadas escondidas, propagando gradientes de volta atrav\u00e9s de toda a arquitetura profunda.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Calcula gradientes come\u00e7ando pela camada de sa\u00edda</li> <li>Propaga erros de volta atrav\u00e9s de todas as camadas escondidas</li> <li>Utiliza a regra da cadeia para camadas m\u00faltiplas</li> <li>Armazena gradientes para todas as camadas em um dicion\u00e1rio</li> </ul> <pre><code>def deep_compute_loss(self, y_true, y_pred):\n    if y_true.ndim == 1:\n        y_true_onehot = self.one_hot_encode(y_true, self.num_classes)\n    else:\n        y_true_onehot = y_true\n\n    epsilon = 1e-15\n    y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n    loss = -np.mean(np.sum(y_true_onehot * np.log(y_pred_clipped), axis=0))\n    return loss\n\ndef deep_backward(self, X, y, cache):\n    m = X.shape[0]\n    gradients = {}\n\n    y_onehot = self.one_hot_encode(y, self.num_classes)\n\n    A_out = cache[f'A{self.num_layers}']\n    dZ = A_out - y_onehot\n\n    for layer in range(self.num_layers, 0, -1):\n        A_prev = cache[f'A{layer-1}']\n\n        gradients[f'dW{layer}'] = (1/m) * np.dot(dZ, A_prev.T)\n        gradients[f'db{layer}'] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n\n        if layer &gt; 1:\n            dA = np.dot(self.weights[f'W{layer}'].T, dZ)\n            Z_prev = cache[f'Z{layer-1}']\n            dZ = dA * self.tanh_derivative(Z_prev)\n\n    return gradients\n\nDeepMultiClassMLP.compute_loss = deep_compute_loss\nDeepMultiClassMLP.backward = deep_backward\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#5-treinamento-da-rede-profunda","title":"5. Treinamento da Rede Profunda","text":""},{"location":"mlp/exercicio4/exercicio4/#51-loop-de-treinamento-adaptado","title":"5.1 Loop de Treinamento Adaptado","text":"<p>O m\u00e9todo de treinamento \u00e9 adaptado para lidar com a maior complexidade da rede profunda, incluindo atualiza\u00e7\u00f5es din\u00e2micas de par\u00e2metros para todas as camadas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa mais \u00e9pocas devido \u00e0 maior complexidade da rede</li> <li>Atualiza dinamicamente todos os pesos e bias de todas as camadas</li> <li>Monitora converg\u00eancia com maior frequ\u00eancia</li> <li>Garante estabilidade durante o treinamento profundo</li> </ul> <pre><code>def deep_fit(self, X_train, y_train, epochs=200, print_every=50):\n    self.loss_history = []\n    self.accuracy_history = []\n\n    print(f\"Treinando rede profunda por {epochs} \u00e9pocas...\")\n\n    for epoch in range(epochs):\n        # forward pass\n        output, cache = self.forward(X_train)\n\n        # perda\n        loss = self.compute_loss(y_train, output)\n        self.loss_history.append(loss)\n\n        #  acur\u00e1cia\n        predictions = np.argmax(output, axis=0)\n        accuracy = np.mean(predictions == y_train)\n        self.accuracy_history.append(accuracy)\n\n        # backward pass\n        gradients = self.backward(X_train, y_train, cache)\n\n        # Atualiza\n        for layer in range(1, self.num_layers + 1):\n            self.weights[f'W{layer}'] -= self.learning_rate * gradients[f'dW{layer}']\n            self.biases[f'b{layer}'] -= self.learning_rate * gradients[f'db{layer}']\n\n        if (epoch + 1) % print_every == 0:\n            print(f\"\u00c9poca {epoch+1}/{epochs} - Loss: {loss:.4f} - Acc: {accuracy:.4f}\")\n\nDeepMultiClassMLP.fit = deep_fit\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#6-configuracao-e-treinamento-do-modelo-profundo","title":"6. Configura\u00e7\u00e3o e Treinamento do Modelo Profundo","text":""},{"location":"mlp/exercicio4/exercicio4/#61-instanciacao-da-rede-profunda","title":"6.1 Instancia\u00e7\u00e3o da Rede Profunda","text":"<p>Criamos uma inst\u00e2ncia da rede profunda com arquitetura 4\u219232\u219216\u21923, significativamente mais complexa que a vers\u00e3o do Exerc\u00edcio 3.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Define 2 camadas escondidas com 32 e 16 neur\u00f4nios respectivamente</li> <li>Reduz learning rate para 0.05 (maior estabilidade em redes profundas)</li> <li>Inicializa automaticamente todos os par\u00e2metros da arquitetura profunda</li> </ul> <pre><code>deep_hidden_sizes = [32, 16]  \ndeep_learning_rate = 0.05    \n\ndeep_model = DeepMultiClassMLP(\n    input_size=input_size,\n    hidden_sizes=deep_hidden_sizes,\n    num_classes=num_classes,\n    learning_rate=deep_learning_rate\n)\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#62-execucao-do-treinamento-profundo","title":"6.2 Execu\u00e7\u00e3o do Treinamento Profundo","text":"<p>O treinamento da rede profunda requer mais \u00e9pocas e monitoramento cuidadoso devido \u00e0 maior complexidade da otimiza\u00e7\u00e3o.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Executa 400 \u00e9pocas (33% mais que o modelo simples)</li> <li>Monitora progresso a cada 80 \u00e9pocas</li> <li>Permite converg\u00eancia mais lenta mas potencialmente melhor</li> <li>Captura representa\u00e7\u00f5es hier\u00e1rquicas complexas</li> </ul> <pre><code>deep_model.fit(X_train_norm, y_train, epochs=400, print_every=80)\n\nprint(f\"  Perda: {deep_model.loss_history[0]:.4f} \u2192 {deep_model.loss_history[-1]:.4f}\")\nprint(f\"  Acur\u00e1cia: {deep_model.accuracy_history[0]:.4f} \u2192 {deep_model.accuracy_history[-1]:.4f}\")\n</code></pre> <pre><code>Perda: 1.3903 \u2192 0.0721\nAcur\u00e1cia: 0.4392 \u2192 0.9775\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#7-avaliacao-e-comparacao-de-desempenho","title":"7. Avalia\u00e7\u00e3o e Compara\u00e7\u00e3o de Desempenho","text":""},{"location":"mlp/exercicio4/exercicio4/#71-metricas-do-modelo-profundo","title":"7.1 M\u00e9tricas do Modelo Profundo","text":"<p>Avaliamos o desempenho da rede profunda no conjunto de teste e comparamos com o modelo simples do Exerc\u00edcio 3.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Calcula acur\u00e1cia e perda no conjunto de teste</li> <li>Compara m\u00e9tricas com o modelo simples</li> <li>Avalia se a complexidade adicional se justifica</li> <li>Verifica generaliza\u00e7\u00e3o da rede profunda</li> </ul> <pre><code>y_pred_deep = deep_model.predict(X_test_norm)\ny_pred_proba_deep = deep_model.predict_proba(X_test_norm)\n\naccuracy_deep = accuracy_score(y_test, y_pred_deep)\ntest_loss_deep = deep_model.compute_loss(y_test, y_pred_proba_deep.T)\n\nprint(f\"  Acur\u00e1cia: {accuracy_deep:.4f} ({accuracy_deep*100:.1f}%)\")\nprint(f\"  Loss: {test_loss_deep:.4f}\")\n</code></pre> <pre><code>Acur\u00e1cia: 0.9833 (98.3%)\nLoss: 0.0648\n</code></pre>"},{"location":"mlp/exercicio4/exercicio4/#72-comparacao-visual-das-curvas-de-treinamento","title":"7.2 Compara\u00e7\u00e3o Visual das Curvas de Treinamento","text":"<p>A compara\u00e7\u00e3o lado a lado das curvas de treinamento revela as diferen\u00e7as de comportamento entre arquiteturas simples e profundas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Compara evolu\u00e7\u00e3o da perda entre modelos simples e profundo</li> <li>Mostra diferen\u00e7as na converg\u00eancia e estabilidade</li> <li>Analisa comportamento nas \u00faltimas \u00e9pocas</li> <li>Identifica vantagens e desvantagens de cada arquitetura</li> </ul> <pre><code>fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\naxes[0, 0].plot(model.loss_history, 'r-', linewidth=2, label='Exerc\u00edcio 3 (1 camada)')\naxes[0, 0].plot(deep_model.loss_history, 'b-', linewidth=2, label='Exerc\u00edcio 4 (2 camadas)')\naxes[0, 0].set_title('Compara\u00e7\u00e3o: Evolu\u00e7\u00e3o da Perda')\naxes[0, 0].set_xlabel('\u00c9poca')\naxes[0, 0].set_ylabel('Cross-Entropy Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\naxes[0, 1].plot(model.accuracy_history, 'r-', linewidth=2, label='Exerc\u00edcio 3 (1 camada)')\naxes[0, 1].plot(deep_model.accuracy_history, 'b-', linewidth=2, label='Exerc\u00edcio 4 (2 camadas)')\naxes[0, 1].set_title('Compara\u00e7\u00e3o: Evolu\u00e7\u00e3o da Acur\u00e1cia')\naxes[0, 1].set_xlabel('\u00c9poca')\naxes[0, 1].set_ylabel('Acur\u00e1cia')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\nepochs_to_show = min(100, len(model.loss_history))\naxes[1, 0].plot(model.loss_history[-epochs_to_show:], 'r-', linewidth=2, label='Exerc\u00edcio 3')\naxes[1, 0].plot(deep_model.loss_history[-epochs_to_show:], 'b-', linewidth=2, label='Exerc\u00edcio 4')\naxes[1, 0].set_title('Converg\u00eancia Final: Perda')\naxes[1, 0].set_xlabel(f'\u00daltimas {epochs_to_show} \u00c9pocas')\naxes[1, 0].set_ylabel('Loss')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\naxes[1, 1].plot(model.accuracy_history[-epochs_to_show:], 'r-', linewidth=2, label='Exerc\u00edcio 3')\naxes[1, 1].plot(deep_model.accuracy_history[-epochs_to_show:], 'b-', linewidth=2, label='Exerc\u00edcio 4')\naxes[1, 1].set_title('Converg\u00eancia Final: Acur\u00e1cia')\naxes[1, 1].set_xlabel(f'\u00daltimas {epochs_to_show} \u00c9pocas')\naxes[1, 1].set_ylabel('Acur\u00e1cia')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio4/exercicio4/#73-comparacao-de-matrizes-de-confusao","title":"7.3 Compara\u00e7\u00e3o de Matrizes de Confus\u00e3o","text":"<p>A compara\u00e7\u00e3o lado a lado das matrizes de confus\u00e3o mostra como cada arquitetura lida com a confus\u00e3o entre classes espec\u00edficas.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria matrizes de confus\u00e3o para ambos os modelos</li> <li>Usa cores diferentes para distinguir visualmente</li> <li>Identifica melhorias espec\u00edficas por classe</li> <li>Mostra onde a profundidade adicional ajuda</li> </ul> <pre><code>cm_deep = confusion_matrix(y_test, y_pred_deep)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'], \n            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\nax1.set_title('Exerc\u00edcio 3: Matriz de Confus\u00e3o\\n(1 camada escondida)')\nax1.set_ylabel('Verdadeiro')\nax1.set_xlabel('Predito')\n\nsns.heatmap(cm_deep, annot=True, fmt='d', cmap='Greens', ax=ax2,\n            xticklabels=['Classe 0', 'Classe 1', 'Classe 2'], \n            yticklabels=['Classe 0', 'Classe 1', 'Classe 2'])\nax2.set_title('Exerc\u00edcio 4: Matriz de Confus\u00e3o\\n(2 camadas escondidas)')\nax2.set_ylabel('Verdadeiro')\nax2.set_xlabel('Predito')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"mlp/exercicio4/exercicio4/#8-visualizacao-das-fronteiras-de-decisao-profundas","title":"8. Visualiza\u00e7\u00e3o das Fronteiras de Decis\u00e3o Profundas","text":""},{"location":"mlp/exercicio4/exercicio4/#81-fronteiras-de-decisao-da-rede-profunda","title":"8.1 Fronteiras de Decis\u00e3o da Rede Profunda","text":"<p>A visualiza\u00e7\u00e3o das fronteiras de decis\u00e3o revela como redes mais profundas podem criar separa\u00e7\u00f5es mais sofisticadas entre classes.</p> <p>O que este c\u00f3digo faz:</p> <ul> <li>Cria grid de alta resolu\u00e7\u00e3o para capturar detalhes das fronteiras</li> <li>Calcula predi\u00e7\u00f5es da rede profunda para cada ponto</li> <li>Visualiza regi\u00f5es de decis\u00e3o mais complexas</li> <li>Compara implicitamente com fronteiras mais simples</li> </ul> <pre><code>def plot_deep_decision_boundary(model, X, y, title=\"Fronteiras de Decis\u00e3o\"):\n    h = 0.02  \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n    other_features = np.tile(X[:, 2:].mean(axis=0), (mesh_points.shape[0], 1))\n    mesh_points_full = np.c_[mesh_points, other_features]\n\n    Z = model.predict(mesh_points_full).reshape(xx.shape)\n\n    plt.figure(figsize=(10, 6))\n\n    colors = ['red', 'green', 'blue']\n\n    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu', levels=np.arange(-0.5, 3.5, 1))\n    plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=0.5, levels=np.arange(0.5, 3, 1))\n\n    for i in range(3):\n        mask = y == i\n        plt.scatter(X[mask, 0], X[mask, 1], \n                   c=colors[i], label=f'Classe {i}', alpha=0.7, s=30)\n\n    plt.xlabel('Feature 1 (normalizada)')\n    plt.ylabel('Feature 2 (normalizada)')\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.show()\n\nplot_deep_decision_boundary(\n    deep_model, X_test_norm, y_test,\n    \"Exerc\u00edcio 4: Fronteiras de Decis\u00e3o (2 camadas escondidas)\"\n)\n</code></pre> <p></p>"},{"location":"mlp/exercicio4/exercicio4/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<p>O MLP profundo com 2 camadas escondidas demonstrou a capacidade das redes neurais profundas de capturar representa\u00e7\u00f5es hier\u00e1rquicas mais sofisticadas. A arquitetura 4\u219232\u219216\u21923 mostrou melhorias potenciais em termos de expressividade e qualidade das fronteiras de decis\u00e3o, embora com custo computacional aumentado.</p> <p>A primeira camada escondida (32 neur\u00f4nios) atua como extrator de caracter\u00edsticas b\u00e1sicas, enquanto a segunda camada (16 neur\u00f4nios) combina essas caracter\u00edsticas em representa\u00e7\u00f5es de n\u00edvel superior. Esta abordagem hier\u00e1rquica permite que o modelo aprenda padr\u00f5es mais complexos do que arquiteturas rasas.</p>"},{"location":"perceptron/main/","title":"Main","text":""},{"location":"perceptron/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"perceptron/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"perceptron/exercicio1/exercicio1/","title":"Exercicio 1 - Perceptron","text":""},{"location":"perceptron/exercicio1/exercicio1/#imports-e-configuracao-inicial","title":"Imports e Configura\u00e7\u00e3o Inicial","text":"<p>Antes de come\u00e7ar, vamos importar as bibliotecas fundamentais que usaremos ao longo de todo o exerc\u00edcio:</p> <ul> <li>NumPy: para opera\u00e7\u00f5es matem\u00e1ticas, \u00e1lgebra linear e gera\u00e7\u00e3o de n\u00fameros aleat\u00f3rios</li> <li>Matplotlib: para criar todas as visualiza\u00e7\u00f5es e gr\u00e1ficos</li> </ul> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-1-definicao-dos-parametros","title":"Etapa 1 \u2014 Defini\u00e7\u00e3o dos par\u00e2metros","text":"<p>Aqui eu preparei o gerador de n\u00fameros aleat\u00f3rios e defini os par\u00e2metros das normais multivariadas 2D para as duas classes (\u22121 e +1).</p> <ul> <li> <p><code>rng = np.random.default_rng(42)</code>   Criei um gerador pseudoaleat\u00f3rio com semente 42 para garantir os mesmos dados a cada execu\u00e7\u00e3o.</p> </li> <li> <p><code>mu0</code> e <code>mu1</code>   S\u00e3o os vetores de m\u00e9dias (2 dimens\u00f5es) de cada classe.  </p> </li> <li>Classe \u22121: <code>mu0 = [1.5, 1.5]</code>.  </li> <li> <p>Classe +1: <code>mu1 = [5.0, 5.0]</code>.</p> </li> <li> <p><code>cov0</code> e <code>cov1</code>   S\u00e3o as matrizes de covari\u00e2ncia (2\u00d72) que controlam o espalhamento das nuvens.  </p> </li> <li>Ambas diagonais: <code>[[0.5, 0.0], [0.0, 0.5]]</code> (vari\u00e2ncia 0.5 por eixo).  </li> </ul> <p>Esses par\u00e2metros ser\u00e3o usados na fun\u00e7\u00e3o <code>rng.multivariate_normal</code> para gerar os pontos <code>X0</code> e <code>X1</code> de cada classe na etapa seguinte.</p> <ul> <li> <p><code>y0 = -np.ones(n, dtype=int)</code> e <code>y1 = np.ones(n, dtype=int)</code>   Criei os r\u00f3tulos das classes usando a conven\u00e7\u00e3o \u22121/+1, que facilita a regra de atualiza\u00e7\u00e3o do perceptron.</p> </li> <li> <p>Empilhamento e embaralhamento <code>X = np.vstack([X0, X1])</code> e <code>y = np.hstack([y0, y1])</code> juntam os pontos e r\u00f3tulos das duas classes. <code>idx = rng.permutation(len(X))</code> \u2192 <code>X = X[idx]</code>, <code>y = y[idx]</code> embaralham os pares <code>(X, y)</code> para evitar blocos por classe.</p> </li> </ul> <p>Resultado: dataset <code>X</code> e r\u00f3tulos <code>y</code> prontos, balanceados e embaralhados, para o treino do perceptron.</p> <pre><code>rng = np.random.default_rng(42)\n\nn = 1000\n\nmu0 = np.array([1.5, 1.5], dtype=float)\nmu1 = np.array([5.0, 5.0], dtype=float)\n\ncov0 = np.array([[0.5, 0.0],[0.0, 0.5]], dtype=float)\ncov1 = np.array([[0.5, 0.0],[0.0, 0.5]], dtype=float)\n\nX0 = rng.multivariate_normal(mean=mu0, cov=cov0, size=n)\nX1 = rng.multivariate_normal(mean=mu1, cov=cov1, size=n)\n\ny0 = -np.ones(n, dtype=int)\ny1 =  np.ones(n, dtype=int)\n\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])\n\nidx = rng.permutation(len(X))\nX = X[idx]\ny = y[idx]\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-2-visualizacao-inicial-dos-dados","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o inicial dos dados","text":"<p>Aqui eu plotei a distribui\u00e7\u00e3o dos pontos por classe para inspecionar rapidamente o dataset antes do treino.</p> <ul> <li><code>mask_pos</code> e <code>mask_neg</code> separam os \u00edndices das amostras da classe +1 e classe \u22121.</li> <li>Desenhei dois gr\u00e1ficos de dispers\u00e3o (um por classe), usando marcadores diferentes.</li> </ul> <p>Objetivo: verificar forma, espalhamento e poss\u00edvel sobreposi\u00e7\u00e3o entre as classes, validando a separabilidade linear esperada para o perceptron.</p> <pre><code>mask_pos = y == 1\nmask_neg = y == -1\n\nplt.figure()\nplt.scatter(X[mask_neg, 0], X[mask_neg, 1], label=\"Classe -1\", marker=\"o\", alpha=0.7)\nplt.scatter(X[mask_pos, 0], X[mask_pos, 1], label=\"Classe +1\", marker=\"s\", alpha=0.7)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Distribui\u00e7\u00e3o dos dados por classe\")\nplt.legend()\nplt.show()\n</code></pre> <p> </p> <p>Distribui\u00e7\u00e3o das classes</p>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-3-idealizacao-do-perceptron","title":"Etapa 3 - Idealiza\u00e7\u00e3o do Perceptron","text":"<p>Resumo: O Perceptron \u00e9 uma rede neural de uma \u00fanica camada (um \u00fanico neur\u00f4nio). Ele recebe um vetor de entradas, calcula uma soma ponderada usando pesos e um vi\u00e9s (bias), e aplica uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o em degrau para produzir uma sa\u00edda bin\u00e1ria.</p> <p></p>"},{"location":"perceptron/exercicio1/exercicio1/#31-componentes-da-figura","title":"3.1 Componentes da figura","text":"<ul> <li>Entradas <code>x\u2081, x\u2082, \u2026, x_m</code> (Input): caracter\u00edsticas do exemplo a classificar.  </li> <li>Pesos <code>w\u2081, w\u2082, \u2026, w_m</code> (Weight): import\u00e2ncia de cada entrada.  </li> <li>Somat\u00f3rio <code>\u03a3</code> (Network input function): calcula a soma ponderada.  </li> <li>Fun\u00e7\u00e3o de ativa\u00e7\u00e3o (Activation function): converte o n\u00famero real em decis\u00e3o bin\u00e1ria.  </li> <li>Sa\u00edda (Output): r\u00f3tulo previsto.</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#32-equacoes","title":"3.2 Equa\u00e7\u00f5es","text":"<ol> <li> <p>Soma ponderada (potencial de ativa\u00e7\u00e3o)    $$    a \\;=\\; \\sum_{i=1}^{m} w_i\\,x_i \\;+\\; b \\;=\\; w \\cdot x \\;+\\; b    $$</p> </li> <li> <p>Ativa\u00e7\u00e3o (degrau) </p> </li> <li>Forma 0/1:      $$      \\text{activation}(a) =      \\begin{cases}        1, &amp; a \\ge 0 \\        0, &amp; a &lt; 0      \\end{cases}      $$</li> <li> <p>Forma \u22121/+1 (a que usaremos):      $$      \\hat{y} =      \\begin{cases}        +1, &amp; a \\ge 0 \\        -1, &amp; a &lt; 0      \\end{cases}      $$</p> </li> <li> <p>Regra de decis\u00e3o (equivalente)    $$    \\hat{y} = \\mathrm{sign}(w \\cdot x + b)    $$</p> </li> </ol>"},{"location":"perceptron/exercicio1/exercicio1/#33-interpretacao-geometrica-por-que-e-linear","title":"3.3 Interpreta\u00e7\u00e3o geom\u00e9trica (por que \u00e9 linear)","text":"<ul> <li>A fronteira de decis\u00e3o \u00e9 o conjunto de pontos que satisfaz   $$   w \\cdot x + b = 0.   $$</li> <li>Em 2D, essa fronteira \u00e9 uma reta(nosso caso); em dimens\u00f5es maiores, um hiperplano.  </li> <li>O vetor w \u00e9 perpendicular \u00e0 fronteira; o bias \\(b\\) desloca a reta/hiperplano.</li> </ul> <p>Pr\u00f3xima etapa: veremos como o perceptron aprende \u2014 calculando o erro entre \\(\\hat{y}\\) e o r\u00f3tulo verdadeiro \\(y\\) e ajustando \\(w\\) e \\(b\\) com a regra de atualiza\u00e7\u00e3o.</p>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-4-implementacao-do-perceptron","title":"Etapa 4 \u2014 Implementa\u00e7\u00e3o do Perceptron","text":""},{"location":"perceptron/exercicio1/exercicio1/#41-funcao-degrau-step","title":"4.1 Fun\u00e7\u00e3o degrau (step)","text":"<p>Ideia: Dado um escalar ou vetor <code>z</code>, retornamos +1 quando <code>z \u2265 0</code> e \u22121 quando <code>z &lt; 0</code>. Isso corresponde \u00e0 fun\u00e7\u00e3o de ativa\u00e7\u00e3o do perceptron.</p> <p>F\u00f3rmula:</p> \\[ \\text{sign}(z) = \\begin{cases} +1, &amp; z \\geq 0 \\\\ -1, &amp; z &lt; 0 \\end{cases} \\] <pre><code>def step_sign(z):\n    \"\"\"  \n    Par\u00e2metros:\n    - z: potencial de ativa\u00e7\u00e3o\n\n    Retorna:\n    - +1 para valores \u2265 0, -1 para valores &lt; 0\n    \"\"\"\n    return np.where(z &gt;= 0, 1, -1)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#42-predicao-forward-pass","title":"4.2 Predi\u00e7\u00e3o (forward pass)","text":"<p>Ideia: Para cada amostra <code>x</code>, calculamos o potencial de ativa\u00e7\u00e3o <code>a = w \u00b7 x + b</code> e aplicamos a fun\u00e7\u00e3o degrau. Se <code>a \u2265 0</code> \u2192 classe +1; caso contr\u00e1rio \u2192 \u22121.</p> <p>F\u00f3rmula</p> \\[ \\hat{y} = \\text{sign}(w \\cdot x + b) \\] <p>Par\u00e2metros:</p> <ul> <li> <p>X: <code>matriz de features (n_amostras, n_dimens\u00f5es)</code></p> </li> <li> <p>w: <code>vetor de pesos (n_dimens\u00f5es,)</code></p> </li> <li> <p>b: <code>bias (escalar)</code></p> </li> </ul> <p>Retorna:</p> <ul> <li>vetor de predi\u00e7\u00f5es {-1, +1} para cada amostra</li> </ul> <pre><code>def predict(X, w, b):\n    return step_sign(X @ w + b)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#43-treinamento-aprendizado-com-o-erro","title":"4.3 Treinamento (aprendizado com o erro)","text":"<p>Ideia central: percorrer os dados v\u00e1rias vezes (\u00e9pocas); para cada amostra <code>x_i</code> com r\u00f3tulo <code>y_i \u2208 {\u22121, +1}</code>, se errou (isto \u00e9, <code>y_i (w \u00b7 x_i + b) \u2264 0</code>), ajuste os par\u00e2metros:</p> <p>Regra de atualiza\u00e7\u00e3o</p> \\[ w \\leftarrow w + \\eta y_i x_i, \\quad\\quad b \\leftarrow b + \\eta y_i \\] <ul> <li>eta \u00e9 a taxa de aprendizado.  </li> <li>Embaralhamos as amostras a cada \u00e9poca (opcional) para evitar ciclos.  </li> <li>Guardamos hist\u00f3rico de acur\u00e1cia e n\u00ba de atualiza\u00e7\u00f5es para avaliar a converg\u00eancia.</li> </ul> Fun\u00e7\u00e3o - train_perceptron <pre><code>def train_perceptron(X, y, eta=0.01, max_epochs=100, shuffle=True):\n    \"\"\"    \n    Para cada amostra (x\u1d62, y\u1d62) classificada incorretamente:\n    w \u2190 w + \u03b7\u00b7y\u1d62\u00b7x\u1d62    (atualiza\u00e7\u00e3o dos pesos)\n    b \u2190 b + \u03b7\u00b7y\u1d62      (atualiza\u00e7\u00e3o do bias)\n\n    Uma amostra \u00e9 considerada incorreta se: y\u1d62\u00b7(w\u00b7x\u1d62 + b) \u2264 0\n\n    Par\u00e2metros:\n    - X: matriz de features (n_amostras, n_dimens\u00f5es)\n    - y: vetor de r\u00f3tulos {-1, +1}\n    - eta: taxa de aprendizado\n    - max_epochs: n\u00famero m\u00e1ximo de \u00e9pocas\n    - shuffle: embaralhar amostras a cada \u00e9poca\n\n    Retorna:\n    - w: pesos finais\n    - b: bias final\n    - history: hist\u00f3rico de treinamento (acur\u00e1cia, atualiza\u00e7\u00f5es, par\u00e2metros)\n    - y_hat: predi\u00e7\u00f5es finais\n    \"\"\"\n    n, d = X.shape\n\n    np.random.seed(42)\n    w = np.random.uniform(-2, 2, d)      \n    b = np.random.uniform(-5, 5)        \n\n    history = []\n\n    # percorrer \u00e9pocas\n    for epoch in range(1, max_epochs + 1):\n        updates = 0\n\n        if shuffle:\n            idx = np.random.permutation(n)\n            X_epoch, y_epoch = X[idx], y[idx]\n        else:\n            X_epoch, y_epoch = X, y\n\n        # PERCORRER CADA AMOSTRA NA \u00c9POCA\n        for xi, yi in zip(X_epoch, y_epoch):\n            # potencial de ativa\u00e7\u00e3o: a = w\u00b7x + b\n            z = np.dot(w, xi) + b\n\n            # CRIT\u00c9RIO DE ERRO: yi * z &lt;= 0\n            # Se yi=+1 e z&lt;0: predi\u00e7\u00e3o errada (-1), precisa corrigir\n            # Se yi=-1 e z&gt;0: predi\u00e7\u00e3o errada (+1), precisa corrigir\n            if yi * z &lt;= 0:  \n                # REGRA DE ATUALIZA\u00c7\u00c3O\n                w += eta * yi * xi  # w \u2190 w + \u03b7\u00b7yi\u00b7xi\n                b += eta * yi       # b \u2190 b + \u03b7\u00b7yi\n                updates += 1\n\n        # performance da epoca\n        y_hat = predict(X, w, b)\n        acc = (y_hat == y).mean()  # acuracia\n\n        history.append({\n            \"epoch\": epoch,\n            \"accuracy\": acc,\n            \"updates\": updates,\n            \"w\": w.copy(),\n            \"b\": b\n        })\n\n        # converg\u00eancia\n        if updates == 0:\n            print(f\"Converg\u00eancia alcan\u00e7ada na \u00e9poca {epoch}\")\n            break\n\n    return w, b, history, y_hat\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#44-checagens","title":"4.4 Checagens","text":"<p>Acur\u00e1cia simples:</p> <pre><code>def accuracy(y_true, y_pred):\n    return np.mean(y_true == y_pred)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-5-treinamento-do-perceptron-com-os-dados-do-exercicio","title":"Etapa 5 \u2014 Treinamento do Perceptron com os Dados do Exerc\u00edcio","text":"<p>Objetivo: aplicar o perceptron implementado nos dados gerados na Etapa 1:</p> <ul> <li>Taxa de aprendizado \u03b7 = 0.01</li> <li>M\u00e1ximo de 100 \u00e9pocas</li> <li>Parada antecipada por converg\u00eancia</li> <li>Rastreamento da acur\u00e1cia ap\u00f3s cada \u00e9poca</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#51-treinamento-e-resultados","title":"5.1 Treinamento e resultados","text":"<pre><code>w_final, b_final, history, y_pred_final = train_perceptron(\n    X, y, \n    eta=0.001,       \n    max_epochs=100,  \n    shuffle=True     \n)\n\n\n# metricas\nfinal_accuracy = accuracy(y, y_pred_final)  \nfinal_epoch = history[-1][\"epoch\"]         \ntotal_updates = sum([h[\"updates\"] for h in history]) \n\nprint(\"=== RESULTADOS FINAIS ===\")\nprint(f\"Pesos finais: w = [{w_final[0]:.4f}, {w_final[1]:.4f}]\")\nprint(f\"Vi\u00e9s final: b = {b_final:.4f}\")\nprint(f\"Acur\u00e1cia final: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\nprint(f\"Total de atualiza\u00e7\u00f5es durante todo o treinamento: {total_updates}\")\n\n# erros\nmisclassified_mask = (y != y_pred_final)  \nn_misclassified = np.sum(misclassified_mask)\nprint(f\"Pontos mal classificados: {n_misclassified} de {len(y)} ({n_misclassified/len(y)*100:.2f}%)\")\n</code></pre> <pre><code>Converg\u00eancia alcan\u00e7ada na \u00e9poca 16!\n\n=== RESULTADOS FINAIS ===\nPesos finais: w = [0.0205, 0.0451]\nVi\u00e9s final: b = -0.2251\nAcur\u00e1cia final: 1.0000 (100.00%)\nTotal de atualiza\u00e7\u00f5es durante todo o treinamento: 4341\nPontos mal classificados: 0 de 2000 (0.00%)\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#etapa-6-visualizacao-dos-resultados","title":"Etapa 6 \u2014 Visualiza\u00e7\u00e3o dos Resultados","text":"<p>Objetivo: criar visualiza\u00e7\u00f5es claras e informativas que mostrem:</p> <ol> <li>Evolu\u00e7\u00e3o da acur\u00e1cia durante o treinamento</li> <li>Fronteira de decis\u00e3o aprendida pelo perceptron</li> <li>Pontos mal classificados</li> </ol>"},{"location":"perceptron/exercicio1/exercicio1/#61-curva-de-acuracia-por-epoca","title":"6.1 Curva de acur\u00e1cia por \u00e9poca","text":"<p>A primeira visualiza\u00e7\u00e3o importante \u00e9 observar como a acur\u00e1cia evolui durante o treinamento. Isso nos mostra:</p> <ol> <li>Velocidade de converg\u00eancia: quantas \u00e9pocas o perceptron precisou para aprender</li> <li>Estabilidade: se a acur\u00e1cia melhora consistentemente ou oscila</li> <li>Padr\u00e3o de atualiza\u00e7\u00f5es: como o n\u00famero de corre\u00e7\u00f5es diminui \u00e0 medida que o modelo aprende</li> </ol> <p></p> <p>Os gr\u00e1ficos indicam converg\u00eancia r\u00e1pida e est\u00e1vel do perceptron:</p> <ul> <li>Velocidade de converg\u00eancia: a acur\u00e1cia sai de ~0,4 nas primeiras \u00e9pocas, d\u00e1 um salto por volta da \u00e9poca 4 e atinge 1,0 entre as \u00e9pocas 5\u20136, mantendo-se em plat\u00f4 at\u00e9 o fim.</li> <li>Estabilidade: ap\u00f3s atingir 100%, a acur\u00e1cia permanece constante, sugerindo que n\u00e3o h\u00e1 oscila\u00e7\u00f5es causadas por taxa de aprendizado alta ou por dados ruidosos.</li> <li>Padr\u00e3o de atualiza\u00e7\u00f5es: o n\u00famero de corre\u00e7\u00f5es come\u00e7a muito alto (~1,1k), cai abruptamente ap\u00f3s a \u00e9poca 4 e tende a zero a partir da 6\u00aa \u2014 sinal de que o hiperplano j\u00e1 separa corretamente todas as amostras.</li> </ul>"},{"location":"perceptron/exercicio1/exercicio1/#62-visualizacao-da-fronteira-de-decisao-final","title":"6.2 Visualiza\u00e7\u00e3o da fronteira de decis\u00e3o final","text":"<p>Agora vamos criar uma visualiza\u00e7\u00e3o detalhada do resultado final, mostrando:</p> <ul> <li>Pontos corretamente classificados: marcados com s\u00edmbolos normais</li> <li>Pontos mal classificados: destacados com um 'X' para f\u00e1cil identifica\u00e7\u00e3o  </li> <li>Fronteira de decis\u00e3o: a linha aprendida pelo perceptron</li> <li>Equa\u00e7\u00e3o da fronteira: mostrando os valores finais de w\u2081, w\u2082 e b</li> </ul> <p>Esta visualiza\u00e7\u00e3o nos permite avaliar visualmente a qualidade da separa\u00e7\u00e3o:</p> <p></p> Fronteira de Decis\u00e3o <pre><code># Fun\u00e7\u00e3o para plotar fronteira de decis\u00e3o\ndef plot_decision_boundary(X, y, w, b, title=\"Fronteira de Decis\u00e3o\"):\n    plt.figure(figsize=(10, 8))\n\n    # Separar pontos por classe\n    mask_pos = y == 1\n    mask_neg = y == -1\n\n    # Plot dos pontos corretamente classificados\n    correct_mask = (y == predict(X, w, b))\n\n    plt.scatter(X[mask_neg &amp; correct_mask, 0], X[mask_neg &amp; correct_mask, 1], \n            c='blue', marker='o', alpha=0.7, s=30, label=\"Classe -1 (correto)\")\n    plt.scatter(X[mask_pos &amp; correct_mask, 0], X[mask_pos &amp; correct_mask, 1], \n            c='red', marker='s', alpha=0.7, s=30, label=\"Classe +1 (correto)\")\n\n    # Plot dos pontos mal classificados (se existirem)\n    if np.any(~correct_mask):\n        plt.scatter(X[mask_neg &amp; ~correct_mask, 0], X[mask_neg &amp; ~correct_mask, 1], \n                c='blue', marker='x', s=100, linewidth=3, label=\"Classe -1 (ERRO)\")\n        plt.scatter(X[mask_pos &amp; ~correct_mask, 0], X[mask_pos &amp; ~correct_mask, 1], \n                c='red', marker='x', s=100, linewidth=3, label=\"Classe +1 (ERRO)\")\n\n    # Calcular e plotar a linha de decis\u00e3o\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n\n    if abs(w[1]) &gt; 1e-10:\n        x1_line = np.array([x1_min, x1_max])\n        x2_line = -(w[0] * x1_line + b) / w[1]\n        plt.plot(x1_line, x2_line, 'k-', linewidth=2, label=f\"Fronteira: {w[0]:.3f}x\u2081 + {w[1]:.3f}x\u2082 + {b:.3f} = 0\")\n\n    plt.xlabel(\"x\u2081\")\n    plt.ylabel(\"x\u2082\")\n    plt.title(title)\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.axis('equal')\n\n    return plt.gca()\n\nplot_decision_boundary(X, y, w_final, b_final, \n                    f\"Resultado Final do Perceptron (\u00c9poca {final_epoch})\")\nplt.show()\n</code></pre>"},{"location":"perceptron/exercicio1/exercicio1/#63-evolucao-do-perceptron-por-epoca","title":"6.3 Evolu\u00e7\u00e3o do Perceptron por \u00c9poca","text":"<p>O painel mostra, \u00e9poca a \u00e9poca, como o perceptron aprende a separar duas classes (azul = \u22121, vermelho = +1). Em cada subgr\u00e1fico aparece:</p> <ul> <li>Fronteira de decis\u00e3o (reta preta) definida por \\(w\\cdot x + b = 0\\).</li> <li>Pontos das classes e a acur\u00e1cia obtida naquela \u00e9poca.</li> <li>A cada \u00e9poca, os pesos \\(w\\) e o bias \\(b\\) s\u00e3o ajustados a partir das amostras mal classificadas (regra do perceptron), fazendo a reta girar (mudan\u00e7a em \\(w\\)) e transladar (mudan\u00e7a em \\(b\\)) at\u00e9 atingir a separa\u00e7\u00e3o correta.</li> </ul> <p>O que observar:</p> <ul> <li>Orienta\u00e7\u00e3o da reta: o modelo explora diferentes dire\u00e7\u00f5es (sinal e magnitude dos pesos), buscando a que melhor separa os grupos.</li> <li>Posicionamento da reta: o bias desloca a fronteira para cima/baixo, refinando a divis\u00e3o.</li> <li>Acur\u00e1cia por \u00e9poca: indica o progresso do aprendizado e o qu\u00e3o perto o modelo est\u00e1 do resultado esperado.</li> <li>Estabiliza\u00e7\u00e3o: quando n\u00e3o h\u00e1 mais corre\u00e7\u00f5es necess\u00e1rias, a reta para de se mover e a acur\u00e1cia se mant\u00e9m.</li> </ul> <p></p>"},{"location":"perceptron/exercicio1/exercicio1/#64-conclusoes-a-partir-do-grafico","title":"6.4 Conclus\u00f5es a partir do gr\u00e1fico","text":"<p>O painel mostra um aprendizado progressivo e est\u00e1vel do perceptron:</p> <ul> <li>Arranque e salto de desempenho. A acur\u00e1cia parte baixa (\u2248 0,39 na \u00e9poca 1), evolui lentamente nas \u00e9pocas 2\u20133, e d\u00e1 um salto na \u00e9poca 4 (\u2248 0,73).  </li> <li>Quase perfeito cedo. J\u00e1 na \u00e9poca 5 o modelo atinge \u2248 0,9895, e entre as \u00e9pocas 6\u20138 estabiliza acima de 0,994.</li> <li>Ajustes finos com pequenas oscila\u00e7\u00f5es. Entre as \u00e9pocas 9\u201314 h\u00e1 microvaria\u00e7\u00f5es (ex.: \u00e9poca 11 \u2248 0,9950) t\u00edpicas do ajuste online em pontos pr\u00f3ximos \u00e0 fronteira; em seguida, a acur\u00e1cia volta a 0,999+.</li> <li>Converg\u00eancia. A partir das \u00e9pocas 15\u201316 a acur\u00e1cia chega a 1,0000, indicando nenhum erro no conjunto de treino.</li> <li>Geometria da fronteira. A reta inicia com inclina\u00e7\u00e3o positiva, depois gira e assume inclina\u00e7\u00e3o negativa, evidenciando a corre\u00e7\u00e3o do vetor de pesos e o deslocamento via bias at\u00e9 alinhar a separa\u00e7\u00e3o aos dados.</li> <li>Diagn\u00f3stico dos dados. O alcance de 100% sem instabilidades sugere dados linearmente separ\u00e1veis e taxa de aprendizado adequada (sem oscila\u00e7\u00f5es amplas).</li> </ul>"},{"location":"perceptron/exercicio2/exercicio2/","title":"Exerc\u00edcio 2 - Perceptron","text":"<p>O objetivo desse exercicio \u00e9 mostrar como esse modelo de perceptron age para um dataset muito mais complicado. Al\u00e9m disso, vamos refletir sobre poss\u00edveis saidas para esse problema.</p>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-1-geracao-do-conjunto-com-overlap-dados-de-treino","title":"Etapa 1 \u2014 Gera\u00e7\u00e3o do conjunto com overlap (dados de treino)","text":"<p>Objetivo do bloco: criar duas classes 2D com m\u00e9dias pr\u00f3ximas e vari\u00e2ncia alta para induzir overlap (n\u00e3o separabilidade linear perfeita), j\u00e1 com r\u00f3tulos em {-1, +1} e dados embaralhados no final.</p> <p>O que o c\u00f3digo faz:</p> <ul> <li><code>rng2 = np.random.default_rng(42)</code>: fixa a semente para reprodutibilidade.</li> <li><code>n2 = 1000</code>: define 1.000 amostras por classe (total = 2.000).</li> <li><code>mu0_ex2</code>, <code>mu1_ex2</code>: m\u00e9dias das classes [-1] \u2192 [3, 3] e [+1] \u2192 [4, 4].</li> <li><code>sigma2 = 1.5</code> e <code>cov_ex2 = [[1.5, 0], [0, 1.5]]</code>: covari\u00e2ncia isotr\u00f3pica com vari\u00e2ncia 1.5 em cada eixo.</li> <li><code>X0_ex2</code>, <code>X1_ex2</code>: amostragem multivariada normal de cada classe.</li> <li><code>y0_ex2 = -1</code>, <code>y1_ex2 = +1</code>: r\u00f3tulos.</li> <li><code>X_ex2 = vstack(...)</code>, <code>y_ex2 = hstack(...)</code>: concatena as duas classes em um \u00fanico conjunto.</li> <li><code>idx2 = rng2.permutation(...)</code> e reindexa\u00e7\u00e3o: embaralha as amostras para n\u00e3o ficarem em blocos por classe.</li> </ul> <p>Sa\u00eddas esperadas:</p> <ul> <li><code>X_ex2</code> com shape (2000, 2) e <code>y_ex2</code> com shape (2000,).</li> <li>Distribui\u00e7\u00e3o com overlap vis\u00edvel \u2014 ideal para avaliar as limita\u00e7\u00f5es do Perceptron em dados n\u00e3o totalmente separ\u00e1veis.</li> </ul> <pre><code>rng2 = np.random.default_rng(42)\n\nn2 = 1000 \n\nmu0_ex2 = np.array([3.0, 3.0])  \nmu1_ex2 = np.array([4.0, 4.0])\n\nsigma2 = 1.5\ncov_ex2 = np.array([[sigma2, 0.0], [0.0, sigma2]])\n\nX0_ex2 = rng2.multivariate_normal(mu0_ex2, cov_ex2, n2)\nX1_ex2 = rng2.multivariate_normal(mu1_ex2, cov_ex2, n2)\n\ny0_ex2 = -np.ones(n2, dtype=int)\ny1_ex2 = np.ones(n2, dtype=int)\n\nX_ex2 = np.vstack([X0_ex2, X1_ex2])\ny_ex2 = np.hstack([y0_ex2, y1_ex2])\n\nidx2 = rng2.permutation(len(X_ex2))\nX_ex2 = X_ex2[idx2]\ny_ex2 = y_ex2[idx2]\n</code></pre>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-2-visualizacao-inicial-das-classes","title":"Etapa 2 \u2014 Visualiza\u00e7\u00e3o inicial das classes","text":"<p>Vamos plotar um gr\u00e1fico de dispers\u00e3o (x\u2081 vs x\u2082) separando as amostras por classe (\u22121 em azul, +1 em vermelho). Essa visualiza\u00e7\u00e3o serve para:</p> <ul> <li>Confirmar o overlap entre as distribui\u00e7\u00f5es (as nuvens se interpenetram).</li> <li>Ver forma e dispers\u00e3o dos clusters (vari\u00e2ncia maior, sem correla\u00e7\u00e3o entre eixos).</li> <li>Antecipar o comportamento do Perceptron: como a separa\u00e7\u00e3o n\u00e3o \u00e9 perfeita, a fronteira linear dever\u00e1 resultar em erros residuais mesmo ap\u00f3s o treino.</li> </ul> <p>Esta etapa \u00e9 o \u201cantes do treino\u201d: \u00e9 nossa linha de base visual para comparar depois com a fronteira aprendida.</p> <pre><code>plt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 2)\nmask_pos_ex2 = y_ex2 == 1\nmask_neg_ex2 = y_ex2 == -1\nplt.scatter(X_ex2[mask_neg_ex2, 0], X_ex2[mask_neg_ex2, 1], c='blue', alpha=0.6, s=20, label=\"Classe -1\")\nplt.scatter(X_ex2[mask_pos_ex2, 0], X_ex2[mask_pos_ex2, 1], c='red', alpha=0.6, s=20, label=\"Classe +1\")\nplt.title(\"Dispers\u00e3o das classes -1 e +1\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>An\u00e1lise do scatter</p> <ul> <li>As nuvens azul (\u22121) e vermelha (+1) est\u00e3o fortemente sobrepostas no miolo do plano \u2014 coerente com m\u00e9dias pr\u00f3ximas (\u2248[3,3] e [4,4]) e vari\u00e2ncia alta (1.5).  </li> <li>H\u00e1 uma tend\u00eancia da classe +1 ocupar valores um pouco maiores de \\(x_1\\) e \\(x_2\\), mas sem um \u201cgap\u201d limpo.  </li> <li>Consequ\u00eancia pr\u00e1tica: um limite linear dever\u00e1 separar \u201cem m\u00e9dia\u201d as classes, mas erros residuais s\u00e3o inevit\u00e1veis \u2014 esperamos acur\u00e1cia &lt; 100% e atualiza\u00e7\u00f5es persistentes durante o treino do Perceptron.</li> </ul>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-3-treinamento-do-perceptron","title":"Etapa 3 \u2014 Treinamento do Perceptron","text":"<p>Aqui treinamos o modelo reutilizando exatamente a fun\u00e7\u00e3o <code>train_perceptron</code> do Exerc\u00edcio 1 sobre o conjunto com overlap:</p> <ul> <li><code>w_ex2, b_ex2, history_ex2, y_pred_ex2 = train_perceptron(X_ex2, y_ex2, eta=0.001, max_epochs=100, shuffle=True)</code> </li> <li><code>X_ex2</code>, <code>y_ex2</code>: dados gerados na etapa anterior.  </li> <li><code>eta=0.001</code>: taxa de aprendizado menor para estabilizar a aprendizagem em dados n\u00e3o totalmente separ\u00e1veis.  </li> <li><code>max_epochs=100</code>: limite superior de \u00e9pocas.  </li> <li><code>shuffle=True</code>: embaralha as amostras a cada \u00e9poca, o que ajuda a evitar ciclos em cen\u00e1rios com overlap.  </li> <li>Sa\u00eddas:  <ul> <li><code>w_ex2</code>, <code>b_ex2</code>: pesos e vi\u00e9s aprendidos;  </li> <li><code>history_ex2</code>: lista com m\u00e9tricas por \u00e9poca (<code>accuracy</code>, <code>updates</code>, <code>w</code>, <code>b</code>);  </li> <li><code>y_pred_ex2</code>: predi\u00e7\u00f5es finais no treino ({-1, +1}).</li> </ul> </li> </ul> <p>Em seguida calculamos as m\u00e9tricas:</p> <ul> <li><code>final_acc_ex2 = accuracy(y_ex2, y_pred_ex2)</code>: acur\u00e1cia final no conjunto de treino (mesma fun\u00e7\u00e3o de utilit\u00e1rio usada no Exerc\u00edcio 1).  </li> <li><code>final_epoch_ex2 = history_ex2[-1][\"epoch\"]</code>: \u00faltima \u00e9poca executada (serve para inferir se bateu o limite de 100).  </li> <li><code>total_updates_ex2 = sum(h[\"updates\"] for h in history_ex2)</code>: total de atualiza\u00e7\u00f5es (quantas corre\u00e7\u00f5es ocorreram ao longo de todo o treinamento).</li> </ul> <p>Lembrando: como h\u00e1 overlap, \u00e9 esperado que a acur\u00e1cia n\u00e3o atinja 100% e que possamos n\u00e3o zerar atualiza\u00e7\u00f5es antes de chegar a <code>max_epochs</code>.</p> <pre><code>w_ex2, b_ex2, history_ex2, y_pred_ex2 = train_perceptron(\n    X_ex2, y_ex2, \n    eta=0.001,      \n    max_epochs=100, \n    shuffle=True\n)\n\nfinal_acc_ex2 = accuracy(y_ex2, y_pred_ex2)\nfinal_epoch_ex2 = history_ex2[-1][\"epoch\"]\ntotal_updates_ex2 = sum([h[\"updates\"] for h in history_ex2])\n\nprint(\"RESULTADOS:\")\nprint(f\"   Convergiu? {'N\u00c3O' if final_epoch_ex2 &gt;= 100 else 'SIM'}\")\nprint(f\"   \u00c9pocas usadas: {final_epoch_ex2}/100\")\nprint(f\"   Acur\u00e1cia final: {final_acc_ex2:.3f} ({final_acc_ex2*100:.1f}%)\")\nprint(f\"   Total de atualiza\u00e7\u00f5es: {total_updates_ex2}\")\n</code></pre> <pre><code>RESULTADOS:\n   Convergiu? N\u00c3O\n   \u00c9pocas usadas: 100/100\n   Acur\u00e1cia final: 0.725 (72.5%)\n   Total de atualiza\u00e7\u00f5es: 75725\n</code></pre>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-4-fronteira-de-decisao-sobre-os-dados-e-destaque-dos-erros","title":"Etapa 4 \u2014 Fronteira de decis\u00e3o sobre os dados e destaque dos erros","text":"<p>O que este bloco faz:</p> <ul> <li> <p>Reaproveita os par\u00e2metros aprendidos no treino (<code>w_ex2</code>, <code>b_ex2</code>) para sobrepor a fronteira de decis\u00e3o \\(w_1x_1 + w_2x_2 + b = 0\\) ao scatter das classes.</p> </li> <li> <p>Erros de classifica\u00e7\u00e3o s\u00e3o real\u00e7ados com marcadores \u2018x\u2019 amarelos: <code>errors_ex2 = (y_ex2 != y_pred_ex2)</code>.   Em dados com overlap, \u00e9 esperado ver esses pontos concentrados pr\u00f3ximos \u00e0 fronteira, evidenciando a limita\u00e7\u00e3o do classificador linear.</p> </li> </ul> <p>O que observar no resultado:</p> <ul> <li>Inclina\u00e7\u00e3o e posi\u00e7\u00e3o da reta refletem os pesos \\(w\\) e o bias \\(b\\) aprendidos: rota\u00e7\u00e3o \u21e2 \\(w\\), deslocamento \u21e2 \\(b\\).</li> <li>Em fun\u00e7\u00e3o do overlap, haver\u00e1 erros residuais (as marcas amarelas), mesmo ap\u00f3s treinamento, e a acur\u00e1cia n\u00e3o deve chegar a 100%.</li> <li>A fronteira se posiciona como um compromisso entre as duas distribui\u00e7\u00f5es, minimizando erros em m\u00e9dia.</li> </ul> <pre><code>plt.figure(figsize=(14, 5))\n\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_ex2[mask_neg_ex2, 0], X_ex2[mask_neg_ex2, 1], c='blue', alpha=0.6, s=15, label=\"Classe -1\")\nplt.scatter(X_ex2[mask_pos_ex2, 0], X_ex2[mask_pos_ex2, 1], c='red', alpha=0.6, s=15, label=\"Classe +1\")\n\nx1_range_ex2 = np.array([X_ex2[:, 0].min()-0.5, X_ex2[:, 0].max()+0.5])\nif abs(w_ex2[1]) &gt; 1e-10:\n    x2_line_ex2 = -(w_ex2[0] * x1_range_ex2 + b_ex2) / w_ex2[1]\n    plt.plot(x1_range_ex2, x2_line_ex2, 'k-', linewidth=3, label=\"Fronteira\")\n\n# erros\nerrors_ex2 = (y_ex2 != y_pred_ex2)\nif np.any(errors_ex2):\n    plt.scatter(X_ex2[errors_ex2, 0], X_ex2[errors_ex2, 1], \n               c='yellow', marker='x', s=50, linewidth=2, label=\"Erros\")\n\nplt.title(f\"Fronteira de decis\u00e3o: {final_acc_ex2:.3f} acur\u00e1cia\")\nplt.xlabel(\"x\u2081\")\nplt.ylabel(\"x\u2082\")\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"perceptron/exercicio2/exercicio2/#etapa-5-conclusoes-sobre-os-resultados","title":"Etapa 5 \u2014 Conclus\u00f5es sobre os resultados","text":"<p>A fronteira aprendida \u00e9 linear e a acur\u00e1cia final ficou em \u2248 0,725. Os erros (marcados em amarelo) concentram-se na regi\u00e3o central onde as duas distribui\u00e7\u00f5es se sobrep\u00f5em, exatamente onde um limite linear n\u00e3o consegue discriminar perfeitamente.</p> <p>Por que chegamos a ~0,72 de acur\u00e1cia? - Os dados foram gerados por duas gaussianas com m\u00e9dias pr\u00f3ximas \\([3,3]\\) e \\([4,4]\\) e mesma covari\u00e2ncia isotr\u00f3pica \\(\\Sigma = 1.5\\,I\\). Logo, o problema n\u00e3o \u00e9 linearmente separ\u00e1vel. - O Perceptron ter chegado a 0,725 \u00e9 coerente e pr\u00f3ximo do \u00f3timo para um modelo linear.</p> <p>E o MLP? Um Multi-Layer Perceptron (com ativa\u00e7\u00f5es n\u00e3o lineares) pode aprender fronteiras curvas e superar modelos lineares se a separa\u00e7\u00e3o \u00f3tima for n\u00e3o linear. Neste dataset espec\u00edfico (duas gaussianas com covari\u00e2ncias iguais), o \u00f3timo \u00e9 linear; portanto, um MLP bem regularizado tende a empatar com um linear em m\u00e9dia. Ele s\u00f3 superaria de forma consistente se a estrutura real dos dados exigir n\u00e3o linearidade.</p> <p>Resumindo: era previsto que uma reta n\u00e3o separasse perfeitamente essas duas classes. Para ganhos reais, use lineares bem calibrados ou modelos n\u00e3o lineares quando houver evid\u00eancia de fronteira n\u00e3o linear.</p>"},{"location":"projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"thisdocumentation/main/","title":"This documentation","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\.venv\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}